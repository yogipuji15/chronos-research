{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3333333333333333,
  "eval_steps": 500,
  "global_step": 100000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.3355211615562439,
      "learning_rate": 0.0009983333333333333,
      "loss": 2.0673,
      "step": 500
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.37550294399261475,
      "learning_rate": 0.0009966666666666668,
      "loss": 2.0448,
      "step": 1000
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.2985720634460449,
      "learning_rate": 0.000995,
      "loss": 2.0235,
      "step": 1500
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.4061012864112854,
      "learning_rate": 0.0009933333333333333,
      "loss": 1.9993,
      "step": 2000
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.3289767801761627,
      "learning_rate": 0.0009916666666666667,
      "loss": 2.0047,
      "step": 2500
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.3406992554664612,
      "learning_rate": 0.00099,
      "loss": 1.9933,
      "step": 3000
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.32093068957328796,
      "learning_rate": 0.0009883333333333333,
      "loss": 1.9825,
      "step": 3500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.3466475307941437,
      "learning_rate": 0.0009866666666666667,
      "loss": 1.9613,
      "step": 4000
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.3956133723258972,
      "learning_rate": 0.000985,
      "loss": 1.9562,
      "step": 4500
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4207194149494171,
      "learning_rate": 0.0009833333333333332,
      "loss": 1.9328,
      "step": 5000
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5304854512214661,
      "learning_rate": 0.0009816666666666667,
      "loss": 1.9249,
      "step": 5500
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.457515686750412,
      "learning_rate": 0.00098,
      "loss": 1.9151,
      "step": 6000
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.6633057594299316,
      "learning_rate": 0.0009783333333333334,
      "loss": 1.9,
      "step": 6500
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.4478442072868347,
      "learning_rate": 0.0009766666666666667,
      "loss": 1.8836,
      "step": 7000
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.4768136143684387,
      "learning_rate": 0.000975,
      "loss": 1.888,
      "step": 7500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.502360463142395,
      "learning_rate": 0.0009733333333333334,
      "loss": 1.8593,
      "step": 8000
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.46312302350997925,
      "learning_rate": 0.0009716666666666667,
      "loss": 1.8546,
      "step": 8500
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.48460450768470764,
      "learning_rate": 0.0009699999999999999,
      "loss": 1.8411,
      "step": 9000
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.7103081941604614,
      "learning_rate": 0.0009683333333333334,
      "loss": 1.8385,
      "step": 9500
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.4974411129951477,
      "learning_rate": 0.0009666666666666667,
      "loss": 1.8185,
      "step": 10000
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.5890368223190308,
      "learning_rate": 0.000965,
      "loss": 1.8081,
      "step": 10500
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.5818201899528503,
      "learning_rate": 0.0009633333333333334,
      "loss": 1.7835,
      "step": 11000
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.6060959100723267,
      "learning_rate": 0.0009616666666666667,
      "loss": 1.7784,
      "step": 11500
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.5379834771156311,
      "learning_rate": 0.00096,
      "loss": 1.7518,
      "step": 12000
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.6572637557983398,
      "learning_rate": 0.0009583333333333334,
      "loss": 1.7467,
      "step": 12500
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.7304471731185913,
      "learning_rate": 0.0009566666666666666,
      "loss": 1.7383,
      "step": 13000
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.6181036233901978,
      "learning_rate": 0.000955,
      "loss": 1.7267,
      "step": 13500
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.6775724291801453,
      "learning_rate": 0.0009533333333333334,
      "loss": 1.7137,
      "step": 14000
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.6389793157577515,
      "learning_rate": 0.0009516666666666666,
      "loss": 1.7099,
      "step": 14500
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.6595916748046875,
      "learning_rate": 0.00095,
      "loss": 1.6892,
      "step": 15000
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.7719201445579529,
      "learning_rate": 0.0009483333333333334,
      "loss": 1.6796,
      "step": 15500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.5772321820259094,
      "learning_rate": 0.0009466666666666667,
      "loss": 1.6727,
      "step": 16000
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.6944795846939087,
      "learning_rate": 0.000945,
      "loss": 1.6561,
      "step": 16500
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.7987475395202637,
      "learning_rate": 0.0009433333333333334,
      "loss": 1.6509,
      "step": 17000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.7101038694381714,
      "learning_rate": 0.0009416666666666667,
      "loss": 1.6525,
      "step": 17500
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6223574280738831,
      "learning_rate": 0.00094,
      "loss": 1.6332,
      "step": 18000
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.6862496733665466,
      "learning_rate": 0.0009383333333333333,
      "loss": 1.6089,
      "step": 18500
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.733048141002655,
      "learning_rate": 0.0009366666666666667,
      "loss": 1.6074,
      "step": 19000
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.7021152377128601,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.5973,
      "step": 19500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.7779088616371155,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.5806,
      "step": 20000
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.7145970463752747,
      "learning_rate": 0.0009316666666666667,
      "loss": 1.5742,
      "step": 20500
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.8040848970413208,
      "learning_rate": 0.00093,
      "loss": 1.5551,
      "step": 21000
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.7681021094322205,
      "learning_rate": 0.0009283333333333333,
      "loss": 1.5647,
      "step": 21500
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.717124342918396,
      "learning_rate": 0.0009266666666666667,
      "loss": 1.5475,
      "step": 22000
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.7583231329917908,
      "learning_rate": 0.000925,
      "loss": 1.5458,
      "step": 22500
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.7346726059913635,
      "learning_rate": 0.0009233333333333334,
      "loss": 1.5441,
      "step": 23000
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.6872162222862244,
      "learning_rate": 0.0009216666666666667,
      "loss": 1.524,
      "step": 23500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.868769109249115,
      "learning_rate": 0.00092,
      "loss": 1.5086,
      "step": 24000
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.7928684949874878,
      "learning_rate": 0.0009183333333333334,
      "loss": 1.509,
      "step": 24500
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.8284257054328918,
      "learning_rate": 0.0009166666666666666,
      "loss": 1.5067,
      "step": 25000
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.8000340461730957,
      "learning_rate": 0.000915,
      "loss": 1.4875,
      "step": 25500
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.8220829963684082,
      "learning_rate": 0.0009133333333333334,
      "loss": 1.501,
      "step": 26000
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.7681417465209961,
      "learning_rate": 0.0009116666666666666,
      "loss": 1.4751,
      "step": 26500
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.8356546759605408,
      "learning_rate": 0.00091,
      "loss": 1.4736,
      "step": 27000
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.8704735040664673,
      "learning_rate": 0.0009083333333333334,
      "loss": 1.4582,
      "step": 27500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.7940770387649536,
      "learning_rate": 0.0009066666666666666,
      "loss": 1.4569,
      "step": 28000
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.9029849171638489,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.4531,
      "step": 28500
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.8514230847358704,
      "learning_rate": 0.0009033333333333334,
      "loss": 1.4337,
      "step": 29000
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.7928577065467834,
      "learning_rate": 0.0009016666666666666,
      "loss": 1.4334,
      "step": 29500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.819105327129364,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.4233,
      "step": 30000
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.7964473962783813,
      "learning_rate": 0.0008983333333333333,
      "loss": 1.4298,
      "step": 30500
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.8431414365768433,
      "learning_rate": 0.0008966666666666666,
      "loss": 1.4117,
      "step": 31000
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.8380269408226013,
      "learning_rate": 0.0008950000000000001,
      "loss": 1.4037,
      "step": 31500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.8863115310668945,
      "learning_rate": 0.0008933333333333333,
      "loss": 1.4005,
      "step": 32000
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.8764029741287231,
      "learning_rate": 0.0008916666666666667,
      "loss": 1.3905,
      "step": 32500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.8948859572410583,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.3843,
      "step": 33000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.8076921701431274,
      "learning_rate": 0.0008883333333333333,
      "loss": 1.3848,
      "step": 33500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.8373511433601379,
      "learning_rate": 0.0008866666666666667,
      "loss": 1.3655,
      "step": 34000
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.8531045913696289,
      "learning_rate": 0.000885,
      "loss": 1.375,
      "step": 34500
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.7587197422981262,
      "learning_rate": 0.0008833333333333333,
      "loss": 1.3496,
      "step": 35000
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.9257946610450745,
      "learning_rate": 0.0008816666666666668,
      "loss": 1.3579,
      "step": 35500
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8915951251983643,
      "learning_rate": 0.00088,
      "loss": 1.3515,
      "step": 36000
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.9003289341926575,
      "learning_rate": 0.0008783333333333333,
      "loss": 1.3419,
      "step": 36500
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.8823502063751221,
      "learning_rate": 0.0008766666666666668,
      "loss": 1.3341,
      "step": 37000
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.8118137717247009,
      "learning_rate": 0.000875,
      "loss": 1.3396,
      "step": 37500
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.9283266663551331,
      "learning_rate": 0.0008733333333333333,
      "loss": 1.3269,
      "step": 38000
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 1.0429970026016235,
      "learning_rate": 0.0008716666666666667,
      "loss": 1.3305,
      "step": 38500
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.8878975510597229,
      "learning_rate": 0.00087,
      "loss": 1.3142,
      "step": 39000
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.7893909215927124,
      "learning_rate": 0.0008683333333333333,
      "loss": 1.3048,
      "step": 39500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.9881820678710938,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.2989,
      "step": 40000
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.8750188946723938,
      "learning_rate": 0.000865,
      "loss": 1.2905,
      "step": 40500
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.8218732476234436,
      "learning_rate": 0.0008633333333333334,
      "loss": 1.2853,
      "step": 41000
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.7952191829681396,
      "learning_rate": 0.0008616666666666667,
      "loss": 1.2896,
      "step": 41500
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9972625970840454,
      "learning_rate": 0.00086,
      "loss": 1.274,
      "step": 42000
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 1.0251621007919312,
      "learning_rate": 0.0008583333333333333,
      "loss": 1.2763,
      "step": 42500
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.8515802025794983,
      "learning_rate": 0.0008566666666666667,
      "loss": 1.2736,
      "step": 43000
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.9712474346160889,
      "learning_rate": 0.000855,
      "loss": 1.2577,
      "step": 43500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.9134600758552551,
      "learning_rate": 0.0008533333333333334,
      "loss": 1.2615,
      "step": 44000
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.9721280932426453,
      "learning_rate": 0.0008516666666666667,
      "loss": 1.2506,
      "step": 44500
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.8719412684440613,
      "learning_rate": 0.00085,
      "loss": 1.2493,
      "step": 45000
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.900425910949707,
      "learning_rate": 0.0008483333333333334,
      "loss": 1.2444,
      "step": 45500
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 1.0517330169677734,
      "learning_rate": 0.0008466666666666667,
      "loss": 1.2332,
      "step": 46000
    },
    {
      "epoch": 0.155,
      "grad_norm": 1.0166348218917847,
      "learning_rate": 0.0008449999999999999,
      "loss": 1.246,
      "step": 46500
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.9865723252296448,
      "learning_rate": 0.0008433333333333334,
      "loss": 1.2204,
      "step": 47000
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.8766039609909058,
      "learning_rate": 0.0008416666666666667,
      "loss": 1.2317,
      "step": 47500
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.941115140914917,
      "learning_rate": 0.00084,
      "loss": 1.2136,
      "step": 48000
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.9155890941619873,
      "learning_rate": 0.0008383333333333334,
      "loss": 1.2125,
      "step": 48500
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.8064982295036316,
      "learning_rate": 0.0008366666666666667,
      "loss": 1.2173,
      "step": 49000
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.9369194507598877,
      "learning_rate": 0.000835,
      "loss": 1.2047,
      "step": 49500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.8538953065872192,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.1958,
      "step": 50000
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 1.0207866430282593,
      "learning_rate": 0.0008316666666666666,
      "loss": 1.1986,
      "step": 50500
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.9902206659317017,
      "learning_rate": 0.00083,
      "loss": 1.1868,
      "step": 51000
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 1.009329915046692,
      "learning_rate": 0.0008283333333333334,
      "loss": 1.1944,
      "step": 51500
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.9192966222763062,
      "learning_rate": 0.0008266666666666666,
      "loss": 1.1675,
      "step": 52000
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.0058118104934692,
      "learning_rate": 0.000825,
      "loss": 1.1702,
      "step": 52500
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.9155881404876709,
      "learning_rate": 0.0008233333333333334,
      "loss": 1.1773,
      "step": 53000
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 1.1042126417160034,
      "learning_rate": 0.0008216666666666667,
      "loss": 1.1629,
      "step": 53500
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.8705521821975708,
      "learning_rate": 0.00082,
      "loss": 1.1615,
      "step": 54000
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.9926793575286865,
      "learning_rate": 0.0008183333333333333,
      "loss": 1.1688,
      "step": 54500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.996634840965271,
      "learning_rate": 0.0008166666666666667,
      "loss": 1.1609,
      "step": 55000
    },
    {
      "epoch": 0.185,
      "grad_norm": 1.0290559530258179,
      "learning_rate": 0.000815,
      "loss": 1.1509,
      "step": 55500
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.9593789577484131,
      "learning_rate": 0.0008133333333333333,
      "loss": 1.151,
      "step": 56000
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 1.0978747606277466,
      "learning_rate": 0.0008116666666666667,
      "loss": 1.1444,
      "step": 56500
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.9745413064956665,
      "learning_rate": 0.0008100000000000001,
      "loss": 1.1367,
      "step": 57000
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.1057230234146118,
      "learning_rate": 0.0008083333333333333,
      "loss": 1.1402,
      "step": 57500
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.8359063863754272,
      "learning_rate": 0.0008066666666666667,
      "loss": 1.1355,
      "step": 58000
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.920087993144989,
      "learning_rate": 0.000805,
      "loss": 1.1238,
      "step": 58500
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.8549920320510864,
      "learning_rate": 0.0008033333333333333,
      "loss": 1.1278,
      "step": 59000
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 1.044141173362732,
      "learning_rate": 0.0008016666666666667,
      "loss": 1.1185,
      "step": 59500
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9018813371658325,
      "learning_rate": 0.0008,
      "loss": 1.1151,
      "step": 60000
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.8610502481460571,
      "learning_rate": 0.0007983333333333334,
      "loss": 1.1163,
      "step": 60500
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.992937445640564,
      "learning_rate": 0.0007966666666666667,
      "loss": 1.1076,
      "step": 61000
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.9966478943824768,
      "learning_rate": 0.000795,
      "loss": 1.106,
      "step": 61500
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 1.0127167701721191,
      "learning_rate": 0.0007933333333333334,
      "loss": 1.109,
      "step": 62000
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.9203179478645325,
      "learning_rate": 0.0007916666666666666,
      "loss": 1.0903,
      "step": 62500
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.9929056167602539,
      "learning_rate": 0.00079,
      "loss": 1.0903,
      "step": 63000
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 1.018000602722168,
      "learning_rate": 0.0007883333333333334,
      "loss": 1.0884,
      "step": 63500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.8624317049980164,
      "learning_rate": 0.0007866666666666666,
      "loss": 1.0861,
      "step": 64000
    },
    {
      "epoch": 0.215,
      "grad_norm": 1.006980061531067,
      "learning_rate": 0.000785,
      "loss": 1.0903,
      "step": 64500
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.060867190361023,
      "learning_rate": 0.0007833333333333334,
      "loss": 1.0843,
      "step": 65000
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.9617030024528503,
      "learning_rate": 0.0007816666666666666,
      "loss": 1.0781,
      "step": 65500
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.9194023013114929,
      "learning_rate": 0.0007800000000000001,
      "loss": 1.0753,
      "step": 66000
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.9613245129585266,
      "learning_rate": 0.0007783333333333334,
      "loss": 1.0662,
      "step": 66500
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.9853022694587708,
      "learning_rate": 0.0007766666666666666,
      "loss": 1.0639,
      "step": 67000
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.9544127583503723,
      "learning_rate": 0.0007750000000000001,
      "loss": 1.0589,
      "step": 67500
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.069939374923706,
      "learning_rate": 0.0007733333333333333,
      "loss": 1.0615,
      "step": 68000
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 1.1555745601654053,
      "learning_rate": 0.0007716666666666666,
      "loss": 1.0605,
      "step": 68500
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.1689273118972778,
      "learning_rate": 0.0007700000000000001,
      "loss": 1.0471,
      "step": 69000
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 1.1077680587768555,
      "learning_rate": 0.0007683333333333333,
      "loss": 1.0591,
      "step": 69500
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.1474120616912842,
      "learning_rate": 0.0007666666666666667,
      "loss": 1.043,
      "step": 70000
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.8902185559272766,
      "learning_rate": 0.0007650000000000001,
      "loss": 1.0404,
      "step": 70500
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 1.2042303085327148,
      "learning_rate": 0.0007633333333333333,
      "loss": 1.0335,
      "step": 71000
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.8724710941314697,
      "learning_rate": 0.0007616666666666667,
      "loss": 1.0352,
      "step": 71500
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9713504910469055,
      "learning_rate": 0.00076,
      "loss": 1.0344,
      "step": 72000
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.942729115486145,
      "learning_rate": 0.0007583333333333333,
      "loss": 1.0204,
      "step": 72500
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 1.0333727598190308,
      "learning_rate": 0.0007566666666666668,
      "loss": 1.0248,
      "step": 73000
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.9333825707435608,
      "learning_rate": 0.000755,
      "loss": 1.0128,
      "step": 73500
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.9878674745559692,
      "learning_rate": 0.0007533333333333333,
      "loss": 1.0253,
      "step": 74000
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 1.0109387636184692,
      "learning_rate": 0.0007516666666666668,
      "loss": 1.0078,
      "step": 74500
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.8716571927070618,
      "learning_rate": 0.00075,
      "loss": 1.0108,
      "step": 75000
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.8785520195960999,
      "learning_rate": 0.0007483333333333333,
      "loss": 1.0067,
      "step": 75500
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.9054956436157227,
      "learning_rate": 0.0007466666666666667,
      "loss": 1.0122,
      "step": 76000
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.9999340176582336,
      "learning_rate": 0.000745,
      "loss": 1.0104,
      "step": 76500
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.8989091515541077,
      "learning_rate": 0.0007433333333333333,
      "loss": 0.9938,
      "step": 77000
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.062361717224121,
      "learning_rate": 0.0007416666666666667,
      "loss": 0.9927,
      "step": 77500
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.8743816018104553,
      "learning_rate": 0.00074,
      "loss": 0.9904,
      "step": 78000
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.9940075278282166,
      "learning_rate": 0.0007383333333333334,
      "loss": 0.9937,
      "step": 78500
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.8652383089065552,
      "learning_rate": 0.0007366666666666667,
      "loss": 0.9848,
      "step": 79000
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.9694322347640991,
      "learning_rate": 0.000735,
      "loss": 0.9768,
      "step": 79500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.9644596576690674,
      "learning_rate": 0.0007333333333333333,
      "loss": 0.9794,
      "step": 80000
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 1.0764353275299072,
      "learning_rate": 0.0007316666666666667,
      "loss": 0.9773,
      "step": 80500
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.11562180519104,
      "learning_rate": 0.00073,
      "loss": 0.9673,
      "step": 81000
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.8707228899002075,
      "learning_rate": 0.0007283333333333334,
      "loss": 0.9878,
      "step": 81500
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.8687900900840759,
      "learning_rate": 0.0007266666666666667,
      "loss": 0.9676,
      "step": 82000
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.9881256818771362,
      "learning_rate": 0.000725,
      "loss": 0.9597,
      "step": 82500
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 1.2690694332122803,
      "learning_rate": 0.0007233333333333334,
      "loss": 0.957,
      "step": 83000
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.9718475937843323,
      "learning_rate": 0.0007216666666666667,
      "loss": 0.959,
      "step": 83500
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.13005793094635,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.9483,
      "step": 84000
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.9922038912773132,
      "learning_rate": 0.0007183333333333334,
      "loss": 0.9594,
      "step": 84500
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.8931356072425842,
      "learning_rate": 0.0007166666666666667,
      "loss": 0.9526,
      "step": 85000
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.9057169556617737,
      "learning_rate": 0.000715,
      "loss": 0.9499,
      "step": 85500
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.003648042678833,
      "learning_rate": 0.0007133333333333334,
      "loss": 0.9421,
      "step": 86000
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 1.104522705078125,
      "learning_rate": 0.0007116666666666667,
      "loss": 0.9391,
      "step": 86500
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.0526796579360962,
      "learning_rate": 0.00071,
      "loss": 0.944,
      "step": 87000
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.9382848143577576,
      "learning_rate": 0.0007083333333333334,
      "loss": 0.9371,
      "step": 87500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 1.0390491485595703,
      "learning_rate": 0.0007066666666666666,
      "loss": 0.9339,
      "step": 88000
    },
    {
      "epoch": 0.295,
      "grad_norm": 1.0137202739715576,
      "learning_rate": 0.000705,
      "loss": 0.9323,
      "step": 88500
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 1.0043389797210693,
      "learning_rate": 0.0007033333333333334,
      "loss": 0.9361,
      "step": 89000
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 1.0641875267028809,
      "learning_rate": 0.0007016666666666666,
      "loss": 0.9219,
      "step": 89500
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.902735710144043,
      "learning_rate": 0.0007,
      "loss": 0.9284,
      "step": 90000
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.9443297982215881,
      "learning_rate": 0.0006983333333333334,
      "loss": 0.9242,
      "step": 90500
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 1.0209081172943115,
      "learning_rate": 0.0006966666666666667,
      "loss": 0.9239,
      "step": 91000
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.9547789692878723,
      "learning_rate": 0.000695,
      "loss": 0.9197,
      "step": 91500
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.9873912930488586,
      "learning_rate": 0.0006933333333333333,
      "loss": 0.9188,
      "step": 92000
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.9573597311973572,
      "learning_rate": 0.0006916666666666667,
      "loss": 0.9061,
      "step": 92500
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.9724090695381165,
      "learning_rate": 0.00069,
      "loss": 0.9087,
      "step": 93000
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 1.476474642753601,
      "learning_rate": 0.0006883333333333333,
      "loss": 0.9058,
      "step": 93500
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 1.0265682935714722,
      "learning_rate": 0.0006866666666666667,
      "loss": 0.9033,
      "step": 94000
    },
    {
      "epoch": 0.315,
      "grad_norm": 1.2041356563568115,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.9097,
      "step": 94500
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.8538944125175476,
      "learning_rate": 0.0006833333333333333,
      "loss": 0.9109,
      "step": 95000
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.983893871307373,
      "learning_rate": 0.0006816666666666667,
      "loss": 0.901,
      "step": 95500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.0800694227218628,
      "learning_rate": 0.00068,
      "loss": 0.8976,
      "step": 96000
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 1.022412896156311,
      "learning_rate": 0.0006783333333333333,
      "loss": 0.8955,
      "step": 96500
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.9809665083885193,
      "learning_rate": 0.0006766666666666667,
      "loss": 0.893,
      "step": 97000
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.9918899536132812,
      "learning_rate": 0.000675,
      "loss": 0.8846,
      "step": 97500
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 1.0031393766403198,
      "learning_rate": 0.0006733333333333334,
      "loss": 0.8816,
      "step": 98000
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.932267427444458,
      "learning_rate": 0.0006716666666666667,
      "loss": 0.8809,
      "step": 98500
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.047802209854126,
      "learning_rate": 0.00067,
      "loss": 0.8887,
      "step": 99000
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 1.2314518690109253,
      "learning_rate": 0.0006683333333333334,
      "loss": 0.872,
      "step": 99500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.0093920230865479,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.8771,
      "step": 100000
    }
  ],
  "logging_steps": 500,
  "max_steps": 300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 100000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.339396509696e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
