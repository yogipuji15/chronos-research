{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.8333333333333334,
  "eval_steps": 500,
  "global_step": 250000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.47432389855384827,
      "learning_rate": 0.0009983333333333333,
      "loss": 2.4273,
      "step": 500
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.6223739981651306,
      "learning_rate": 0.0009966666666666668,
      "loss": 2.3662,
      "step": 1000
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.5113865733146667,
      "learning_rate": 0.000995,
      "loss": 2.3413,
      "step": 1500
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.4166161119937897,
      "learning_rate": 0.0009933333333333333,
      "loss": 2.3489,
      "step": 2000
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.5026922225952148,
      "learning_rate": 0.0009916666666666667,
      "loss": 2.3092,
      "step": 2500
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.571026623249054,
      "learning_rate": 0.00099,
      "loss": 2.2846,
      "step": 3000
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.6771647334098816,
      "learning_rate": 0.0009883333333333333,
      "loss": 2.2868,
      "step": 3500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.6388105154037476,
      "learning_rate": 0.0009866666666666667,
      "loss": 2.2497,
      "step": 4000
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.6788287162780762,
      "learning_rate": 0.000985,
      "loss": 2.2401,
      "step": 4500
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4822697639465332,
      "learning_rate": 0.0009833333333333332,
      "loss": 2.2257,
      "step": 5000
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5764846801757812,
      "learning_rate": 0.0009816666666666667,
      "loss": 2.2096,
      "step": 5500
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.6190088391304016,
      "learning_rate": 0.00098,
      "loss": 2.2068,
      "step": 6000
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.6824941635131836,
      "learning_rate": 0.0009783333333333334,
      "loss": 2.1722,
      "step": 6500
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.6143842935562134,
      "learning_rate": 0.0009766666666666667,
      "loss": 2.1581,
      "step": 7000
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.5903660655021667,
      "learning_rate": 0.000975,
      "loss": 2.1295,
      "step": 7500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.6020989418029785,
      "learning_rate": 0.0009733333333333334,
      "loss": 2.1147,
      "step": 8000
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.713334858417511,
      "learning_rate": 0.0009716666666666667,
      "loss": 2.1085,
      "step": 8500
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.783708393573761,
      "learning_rate": 0.0009699999999999999,
      "loss": 2.0945,
      "step": 9000
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.6970427632331848,
      "learning_rate": 0.0009683333333333334,
      "loss": 2.0517,
      "step": 9500
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.6578370928764343,
      "learning_rate": 0.0009666666666666667,
      "loss": 2.0723,
      "step": 10000
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.8710381984710693,
      "learning_rate": 0.000965,
      "loss": 2.0495,
      "step": 10500
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.7660230994224548,
      "learning_rate": 0.0009633333333333334,
      "loss": 2.0189,
      "step": 11000
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.7910047769546509,
      "learning_rate": 0.0009616666666666667,
      "loss": 2.0054,
      "step": 11500
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.8109735250473022,
      "learning_rate": 0.00096,
      "loss": 1.9984,
      "step": 12000
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.8906494379043579,
      "learning_rate": 0.0009583333333333334,
      "loss": 1.9742,
      "step": 12500
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.7669569253921509,
      "learning_rate": 0.0009566666666666666,
      "loss": 1.9693,
      "step": 13000
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.8781254887580872,
      "learning_rate": 0.000955,
      "loss": 1.9649,
      "step": 13500
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.860998809337616,
      "learning_rate": 0.0009533333333333334,
      "loss": 1.9366,
      "step": 14000
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.8149706125259399,
      "learning_rate": 0.0009516666666666666,
      "loss": 1.9196,
      "step": 14500
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.8580487966537476,
      "learning_rate": 0.00095,
      "loss": 1.9206,
      "step": 15000
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.8429140448570251,
      "learning_rate": 0.0009483333333333334,
      "loss": 1.9082,
      "step": 15500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.778444230556488,
      "learning_rate": 0.0009466666666666667,
      "loss": 1.8978,
      "step": 16000
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.842940628528595,
      "learning_rate": 0.000945,
      "loss": 1.8788,
      "step": 16500
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.9122191667556763,
      "learning_rate": 0.0009433333333333334,
      "loss": 1.8664,
      "step": 17000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.789079487323761,
      "learning_rate": 0.0009416666666666667,
      "loss": 1.8447,
      "step": 17500
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.8142719864845276,
      "learning_rate": 0.00094,
      "loss": 1.8304,
      "step": 18000
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.8323712348937988,
      "learning_rate": 0.0009383333333333333,
      "loss": 1.8276,
      "step": 18500
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.7962679266929626,
      "learning_rate": 0.0009366666666666667,
      "loss": 1.8082,
      "step": 19000
    },
    {
      "epoch": 0.065,
      "grad_norm": 1.1406816244125366,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.8108,
      "step": 19500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.8697605133056641,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.8088,
      "step": 20000
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.9081452488899231,
      "learning_rate": 0.0009316666666666667,
      "loss": 1.8115,
      "step": 20500
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.4291996955871582,
      "learning_rate": 0.00093,
      "loss": 1.7839,
      "step": 21000
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 1.0401231050491333,
      "learning_rate": 0.0009283333333333333,
      "loss": 1.7661,
      "step": 21500
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.8535482883453369,
      "learning_rate": 0.0009266666666666667,
      "loss": 1.7532,
      "step": 22000
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.9193390011787415,
      "learning_rate": 0.000925,
      "loss": 1.7455,
      "step": 22500
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.9006972908973694,
      "learning_rate": 0.0009233333333333334,
      "loss": 1.7404,
      "step": 23000
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 1.897673487663269,
      "learning_rate": 0.0009216666666666667,
      "loss": 1.7413,
      "step": 23500
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.0200986862182617,
      "learning_rate": 0.00092,
      "loss": 1.7321,
      "step": 24000
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.8214194774627686,
      "learning_rate": 0.0009183333333333334,
      "loss": 1.7145,
      "step": 24500
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.9754721522331238,
      "learning_rate": 0.0009166666666666666,
      "loss": 1.7167,
      "step": 25000
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.9388805627822876,
      "learning_rate": 0.000915,
      "loss": 1.7252,
      "step": 25500
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.95396488904953,
      "learning_rate": 0.0009133333333333334,
      "loss": 1.7009,
      "step": 26000
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 1.13447904586792,
      "learning_rate": 0.0009116666666666666,
      "loss": 1.6962,
      "step": 26500
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.8911749124526978,
      "learning_rate": 0.00091,
      "loss": 1.675,
      "step": 27000
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.9429997205734253,
      "learning_rate": 0.0009083333333333334,
      "loss": 1.6915,
      "step": 27500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.8621017932891846,
      "learning_rate": 0.0009066666666666666,
      "loss": 1.6686,
      "step": 28000
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.955685019493103,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.6582,
      "step": 28500
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.944513201713562,
      "learning_rate": 0.0009033333333333334,
      "loss": 1.6931,
      "step": 29000
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.8557716012001038,
      "learning_rate": 0.0009016666666666666,
      "loss": 1.653,
      "step": 29500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.9315153956413269,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.6411,
      "step": 30000
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 1.039352297782898,
      "learning_rate": 0.0008983333333333333,
      "loss": 1.6326,
      "step": 30500
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 1.0800373554229736,
      "learning_rate": 0.0008966666666666666,
      "loss": 1.632,
      "step": 31000
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.868090033531189,
      "learning_rate": 0.0008950000000000001,
      "loss": 1.6132,
      "step": 31500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.9386665225028992,
      "learning_rate": 0.0008933333333333333,
      "loss": 1.6258,
      "step": 32000
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.9881996512413025,
      "learning_rate": 0.0008916666666666667,
      "loss": 1.5891,
      "step": 32500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.9697954058647156,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.5807,
      "step": 33000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.9628074765205383,
      "learning_rate": 0.0008883333333333333,
      "loss": 1.605,
      "step": 33500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 1.0503994226455688,
      "learning_rate": 0.0008866666666666667,
      "loss": 1.5958,
      "step": 34000
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.9546058773994446,
      "learning_rate": 0.000885,
      "loss": 1.5948,
      "step": 34500
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.0694688558578491,
      "learning_rate": 0.0008833333333333333,
      "loss": 1.5629,
      "step": 35000
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.9664521217346191,
      "learning_rate": 0.0008816666666666668,
      "loss": 1.5621,
      "step": 35500
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9310898184776306,
      "learning_rate": 0.00088,
      "loss": 1.5742,
      "step": 36000
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 1.0388662815093994,
      "learning_rate": 0.0008783333333333333,
      "loss": 1.5645,
      "step": 36500
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.9525671005249023,
      "learning_rate": 0.0008766666666666668,
      "loss": 1.5835,
      "step": 37000
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.9514592289924622,
      "learning_rate": 0.000875,
      "loss": 1.558,
      "step": 37500
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.9368456602096558,
      "learning_rate": 0.0008733333333333333,
      "loss": 1.5394,
      "step": 38000
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.9298399686813354,
      "learning_rate": 0.0008716666666666667,
      "loss": 1.5485,
      "step": 38500
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.0405867099761963,
      "learning_rate": 0.00087,
      "loss": 1.5483,
      "step": 39000
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.9292435646057129,
      "learning_rate": 0.0008683333333333333,
      "loss": 1.5476,
      "step": 39500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.9674205780029297,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.542,
      "step": 40000
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.9190806150436401,
      "learning_rate": 0.000865,
      "loss": 1.5055,
      "step": 40500
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.9253895878791809,
      "learning_rate": 0.0008633333333333334,
      "loss": 1.5233,
      "step": 41000
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.8696349859237671,
      "learning_rate": 0.0008616666666666667,
      "loss": 1.5071,
      "step": 41500
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9785268902778625,
      "learning_rate": 0.00086,
      "loss": 1.5086,
      "step": 42000
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.8841969966888428,
      "learning_rate": 0.0008583333333333333,
      "loss": 1.5031,
      "step": 42500
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.9725237488746643,
      "learning_rate": 0.0008566666666666667,
      "loss": 1.4819,
      "step": 43000
    },
    {
      "epoch": 0.145,
      "grad_norm": 1.118957281112671,
      "learning_rate": 0.000855,
      "loss": 1.4851,
      "step": 43500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.9113378524780273,
      "learning_rate": 0.0008533333333333334,
      "loss": 1.484,
      "step": 44000
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.9779229164123535,
      "learning_rate": 0.0008516666666666667,
      "loss": 1.4741,
      "step": 44500
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.8975793719291687,
      "learning_rate": 0.00085,
      "loss": 1.4664,
      "step": 45000
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.9764734506607056,
      "learning_rate": 0.0008483333333333334,
      "loss": 1.4831,
      "step": 45500
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.9981516599655151,
      "learning_rate": 0.0008466666666666667,
      "loss": 1.4685,
      "step": 46000
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.936066210269928,
      "learning_rate": 0.0008449999999999999,
      "loss": 1.4645,
      "step": 46500
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.9116196036338806,
      "learning_rate": 0.0008433333333333334,
      "loss": 1.4616,
      "step": 47000
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.0406516790390015,
      "learning_rate": 0.0008416666666666667,
      "loss": 1.458,
      "step": 47500
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.019428014755249,
      "learning_rate": 0.00084,
      "loss": 1.4729,
      "step": 48000
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 1.003953218460083,
      "learning_rate": 0.0008383333333333334,
      "loss": 1.455,
      "step": 48500
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 1.0243335962295532,
      "learning_rate": 0.0008366666666666667,
      "loss": 1.4355,
      "step": 49000
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.9076284766197205,
      "learning_rate": 0.000835,
      "loss": 1.4439,
      "step": 49500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.0536261796951294,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.4254,
      "step": 50000
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.8636842966079712,
      "learning_rate": 0.0008316666666666666,
      "loss": 1.4251,
      "step": 50500
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.9175359010696411,
      "learning_rate": 0.00083,
      "loss": 1.4165,
      "step": 51000
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 1.0685445070266724,
      "learning_rate": 0.0008283333333333334,
      "loss": 1.4161,
      "step": 51500
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.9362931847572327,
      "learning_rate": 0.0008266666666666666,
      "loss": 1.413,
      "step": 52000
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.2082042694091797,
      "learning_rate": 0.000825,
      "loss": 1.4085,
      "step": 52500
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.9101240038871765,
      "learning_rate": 0.0008233333333333334,
      "loss": 1.4008,
      "step": 53000
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 1.0337027311325073,
      "learning_rate": 0.0008216666666666667,
      "loss": 1.4141,
      "step": 53500
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.0465047359466553,
      "learning_rate": 0.00082,
      "loss": 1.4043,
      "step": 54000
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.8858733773231506,
      "learning_rate": 0.0008183333333333333,
      "loss": 1.3934,
      "step": 54500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.1050583124160767,
      "learning_rate": 0.0008166666666666667,
      "loss": 1.3922,
      "step": 55000
    },
    {
      "epoch": 0.185,
      "grad_norm": 0.969018816947937,
      "learning_rate": 0.000815,
      "loss": 1.4107,
      "step": 55500
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 1.0872313976287842,
      "learning_rate": 0.0008133333333333333,
      "loss": 1.3834,
      "step": 56000
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 1.0264407396316528,
      "learning_rate": 0.0008116666666666667,
      "loss": 1.3814,
      "step": 56500
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.9283663630485535,
      "learning_rate": 0.0008100000000000001,
      "loss": 1.3833,
      "step": 57000
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.045661449432373,
      "learning_rate": 0.0008083333333333333,
      "loss": 1.3673,
      "step": 57500
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 1.0285639762878418,
      "learning_rate": 0.0008066666666666667,
      "loss": 1.3697,
      "step": 58000
    },
    {
      "epoch": 0.195,
      "grad_norm": 1.0925381183624268,
      "learning_rate": 0.000805,
      "loss": 1.3605,
      "step": 58500
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.8960201740264893,
      "learning_rate": 0.0008033333333333333,
      "loss": 1.3507,
      "step": 59000
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.9483785629272461,
      "learning_rate": 0.0008016666666666667,
      "loss": 1.3479,
      "step": 59500
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.0343114137649536,
      "learning_rate": 0.0008,
      "loss": 1.3496,
      "step": 60000
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.9194756150245667,
      "learning_rate": 0.0007983333333333334,
      "loss": 1.3488,
      "step": 60500
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.9449324011802673,
      "learning_rate": 0.0007966666666666667,
      "loss": 1.3261,
      "step": 61000
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.9220768809318542,
      "learning_rate": 0.000795,
      "loss": 1.332,
      "step": 61500
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.9888561964035034,
      "learning_rate": 0.0007933333333333334,
      "loss": 1.3364,
      "step": 62000
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.0743476152420044,
      "learning_rate": 0.0007916666666666666,
      "loss": 1.3347,
      "step": 62500
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.9439383149147034,
      "learning_rate": 0.00079,
      "loss": 1.3264,
      "step": 63000
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 1.0916049480438232,
      "learning_rate": 0.0007883333333333334,
      "loss": 1.3478,
      "step": 63500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.9960482120513916,
      "learning_rate": 0.0007866666666666666,
      "loss": 1.3273,
      "step": 64000
    },
    {
      "epoch": 0.215,
      "grad_norm": 1.003000259399414,
      "learning_rate": 0.000785,
      "loss": 1.3284,
      "step": 64500
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.0116784572601318,
      "learning_rate": 0.0007833333333333334,
      "loss": 1.3289,
      "step": 65000
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 1.016558051109314,
      "learning_rate": 0.0007816666666666666,
      "loss": 1.3231,
      "step": 65500
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.873592734336853,
      "learning_rate": 0.0007800000000000001,
      "loss": 1.2932,
      "step": 66000
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 1.08098304271698,
      "learning_rate": 0.0007783333333333334,
      "loss": 1.313,
      "step": 66500
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 1.0379613637924194,
      "learning_rate": 0.0007766666666666666,
      "loss": 1.303,
      "step": 67000
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.100940227508545,
      "learning_rate": 0.0007750000000000001,
      "loss": 1.2984,
      "step": 67500
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.1688249111175537,
      "learning_rate": 0.0007733333333333333,
      "loss": 1.3048,
      "step": 68000
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 1.0635710954666138,
      "learning_rate": 0.0007716666666666666,
      "loss": 1.2893,
      "step": 68500
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.077765941619873,
      "learning_rate": 0.0007700000000000001,
      "loss": 1.3016,
      "step": 69000
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 1.2279577255249023,
      "learning_rate": 0.0007683333333333333,
      "loss": 1.3026,
      "step": 69500
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.0323991775512695,
      "learning_rate": 0.0007666666666666667,
      "loss": 1.295,
      "step": 70000
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.9995609521865845,
      "learning_rate": 0.0007650000000000001,
      "loss": 1.299,
      "step": 70500
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 1.1387648582458496,
      "learning_rate": 0.0007633333333333333,
      "loss": 1.2954,
      "step": 71000
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.9934678077697754,
      "learning_rate": 0.0007616666666666667,
      "loss": 1.2801,
      "step": 71500
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.0997694730758667,
      "learning_rate": 0.00076,
      "loss": 1.2887,
      "step": 72000
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.8869810700416565,
      "learning_rate": 0.0007583333333333333,
      "loss": 1.3009,
      "step": 72500
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 0.9900026917457581,
      "learning_rate": 0.0007566666666666668,
      "loss": 1.2795,
      "step": 73000
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.977488100528717,
      "learning_rate": 0.000755,
      "loss": 1.284,
      "step": 73500
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.9606797695159912,
      "learning_rate": 0.0007533333333333333,
      "loss": 1.2772,
      "step": 74000
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.9423437118530273,
      "learning_rate": 0.0007516666666666668,
      "loss": 1.2632,
      "step": 74500
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.8853288888931274,
      "learning_rate": 0.00075,
      "loss": 1.2639,
      "step": 75000
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 1.0588569641113281,
      "learning_rate": 0.0007483333333333333,
      "loss": 1.258,
      "step": 75500
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 1.006381869316101,
      "learning_rate": 0.0007466666666666667,
      "loss": 1.245,
      "step": 76000
    },
    {
      "epoch": 0.255,
      "grad_norm": 1.037811279296875,
      "learning_rate": 0.000745,
      "loss": 1.2666,
      "step": 76500
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 1.1955734491348267,
      "learning_rate": 0.0007433333333333333,
      "loss": 1.2253,
      "step": 77000
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.9568477869033813,
      "learning_rate": 0.0007416666666666667,
      "loss": 1.2639,
      "step": 77500
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.861428439617157,
      "learning_rate": 0.00074,
      "loss": 1.2605,
      "step": 78000
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 1.139602780342102,
      "learning_rate": 0.0007383333333333334,
      "loss": 1.2347,
      "step": 78500
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 1.035681962966919,
      "learning_rate": 0.0007366666666666667,
      "loss": 1.2418,
      "step": 79000
    },
    {
      "epoch": 0.265,
      "grad_norm": 1.0098193883895874,
      "learning_rate": 0.000735,
      "loss": 1.2153,
      "step": 79500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.3878612518310547,
      "learning_rate": 0.0007333333333333333,
      "loss": 1.2308,
      "step": 80000
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 1.0001606941223145,
      "learning_rate": 0.0007316666666666667,
      "loss": 1.2285,
      "step": 80500
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.0076570510864258,
      "learning_rate": 0.00073,
      "loss": 1.2435,
      "step": 81000
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.9479526281356812,
      "learning_rate": 0.0007283333333333334,
      "loss": 1.227,
      "step": 81500
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.8699554204940796,
      "learning_rate": 0.0007266666666666667,
      "loss": 1.2547,
      "step": 82000
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.0234028100967407,
      "learning_rate": 0.000725,
      "loss": 1.2113,
      "step": 82500
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 1.0605173110961914,
      "learning_rate": 0.0007233333333333334,
      "loss": 1.2133,
      "step": 83000
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 1.0373423099517822,
      "learning_rate": 0.0007216666666666667,
      "loss": 1.2024,
      "step": 83500
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.9527626037597656,
      "learning_rate": 0.0007199999999999999,
      "loss": 1.2157,
      "step": 84000
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 1.0385935306549072,
      "learning_rate": 0.0007183333333333334,
      "loss": 1.2039,
      "step": 84500
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.291468620300293,
      "learning_rate": 0.0007166666666666667,
      "loss": 1.1845,
      "step": 85000
    },
    {
      "epoch": 0.285,
      "grad_norm": 1.0217030048370361,
      "learning_rate": 0.000715,
      "loss": 1.1965,
      "step": 85500
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.052870273590088,
      "learning_rate": 0.0007133333333333334,
      "loss": 1.2044,
      "step": 86000
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.8815146684646606,
      "learning_rate": 0.0007116666666666667,
      "loss": 1.2192,
      "step": 86500
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.007524847984314,
      "learning_rate": 0.00071,
      "loss": 1.2083,
      "step": 87000
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.1784663200378418,
      "learning_rate": 0.0007083333333333334,
      "loss": 1.2125,
      "step": 87500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 1.0781466960906982,
      "learning_rate": 0.0007066666666666666,
      "loss": 1.1945,
      "step": 88000
    },
    {
      "epoch": 0.295,
      "grad_norm": 1.1645253896713257,
      "learning_rate": 0.000705,
      "loss": 1.2005,
      "step": 88500
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.882684588432312,
      "learning_rate": 0.0007033333333333334,
      "loss": 1.185,
      "step": 89000
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.8505842685699463,
      "learning_rate": 0.0007016666666666666,
      "loss": 1.1809,
      "step": 89500
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.9822114109992981,
      "learning_rate": 0.0007,
      "loss": 1.2096,
      "step": 90000
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 1.0882282257080078,
      "learning_rate": 0.0006983333333333334,
      "loss": 1.1968,
      "step": 90500
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 1.0282924175262451,
      "learning_rate": 0.0006966666666666667,
      "loss": 1.1832,
      "step": 91000
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.9753247499465942,
      "learning_rate": 0.000695,
      "loss": 1.1564,
      "step": 91500
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.9016534686088562,
      "learning_rate": 0.0006933333333333333,
      "loss": 1.1698,
      "step": 92000
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.8446000814437866,
      "learning_rate": 0.0006916666666666667,
      "loss": 1.1879,
      "step": 92500
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.9660051465034485,
      "learning_rate": 0.00069,
      "loss": 1.171,
      "step": 93000
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 1.0704026222229004,
      "learning_rate": 0.0006883333333333333,
      "loss": 1.1625,
      "step": 93500
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 1.070731520652771,
      "learning_rate": 0.0006866666666666667,
      "loss": 1.1744,
      "step": 94000
    },
    {
      "epoch": 0.315,
      "grad_norm": 1.1032729148864746,
      "learning_rate": 0.0006850000000000001,
      "loss": 1.1794,
      "step": 94500
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.9473346471786499,
      "learning_rate": 0.0006833333333333333,
      "loss": 1.1701,
      "step": 95000
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.8411795496940613,
      "learning_rate": 0.0006816666666666667,
      "loss": 1.1755,
      "step": 95500
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.9679417014122009,
      "learning_rate": 0.00068,
      "loss": 1.1732,
      "step": 96000
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 1.034650444984436,
      "learning_rate": 0.0006783333333333333,
      "loss": 1.1545,
      "step": 96500
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 1.083620548248291,
      "learning_rate": 0.0006766666666666667,
      "loss": 1.1749,
      "step": 97000
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.9690753817558289,
      "learning_rate": 0.000675,
      "loss": 1.1866,
      "step": 97500
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 1.1261452436447144,
      "learning_rate": 0.0006733333333333334,
      "loss": 1.1571,
      "step": 98000
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 1.0648037195205688,
      "learning_rate": 0.0006716666666666667,
      "loss": 1.1673,
      "step": 98500
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.936797559261322,
      "learning_rate": 0.00067,
      "loss": 1.1664,
      "step": 99000
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 1.0559964179992676,
      "learning_rate": 0.0006683333333333334,
      "loss": 1.1538,
      "step": 99500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.083245873451233,
      "learning_rate": 0.0006666666666666666,
      "loss": 1.1494,
      "step": 100000
    },
    {
      "epoch": 0.335,
      "grad_norm": 1.1186336278915405,
      "learning_rate": 0.000665,
      "loss": 1.1259,
      "step": 100500
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.8890332579612732,
      "learning_rate": 0.0006633333333333334,
      "loss": 1.1365,
      "step": 101000
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 1.0456637144088745,
      "learning_rate": 0.0006616666666666666,
      "loss": 1.1276,
      "step": 101500
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.9813313484191895,
      "learning_rate": 0.00066,
      "loss": 1.1219,
      "step": 102000
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 1.086527705192566,
      "learning_rate": 0.0006583333333333334,
      "loss": 1.1613,
      "step": 102500
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 1.1273479461669922,
      "learning_rate": 0.0006566666666666666,
      "loss": 1.1313,
      "step": 103000
    },
    {
      "epoch": 0.345,
      "grad_norm": 1.2466832399368286,
      "learning_rate": 0.0006550000000000001,
      "loss": 1.1247,
      "step": 103500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.863749086856842,
      "learning_rate": 0.0006533333333333333,
      "loss": 1.1111,
      "step": 104000
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.9307501316070557,
      "learning_rate": 0.0006516666666666666,
      "loss": 1.1353,
      "step": 104500
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.223544716835022,
      "learning_rate": 0.0006500000000000001,
      "loss": 1.1171,
      "step": 105000
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 1.7597463130950928,
      "learning_rate": 0.0006483333333333333,
      "loss": 1.1172,
      "step": 105500
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 1.0324615240097046,
      "learning_rate": 0.0006466666666666666,
      "loss": 1.1259,
      "step": 106000
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.98160719871521,
      "learning_rate": 0.0006450000000000001,
      "loss": 1.1176,
      "step": 106500
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.9975993037223816,
      "learning_rate": 0.0006433333333333333,
      "loss": 1.1265,
      "step": 107000
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.9347502589225769,
      "learning_rate": 0.0006416666666666667,
      "loss": 1.108,
      "step": 107500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.050485610961914,
      "learning_rate": 0.00064,
      "loss": 1.1322,
      "step": 108000
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 1.0128610134124756,
      "learning_rate": 0.0006383333333333333,
      "loss": 1.108,
      "step": 108500
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 1.0713032484054565,
      "learning_rate": 0.0006366666666666667,
      "loss": 1.1171,
      "step": 109000
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.9320716261863708,
      "learning_rate": 0.000635,
      "loss": 1.0958,
      "step": 109500
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.9231444001197815,
      "learning_rate": 0.0006333333333333333,
      "loss": 1.1026,
      "step": 110000
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.9766952991485596,
      "learning_rate": 0.0006316666666666668,
      "loss": 1.1147,
      "step": 110500
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.9259927868843079,
      "learning_rate": 0.00063,
      "loss": 1.1116,
      "step": 111000
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.9608580470085144,
      "learning_rate": 0.0006283333333333333,
      "loss": 1.1114,
      "step": 111500
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.7759478688240051,
      "learning_rate": 0.0006266666666666668,
      "loss": 1.1115,
      "step": 112000
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.9245233535766602,
      "learning_rate": 0.000625,
      "loss": 1.0993,
      "step": 112500
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 1.0268282890319824,
      "learning_rate": 0.0006233333333333333,
      "loss": 1.0877,
      "step": 113000
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 1.1040445566177368,
      "learning_rate": 0.0006216666666666667,
      "loss": 1.1119,
      "step": 113500
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.0026971101760864,
      "learning_rate": 0.00062,
      "loss": 1.1065,
      "step": 114000
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.8645731210708618,
      "learning_rate": 0.0006183333333333333,
      "loss": 1.1086,
      "step": 114500
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.2695618867874146,
      "learning_rate": 0.0006166666666666667,
      "loss": 1.0983,
      "step": 115000
    },
    {
      "epoch": 0.385,
      "grad_norm": 1.0122100114822388,
      "learning_rate": 0.000615,
      "loss": 1.092,
      "step": 115500
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.866367757320404,
      "learning_rate": 0.0006133333333333334,
      "loss": 1.0888,
      "step": 116000
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 1.4225058555603027,
      "learning_rate": 0.0006116666666666667,
      "loss": 1.08,
      "step": 116500
    },
    {
      "epoch": 0.39,
      "grad_norm": 1.1145530939102173,
      "learning_rate": 0.00061,
      "loss": 1.0748,
      "step": 117000
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.9830820560455322,
      "learning_rate": 0.0006083333333333333,
      "loss": 1.0977,
      "step": 117500
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 1.1361579895019531,
      "learning_rate": 0.0006066666666666667,
      "loss": 1.1215,
      "step": 118000
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.8481490015983582,
      "learning_rate": 0.000605,
      "loss": 1.0868,
      "step": 118500
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 0.8244752883911133,
      "learning_rate": 0.0006033333333333334,
      "loss": 1.0862,
      "step": 119000
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 0.9697059392929077,
      "learning_rate": 0.0006016666666666667,
      "loss": 1.0761,
      "step": 119500
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.0538649559020996,
      "learning_rate": 0.0006,
      "loss": 1.1,
      "step": 120000
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 1.1454650163650513,
      "learning_rate": 0.0005983333333333334,
      "loss": 1.0841,
      "step": 120500
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.9222113490104675,
      "learning_rate": 0.0005966666666666667,
      "loss": 1.0754,
      "step": 121000
    },
    {
      "epoch": 0.405,
      "grad_norm": 1.2011466026306152,
      "learning_rate": 0.0005949999999999999,
      "loss": 1.0701,
      "step": 121500
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.9183750748634338,
      "learning_rate": 0.0005933333333333334,
      "loss": 1.0685,
      "step": 122000
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.0377312898635864,
      "learning_rate": 0.0005916666666666667,
      "loss": 1.083,
      "step": 122500
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.946708619594574,
      "learning_rate": 0.00059,
      "loss": 1.0821,
      "step": 123000
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 1.0027682781219482,
      "learning_rate": 0.0005883333333333334,
      "loss": 1.0654,
      "step": 123500
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.039878249168396,
      "learning_rate": 0.0005866666666666667,
      "loss": 1.055,
      "step": 124000
    },
    {
      "epoch": 0.415,
      "grad_norm": 1.0001418590545654,
      "learning_rate": 0.000585,
      "loss": 1.0733,
      "step": 124500
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.9883396625518799,
      "learning_rate": 0.0005833333333333334,
      "loss": 1.0601,
      "step": 125000
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.8593915104866028,
      "learning_rate": 0.0005816666666666666,
      "loss": 1.087,
      "step": 125500
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.8604221343994141,
      "learning_rate": 0.00058,
      "loss": 1.0585,
      "step": 126000
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 0.9515163898468018,
      "learning_rate": 0.0005783333333333334,
      "loss": 1.0548,
      "step": 126500
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.9745625257492065,
      "learning_rate": 0.0005766666666666666,
      "loss": 1.0477,
      "step": 127000
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.9119746685028076,
      "learning_rate": 0.000575,
      "loss": 1.0529,
      "step": 127500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.9047435522079468,
      "learning_rate": 0.0005733333333333334,
      "loss": 1.0488,
      "step": 128000
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.9884381890296936,
      "learning_rate": 0.0005716666666666667,
      "loss": 1.0489,
      "step": 128500
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.999070942401886,
      "learning_rate": 0.00057,
      "loss": 1.077,
      "step": 129000
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.9267870783805847,
      "learning_rate": 0.0005683333333333333,
      "loss": 1.0484,
      "step": 129500
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.8506131768226624,
      "learning_rate": 0.0005666666666666667,
      "loss": 1.0584,
      "step": 130000
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.970471978187561,
      "learning_rate": 0.000565,
      "loss": 1.0316,
      "step": 130500
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 1.001664400100708,
      "learning_rate": 0.0005633333333333333,
      "loss": 1.0476,
      "step": 131000
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.8169705867767334,
      "learning_rate": 0.0005616666666666667,
      "loss": 1.0369,
      "step": 131500
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.9687957763671875,
      "learning_rate": 0.0005600000000000001,
      "loss": 1.0551,
      "step": 132000
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.9215945601463318,
      "learning_rate": 0.0005583333333333333,
      "loss": 1.0417,
      "step": 132500
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 0.8848296403884888,
      "learning_rate": 0.0005566666666666667,
      "loss": 1.0367,
      "step": 133000
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.956636905670166,
      "learning_rate": 0.000555,
      "loss": 1.0459,
      "step": 133500
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.8737824559211731,
      "learning_rate": 0.0005533333333333333,
      "loss": 1.0535,
      "step": 134000
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 0.9674819707870483,
      "learning_rate": 0.0005516666666666667,
      "loss": 1.038,
      "step": 134500
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.8351462483406067,
      "learning_rate": 0.00055,
      "loss": 1.0388,
      "step": 135000
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.9902333617210388,
      "learning_rate": 0.0005483333333333334,
      "loss": 1.0325,
      "step": 135500
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.7981125712394714,
      "learning_rate": 0.0005466666666666667,
      "loss": 1.0206,
      "step": 136000
    },
    {
      "epoch": 0.455,
      "grad_norm": 1.0012836456298828,
      "learning_rate": 0.000545,
      "loss": 1.0243,
      "step": 136500
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 1.000366449356079,
      "learning_rate": 0.0005433333333333334,
      "loss": 1.0148,
      "step": 137000
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 1.2281814813613892,
      "learning_rate": 0.0005416666666666666,
      "loss": 1.0389,
      "step": 137500
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.1655163764953613,
      "learning_rate": 0.00054,
      "loss": 1.0361,
      "step": 138000
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.9905938506126404,
      "learning_rate": 0.0005383333333333334,
      "loss": 1.0094,
      "step": 138500
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 0.8755276799201965,
      "learning_rate": 0.0005366666666666666,
      "loss": 1.0241,
      "step": 139000
    },
    {
      "epoch": 0.465,
      "grad_norm": 1.019909381866455,
      "learning_rate": 0.000535,
      "loss": 1.0063,
      "step": 139500
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.9451307058334351,
      "learning_rate": 0.0005333333333333334,
      "loss": 1.05,
      "step": 140000
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.8503841161727905,
      "learning_rate": 0.0005316666666666666,
      "loss": 1.031,
      "step": 140500
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.899127721786499,
      "learning_rate": 0.0005300000000000001,
      "loss": 1.0313,
      "step": 141000
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.8584011197090149,
      "learning_rate": 0.0005283333333333333,
      "loss": 1.0344,
      "step": 141500
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.904614269733429,
      "learning_rate": 0.0005266666666666666,
      "loss": 1.0194,
      "step": 142000
    },
    {
      "epoch": 0.475,
      "grad_norm": 1.039871096611023,
      "learning_rate": 0.0005250000000000001,
      "loss": 0.9915,
      "step": 142500
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 1.0132337808609009,
      "learning_rate": 0.0005233333333333333,
      "loss": 1.0239,
      "step": 143000
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 0.8637731075286865,
      "learning_rate": 0.0005216666666666666,
      "loss": 1.0112,
      "step": 143500
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.9038057327270508,
      "learning_rate": 0.0005200000000000001,
      "loss": 1.0237,
      "step": 144000
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 1.0058585405349731,
      "learning_rate": 0.0005183333333333333,
      "loss": 1.0142,
      "step": 144500
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.0894362926483154,
      "learning_rate": 0.0005166666666666667,
      "loss": 1.0074,
      "step": 145000
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.876958429813385,
      "learning_rate": 0.000515,
      "loss": 1.025,
      "step": 145500
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 0.9308615326881409,
      "learning_rate": 0.0005133333333333333,
      "loss": 1.0014,
      "step": 146000
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.9047605395317078,
      "learning_rate": 0.0005116666666666667,
      "loss": 1.0079,
      "step": 146500
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.8953337669372559,
      "learning_rate": 0.00051,
      "loss": 0.9978,
      "step": 147000
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.9933213591575623,
      "learning_rate": 0.0005083333333333333,
      "loss": 0.9903,
      "step": 147500
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 1.0183789730072021,
      "learning_rate": 0.0005066666666666668,
      "loss": 0.9887,
      "step": 148000
    },
    {
      "epoch": 0.495,
      "grad_norm": 0.8577847480773926,
      "learning_rate": 0.000505,
      "loss": 1.0283,
      "step": 148500
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 1.0263293981552124,
      "learning_rate": 0.0005033333333333333,
      "loss": 1.0175,
      "step": 149000
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.9743887186050415,
      "learning_rate": 0.0005016666666666668,
      "loss": 1.0154,
      "step": 149500
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.0093353986740112,
      "learning_rate": 0.0005,
      "loss": 0.9982,
      "step": 150000
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 0.8843191862106323,
      "learning_rate": 0.0004983333333333334,
      "loss": 1.0167,
      "step": 150500
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 0.8759251832962036,
      "learning_rate": 0.0004966666666666666,
      "loss": 1.0116,
      "step": 151000
    },
    {
      "epoch": 0.505,
      "grad_norm": 0.8862352967262268,
      "learning_rate": 0.000495,
      "loss": 1.0142,
      "step": 151500
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.9992942214012146,
      "learning_rate": 0.0004933333333333334,
      "loss": 0.9965,
      "step": 152000
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.020437240600586,
      "learning_rate": 0.0004916666666666666,
      "loss": 0.9881,
      "step": 152500
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.008467674255371,
      "learning_rate": 0.00049,
      "loss": 0.9879,
      "step": 153000
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.9285211563110352,
      "learning_rate": 0.0004883333333333333,
      "loss": 1.0113,
      "step": 153500
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 1.1134214401245117,
      "learning_rate": 0.0004866666666666667,
      "loss": 0.9926,
      "step": 154000
    },
    {
      "epoch": 0.515,
      "grad_norm": 1.0124293565750122,
      "learning_rate": 0.00048499999999999997,
      "loss": 1.0001,
      "step": 154500
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.0355983972549438,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.9925,
      "step": 155000
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 0.9174625277519226,
      "learning_rate": 0.0004816666666666667,
      "loss": 1.0036,
      "step": 155500
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.9258151650428772,
      "learning_rate": 0.00048,
      "loss": 0.995,
      "step": 156000
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.9317653179168701,
      "learning_rate": 0.0004783333333333333,
      "loss": 0.9851,
      "step": 156500
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 0.9267379641532898,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.9817,
      "step": 157000
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.0495057106018066,
      "learning_rate": 0.000475,
      "loss": 0.9836,
      "step": 157500
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 1.1499489545822144,
      "learning_rate": 0.00047333333333333336,
      "loss": 0.9699,
      "step": 158000
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 1.0260099172592163,
      "learning_rate": 0.0004716666666666667,
      "loss": 1.0062,
      "step": 158500
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.9032963514328003,
      "learning_rate": 0.00047,
      "loss": 0.9938,
      "step": 159000
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 0.8891928791999817,
      "learning_rate": 0.00046833333333333335,
      "loss": 0.9745,
      "step": 159500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.9035074710845947,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.9786,
      "step": 160000
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.9585856199264526,
      "learning_rate": 0.000465,
      "loss": 0.9831,
      "step": 160500
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.8703633546829224,
      "learning_rate": 0.00046333333333333334,
      "loss": 0.9764,
      "step": 161000
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.7880406379699707,
      "learning_rate": 0.0004616666666666667,
      "loss": 0.9954,
      "step": 161500
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.8329188823699951,
      "learning_rate": 0.00046,
      "loss": 0.9853,
      "step": 162000
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.9440017342567444,
      "learning_rate": 0.0004583333333333333,
      "loss": 0.9805,
      "step": 162500
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 0.9668664932250977,
      "learning_rate": 0.0004566666666666667,
      "loss": 0.9691,
      "step": 163000
    },
    {
      "epoch": 0.545,
      "grad_norm": 1.0463117361068726,
      "learning_rate": 0.000455,
      "loss": 0.9791,
      "step": 163500
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.0637816190719604,
      "learning_rate": 0.0004533333333333333,
      "loss": 0.9726,
      "step": 164000
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.8759242296218872,
      "learning_rate": 0.0004516666666666667,
      "loss": 0.9572,
      "step": 164500
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.8473069071769714,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.9625,
      "step": 165000
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 0.8260942697525024,
      "learning_rate": 0.0004483333333333333,
      "loss": 0.9651,
      "step": 165500
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.9803804755210876,
      "learning_rate": 0.00044666666666666666,
      "loss": 0.992,
      "step": 166000
    },
    {
      "epoch": 0.555,
      "grad_norm": 0.9662245512008667,
      "learning_rate": 0.00044500000000000003,
      "loss": 0.977,
      "step": 166500
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 1.1470247507095337,
      "learning_rate": 0.00044333333333333334,
      "loss": 0.9805,
      "step": 167000
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.8359174132347107,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.9808,
      "step": 167500
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.8951632976531982,
      "learning_rate": 0.00044,
      "loss": 0.9685,
      "step": 168000
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.9051480293273926,
      "learning_rate": 0.0004383333333333334,
      "loss": 0.9769,
      "step": 168500
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.8976870775222778,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.968,
      "step": 169000
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.9173280000686646,
      "learning_rate": 0.000435,
      "loss": 0.9532,
      "step": 169500
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.9810225963592529,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.951,
      "step": 170000
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.8444376587867737,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.9757,
      "step": 170500
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.8853909969329834,
      "learning_rate": 0.00043,
      "loss": 0.9446,
      "step": 171000
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.9866409301757812,
      "learning_rate": 0.00042833333333333335,
      "loss": 0.9657,
      "step": 171500
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.879313588142395,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.963,
      "step": 172000
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.9448009133338928,
      "learning_rate": 0.000425,
      "loss": 0.9739,
      "step": 172500
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.8682571053504944,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.9511,
      "step": 173000
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 0.9508662223815918,
      "learning_rate": 0.0004216666666666667,
      "loss": 0.9445,
      "step": 173500
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.966301441192627,
      "learning_rate": 0.00042,
      "loss": 0.943,
      "step": 174000
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 0.7886266708374023,
      "learning_rate": 0.00041833333333333333,
      "loss": 0.9521,
      "step": 174500
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.9345393776893616,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.9429,
      "step": 175000
    },
    {
      "epoch": 0.585,
      "grad_norm": 0.9254943132400513,
      "learning_rate": 0.000415,
      "loss": 0.9582,
      "step": 175500
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.890137255191803,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.9628,
      "step": 176000
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 1.037055492401123,
      "learning_rate": 0.0004116666666666667,
      "loss": 0.9636,
      "step": 176500
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.9873110055923462,
      "learning_rate": 0.00041,
      "loss": 0.9313,
      "step": 177000
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.87471604347229,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.9449,
      "step": 177500
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.9429104924201965,
      "learning_rate": 0.00040666666666666667,
      "loss": 0.9459,
      "step": 178000
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.8805084824562073,
      "learning_rate": 0.00040500000000000003,
      "loss": 0.9537,
      "step": 178500
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.876922607421875,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.9281,
      "step": 179000
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.8965575695037842,
      "learning_rate": 0.00040166666666666665,
      "loss": 0.9291,
      "step": 179500
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.8918547630310059,
      "learning_rate": 0.0004,
      "loss": 0.9591,
      "step": 180000
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 1.0050208568572998,
      "learning_rate": 0.00039833333333333333,
      "loss": 0.9355,
      "step": 180500
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.8556568026542664,
      "learning_rate": 0.0003966666666666667,
      "loss": 0.9341,
      "step": 181000
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.9083499312400818,
      "learning_rate": 0.000395,
      "loss": 0.938,
      "step": 181500
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.8590967655181885,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.953,
      "step": 182000
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.8902656435966492,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.943,
      "step": 182500
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.1039289236068726,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.9422,
      "step": 183000
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.8432528972625732,
      "learning_rate": 0.0003883333333333333,
      "loss": 0.9396,
      "step": 183500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.7695164084434509,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.9282,
      "step": 184000
    },
    {
      "epoch": 0.615,
      "grad_norm": 0.9772858023643494,
      "learning_rate": 0.00038500000000000003,
      "loss": 0.9433,
      "step": 184500
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.9384788274765015,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.9387,
      "step": 185000
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.8057194948196411,
      "learning_rate": 0.00038166666666666666,
      "loss": 0.9418,
      "step": 185500
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.9314069747924805,
      "learning_rate": 0.00038,
      "loss": 0.9317,
      "step": 186000
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.9096857309341431,
      "learning_rate": 0.0003783333333333334,
      "loss": 0.9342,
      "step": 186500
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.9461167454719543,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.9305,
      "step": 187000
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.9592215418815613,
      "learning_rate": 0.000375,
      "loss": 0.9301,
      "step": 187500
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.8648089170455933,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.9556,
      "step": 188000
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 1.0535942316055298,
      "learning_rate": 0.00037166666666666663,
      "loss": 0.9353,
      "step": 188500
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.3242690563201904,
      "learning_rate": 0.00037,
      "loss": 0.9569,
      "step": 189000
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 0.9296347498893738,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.9275,
      "step": 189500
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.9729810357093811,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.9204,
      "step": 190000
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.9803456664085388,
      "learning_rate": 0.000365,
      "loss": 0.9161,
      "step": 190500
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 0.7315589189529419,
      "learning_rate": 0.00036333333333333335,
      "loss": 0.9317,
      "step": 191000
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.7935696840286255,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.9346,
      "step": 191500
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.7881689071655273,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.9405,
      "step": 192000
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.8763378858566284,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.9411,
      "step": 192500
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 1.0662651062011719,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.9048,
      "step": 193000
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.9461055397987366,
      "learning_rate": 0.000355,
      "loss": 0.9465,
      "step": 193500
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.7227935791015625,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.9053,
      "step": 194000
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.902020275592804,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.9308,
      "step": 194500
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.4199820756912231,
      "learning_rate": 0.00035,
      "loss": 0.9299,
      "step": 195000
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 0.8474165797233582,
      "learning_rate": 0.00034833333333333336,
      "loss": 0.9403,
      "step": 195500
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.9818336367607117,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.9028,
      "step": 196000
    },
    {
      "epoch": 0.655,
      "grad_norm": 0.7769713997840881,
      "learning_rate": 0.000345,
      "loss": 0.8989,
      "step": 196500
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.7857721447944641,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.9409,
      "step": 197000
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.8776190280914307,
      "learning_rate": 0.00034166666666666666,
      "loss": 0.9226,
      "step": 197500
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.8064696788787842,
      "learning_rate": 0.00034,
      "loss": 0.9217,
      "step": 198000
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.8195945620536804,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.9242,
      "step": 198500
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.9068187475204468,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.9113,
      "step": 199000
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.8231279253959656,
      "learning_rate": 0.000335,
      "loss": 0.9237,
      "step": 199500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.9574490189552307,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.9149,
      "step": 200000
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.8950186371803284,
      "learning_rate": 0.0003316666666666667,
      "loss": 0.9009,
      "step": 200500
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.8415170311927795,
      "learning_rate": 0.00033,
      "loss": 0.9206,
      "step": 201000
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 0.9679372906684875,
      "learning_rate": 0.0003283333333333333,
      "loss": 0.9151,
      "step": 201500
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 0.766433596611023,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.9188,
      "step": 202000
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.8275516033172607,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.9211,
      "step": 202500
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.8363696932792664,
      "learning_rate": 0.0003233333333333333,
      "loss": 0.9068,
      "step": 203000
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 0.8515336513519287,
      "learning_rate": 0.00032166666666666666,
      "loss": 0.8944,
      "step": 203500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.8163334727287292,
      "learning_rate": 0.00032,
      "loss": 0.9312,
      "step": 204000
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 0.7452774047851562,
      "learning_rate": 0.00031833333333333334,
      "loss": 0.9221,
      "step": 204500
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.831689178943634,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.903,
      "step": 205000
    },
    {
      "epoch": 0.685,
      "grad_norm": 1.0121835470199585,
      "learning_rate": 0.000315,
      "loss": 0.9109,
      "step": 205500
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.9930310249328613,
      "learning_rate": 0.0003133333333333334,
      "loss": 0.9033,
      "step": 206000
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.983854353427887,
      "learning_rate": 0.00031166666666666663,
      "loss": 0.9004,
      "step": 206500
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.7307129502296448,
      "learning_rate": 0.00031,
      "loss": 0.8983,
      "step": 207000
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.9592564105987549,
      "learning_rate": 0.00030833333333333337,
      "loss": 0.9058,
      "step": 207500
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.8576548099517822,
      "learning_rate": 0.0003066666666666667,
      "loss": 0.8951,
      "step": 208000
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.8621569871902466,
      "learning_rate": 0.000305,
      "loss": 0.8773,
      "step": 208500
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 0.7914275527000427,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.8969,
      "step": 209000
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 0.8070884346961975,
      "learning_rate": 0.0003016666666666667,
      "loss": 0.9089,
      "step": 209500
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.8711205124855042,
      "learning_rate": 0.0003,
      "loss": 0.9027,
      "step": 210000
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.8186302185058594,
      "learning_rate": 0.00029833333333333334,
      "loss": 0.8934,
      "step": 210500
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 0.9761987924575806,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.9045,
      "step": 211000
    },
    {
      "epoch": 0.705,
      "grad_norm": 0.9909000992774963,
      "learning_rate": 0.000295,
      "loss": 0.9067,
      "step": 211500
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.8983033895492554,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.8929,
      "step": 212000
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.7553621530532837,
      "learning_rate": 0.0002916666666666667,
      "loss": 0.9058,
      "step": 212500
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.9079058170318604,
      "learning_rate": 0.00029,
      "loss": 0.8878,
      "step": 213000
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 0.79245525598526,
      "learning_rate": 0.0002883333333333333,
      "loss": 0.8912,
      "step": 213500
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 1.0465394258499146,
      "learning_rate": 0.0002866666666666667,
      "loss": 0.8943,
      "step": 214000
    },
    {
      "epoch": 0.715,
      "grad_norm": 0.7707652449607849,
      "learning_rate": 0.000285,
      "loss": 0.9005,
      "step": 214500
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.8105303049087524,
      "learning_rate": 0.00028333333333333335,
      "loss": 0.9067,
      "step": 215000
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 0.8453601002693176,
      "learning_rate": 0.00028166666666666666,
      "loss": 0.8984,
      "step": 215500
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.8187044858932495,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.8811,
      "step": 216000
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 1.0918207168579102,
      "learning_rate": 0.00027833333333333334,
      "loss": 0.8982,
      "step": 216500
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 0.9381600022315979,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.8873,
      "step": 217000
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.8383955359458923,
      "learning_rate": 0.000275,
      "loss": 0.9074,
      "step": 217500
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.7748206853866577,
      "learning_rate": 0.00027333333333333333,
      "loss": 0.899,
      "step": 218000
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.8774837851524353,
      "learning_rate": 0.0002716666666666667,
      "loss": 0.8772,
      "step": 218500
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.202093243598938,
      "learning_rate": 0.00027,
      "loss": 0.8664,
      "step": 219000
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.8663104772567749,
      "learning_rate": 0.0002683333333333333,
      "loss": 0.8881,
      "step": 219500
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.872292160987854,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.8912,
      "step": 220000
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.8900690674781799,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.8878,
      "step": 220500
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 1.0133659839630127,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.8818,
      "step": 221000
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 1.0511263608932495,
      "learning_rate": 0.00026166666666666667,
      "loss": 0.8874,
      "step": 221500
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.9216418862342834,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.8953,
      "step": 222000
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.8849994540214539,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.8784,
      "step": 222500
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 0.8163157105445862,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.907,
      "step": 223000
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.8629305958747864,
      "learning_rate": 0.000255,
      "loss": 0.8565,
      "step": 223500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.8518611788749695,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.897,
      "step": 224000
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 0.7653042078018188,
      "learning_rate": 0.00025166666666666664,
      "loss": 0.9018,
      "step": 224500
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.8572489619255066,
      "learning_rate": 0.00025,
      "loss": 0.8856,
      "step": 225000
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 0.9155727624893188,
      "learning_rate": 0.0002483333333333333,
      "loss": 0.8781,
      "step": 225500
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.8335657715797424,
      "learning_rate": 0.0002466666666666667,
      "loss": 0.8795,
      "step": 226000
    },
    {
      "epoch": 0.755,
      "grad_norm": 0.8885199427604675,
      "learning_rate": 0.000245,
      "loss": 0.8879,
      "step": 226500
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 0.8643656969070435,
      "learning_rate": 0.00024333333333333336,
      "loss": 0.8752,
      "step": 227000
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.914594829082489,
      "learning_rate": 0.00024166666666666667,
      "loss": 0.8806,
      "step": 227500
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.8142217397689819,
      "learning_rate": 0.00024,
      "loss": 0.8835,
      "step": 228000
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 0.9130910038948059,
      "learning_rate": 0.00023833333333333334,
      "loss": 0.8717,
      "step": 228500
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 1.0513182878494263,
      "learning_rate": 0.00023666666666666668,
      "loss": 0.8858,
      "step": 229000
    },
    {
      "epoch": 0.765,
      "grad_norm": 0.9883931279182434,
      "learning_rate": 0.000235,
      "loss": 0.89,
      "step": 229500
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.7845582962036133,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.8778,
      "step": 230000
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 0.9550590515136719,
      "learning_rate": 0.00023166666666666667,
      "loss": 0.8736,
      "step": 230500
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.792184054851532,
      "learning_rate": 0.00023,
      "loss": 0.8724,
      "step": 231000
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.8143040537834167,
      "learning_rate": 0.00022833333333333334,
      "loss": 0.8758,
      "step": 231500
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.9019837975502014,
      "learning_rate": 0.00022666666666666666,
      "loss": 0.867,
      "step": 232000
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.9435248970985413,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.8602,
      "step": 232500
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 0.6888678073883057,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.8794,
      "step": 233000
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 1.0904098749160767,
      "learning_rate": 0.00022166666666666667,
      "loss": 0.873,
      "step": 233500
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.8083070516586304,
      "learning_rate": 0.00022,
      "loss": 0.8539,
      "step": 234000
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 0.7247563004493713,
      "learning_rate": 0.00021833333333333332,
      "loss": 0.843,
      "step": 234500
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.8253049254417419,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.8713,
      "step": 235000
    },
    {
      "epoch": 0.785,
      "grad_norm": 0.7620580196380615,
      "learning_rate": 0.000215,
      "loss": 0.8808,
      "step": 235500
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.7033629417419434,
      "learning_rate": 0.00021333333333333336,
      "loss": 0.8603,
      "step": 236000
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.8962076902389526,
      "learning_rate": 0.00021166666666666667,
      "loss": 0.8796,
      "step": 236500
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.7668178081512451,
      "learning_rate": 0.00021,
      "loss": 0.8805,
      "step": 237000
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.7641617655754089,
      "learning_rate": 0.00020833333333333335,
      "loss": 0.8551,
      "step": 237500
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 0.8901140093803406,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.8718,
      "step": 238000
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.7831805944442749,
      "learning_rate": 0.000205,
      "loss": 0.8702,
      "step": 238500
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 0.8038454055786133,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.8709,
      "step": 239000
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 0.9277320504188538,
      "learning_rate": 0.00020166666666666667,
      "loss": 0.8806,
      "step": 239500
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9216025471687317,
      "learning_rate": 0.0002,
      "loss": 0.8814,
      "step": 240000
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 0.885222315788269,
      "learning_rate": 0.00019833333333333335,
      "loss": 0.8564,
      "step": 240500
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 0.9179432392120361,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.868,
      "step": 241000
    },
    {
      "epoch": 0.805,
      "grad_norm": 0.856759250164032,
      "learning_rate": 0.00019500000000000002,
      "loss": 0.8844,
      "step": 241500
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.8036414980888367,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.8689,
      "step": 242000
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.9345080852508545,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.8707,
      "step": 242500
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.8026953339576721,
      "learning_rate": 0.00019,
      "loss": 0.8698,
      "step": 243000
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.9144967794418335,
      "learning_rate": 0.00018833333333333332,
      "loss": 0.8792,
      "step": 243500
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.9282362461090088,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.8711,
      "step": 244000
    },
    {
      "epoch": 0.815,
      "grad_norm": 0.8736507892608643,
      "learning_rate": 0.000185,
      "loss": 0.8573,
      "step": 244500
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.9074607491493225,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.8491,
      "step": 245000
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 0.757023811340332,
      "learning_rate": 0.00018166666666666667,
      "loss": 0.8605,
      "step": 245500
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.7987322807312012,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.8733,
      "step": 246000
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 0.7490432262420654,
      "learning_rate": 0.00017833333333333335,
      "loss": 0.8561,
      "step": 246500
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.7396714091300964,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.8731,
      "step": 247000
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.9330662488937378,
      "learning_rate": 0.000175,
      "loss": 0.8691,
      "step": 247500
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.9956265091896057,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.8665,
      "step": 248000
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 0.8065199255943298,
      "learning_rate": 0.00017166666666666667,
      "loss": 0.8584,
      "step": 248500
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.895201563835144,
      "learning_rate": 0.00017,
      "loss": 0.8535,
      "step": 249000
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.7510488033294678,
      "learning_rate": 0.00016833333333333335,
      "loss": 0.8562,
      "step": 249500
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.9118993282318115,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.8749,
      "step": 250000
    }
  ],
  "logging_steps": 500,
  "max_steps": 300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 50000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.084849127424e+18,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
