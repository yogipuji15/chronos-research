{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonts in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy<2.2,>=1.16 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts) (1.23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.0 in /home/yogi/.local/lib/python3.10/site-packages (from gluonts) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts) (2.9.2)\n",
      "Requirement already satisfied: tqdm~=4.23 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts) (4.67.0)\n",
      "Requirement already satisfied: toolz~=0.10 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogi/.local/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts) (1.16.0)\n",
      "Requirement already satisfied: mxnet-mkl==1.6.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy==1.23.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (1.23.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from mxnet-mkl==1.6.0) (2.32.3)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from mxnet-mkl==1.6.0) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.8.30)\n",
      "Requirement already satisfied: gluonts[torch] in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy<2.2,>=1.16 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (1.23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.0 in /home/yogi/.local/lib/python3.10/site-packages (from gluonts[torch]) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (2.9.2)\n",
      "Requirement already satisfied: tqdm~=4.23 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (4.67.0)\n",
      "Requirement already satisfied: toolz~=0.10 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (0.12.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (4.11.0)\n",
      "Requirement already satisfied: torch<3,>=1.9 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (2.5.1)\n",
      "Requirement already satisfied: lightning<2.5,>=2.2.2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (2.4.0)\n",
      "Requirement already satisfied: pytorch-lightning<2.5,>=2.2.2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (2.4.0)\n",
      "Requirement already satisfied: scipy~=1.10 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from gluonts[torch]) (1.12.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (2024.10.0)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (0.11.8)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (24.1)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts[torch]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts[torch]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yogi/.local/lib/python3.10/site-packages (from pandas<3,>=1.0->gluonts[torch]) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (2.23.4)\n",
      "Requirement already satisfied: filelock in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=1.9->gluonts[torch]) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (3.10.10)\n",
      "Requirement already satisfied: setuptools in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.5,>=2.2.2->gluonts[torch]) (75.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts[torch]) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yogi/.local/lib/python3.10/site-packages (from jinja2->torch<3,>=1.9->gluonts[torch]) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (3.10)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yogi/miniconda3/envs/baseline/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonts\n",
    "!pip install mxnet-mkl==1.6.0 numpy==1.23.1\n",
    "!pip install \"gluonts[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACES =  3959\n",
      "AMRT =  3647\n",
      "ASII =  5670\n",
      "BBRI =  5000\n",
      "BRIS =  1218\n",
      "CPIN =  5670\n",
      "GGRM =  5670\n",
      "ICBP =  3197\n",
      "INKP =  5670\n",
      "ITMG =  3929\n",
      "MDKA =  1971\n",
      "PGAS =  4975\n",
      "SMGR =  5670\n",
      "TOWR =  3350\n",
      "ADRO =  3778\n",
      "ANTM =  5670\n",
      "BBCA =  5670\n",
      "BBTN =  3407\n",
      "BRPT =  5670\n",
      "ESSA =  2853\n",
      "GOTO =  195\n",
      "INCO =  5670\n",
      "INTP =  5670\n",
      "KLBF =  5670\n",
      "MEDC =  5670\n",
      "PTBA =  5230\n",
      "SRTG =  2488\n",
      "UNTR =  5670\n",
      "AKRA =  5670\n",
      "ARTO =  1824\n",
      "BBNI =  5670\n",
      "BMRI =  5085\n",
      "BUKA =  371\n",
      "EXCL =  4507\n",
      "HRUM =  3198\n",
      "INDF =  5670\n",
      "ISAT =  5670\n",
      "MAPI =  4738\n",
      "MTEL =  295\n",
      "SIDO =  2363\n",
      "TLKM =  5670\n",
      "UNVR =  5670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_arrow(csv_path: str):\n",
    "    \"\"\"\n",
    "    Store a given set of series into Arrow format at the specified path.\n",
    "\n",
    "    Input data can be a CSV file with a 'close' column.\n",
    "    \"\"\"\n",
    "    # Baca file CSV\n",
    "    data = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "\n",
    "    # Hitung jumlah total baris\n",
    "    total_rows = len(data)\n",
    "\n",
    "    # Hitung 70% dari total baris\n",
    "    rows_to_take = int(0.7 * total_rows)\n",
    "\n",
    "    # Ambil 70% pertama dari data\n",
    "    data = data.head(rows_to_take)\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Ambil array dari kolom 'close'\n",
    "    time_series = data['close'].to_numpy()\n",
    "    \n",
    "    # Set an arbitrary start time\n",
    "    start = np.datetime64(data['timestamp'].iloc[0], \"s\")\n",
    "\n",
    "    # Buat dataset yang terdiri dari satu seri waktu dengan satu start time\n",
    "    dataset = {\"start\": start, \"target\": time_series}\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 42 LQ45\n",
    "dataset_name=[\"ACES\",\"AMRT\",\"ASII\",\"BBRI\",\"BRIS\",\"CPIN\",\"GGRM\",\"ICBP\",\n",
    "              \"INKP\",\"ITMG\",\"MDKA\",\"PGAS\",\"SMGR\",\"TOWR\",\"ADRO\",\"ANTM\",\n",
    "              \"BBCA\",\"BBTN\",\"BRPT\",\"ESSA\",\"GOTO\",\"INCO\",\"INTP\",\"KLBF\",\n",
    "              \"MEDC\",\"PTBA\",\"SRTG\",\"UNTR\",\"AKRA\",\"ARTO\",\"BBNI\",\"BMRI\",\n",
    "              \"BUKA\",\"EXCL\",\"HRUM\",\"INDF\",\"ISAT\",\"MAPI\",\"MTEL\",\"SIDO\",\n",
    "              \"TLKM\",\"UNVR\"]\n",
    "\n",
    "dataset_train_model =[]\n",
    "\n",
    "count=0\n",
    "for ds in dataset_name:\n",
    "  # Sesuaikan path ke lokasi file CSV dan output yang diinginkan\n",
    "\n",
    "\n",
    "  data = pd.read_csv(f\"/home/yogi/chronos-research/dataset/LQ45-daily/{ds}.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "  # Hitung jumlah total baris\n",
    "  total_rows = len(data)\n",
    "\n",
    "  print(ds,\"= \",total_rows)\n",
    "  count+=1\n",
    "\n",
    "  dataset_train_model.append(convert_to_arrow(f\"/home/yogi/chronos-research/dataset/LQ45-daily/{ds}.csv\"))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogi/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/json.py:102: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1474780/1014250511.py:37: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method=fillna_method, inplace=True)\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/yogi/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/yogi/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type        | Params | Mode  | In sizes                                                         | Out sizes  \n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 25.9 K | train | [[1, 1], [1, 1], [1, 1604, 4], [1, 1604], [1, 1604], [1, 64, 4]] | [1, 20, 64]\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "25.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.9 K    Total params\n",
      "0.104     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 0/? [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 60.31 MiB is free. Process 1230636 has 7.87 GiB memory in use. Process 1435290 has 3.67 GiB memory in use. Including non-PyTorch memory, this process has 154.00 MiB memory in use. Of the allocated memory 18.48 MiB is allocated by PyTorch, and 5.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 96\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# train model dataset\u001b[39;00m\n\u001b[1;32m     91\u001b[0m train_model_data \u001b[38;5;241m=\u001b[39m ListDataset(\n\u001b[1;32m     92\u001b[0m     dataset_train_model,\n\u001b[1;32m     93\u001b[0m     freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m )\n\u001b[0;32m---> 96\u001b[0m model \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mtrain(train_model_data, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Make forecast\u001b[39;00m\n\u001b[1;32m    100\u001b[0m forecast_it, ts_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(\n\u001b[1;32m    101\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset, predictor\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m    102\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/torch/model/estimator.py:246\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train\u001b[0;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    239\u001b[0m     training_data: Dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchPredictor:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model(\n\u001b[1;32m    247\u001b[0m         training_data,\n\u001b[1;32m    248\u001b[0m         validation_data,\n\u001b[1;32m    249\u001b[0m         shuffle_buffer_length\u001b[38;5;241m=\u001b[39mshuffle_buffer_length,\n\u001b[1;32m    250\u001b[0m         cache_data\u001b[38;5;241m=\u001b[39mcache_data,\n\u001b[1;32m    251\u001b[0m         ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[1;32m    252\u001b[0m     )\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/torch/model/estimator.py:209\u001b[0m, in \u001b[0;36mPyTorchLightningEstimator.train_model\u001b[0;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data, ckpt_path, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m custom_callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m    201\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     }\n\u001b[1;32m    207\u001b[0m )\n\u001b[0;32m--> 209\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    210\u001b[0m     model\u001b[38;5;241m=\u001b[39mtraining_network,\n\u001b[1;32m    211\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtraining_data_loader,\n\u001b[1;32m    212\u001b[0m     val_dataloaders\u001b[38;5;241m=\u001b[39mvalidation_data_loader,\n\u001b[1;32m    213\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint\u001b[38;5;241m.\u001b[39mbest_model_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    217\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading best model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;241m.\u001b[39mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    540\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[1;32m    269\u001b[0m     trainer,\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    271\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[1;32m    272\u001b[0m     batch_idx,\n\u001b[1;32m    273\u001b[0m     optimizer,\n\u001b[1;32m    274\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    275\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    170\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1277\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1281\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    205\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Steppable,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/torch/model/deepar/lightning_module.py:71\u001b[0m, in \u001b[0;36mDeepARLightningModule.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx: \u001b[38;5;28mint\u001b[39m):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Execute training step.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, batch),\n\u001b[1;32m     73\u001b[0m         future_observed_values\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture_observed_values\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     74\u001b[0m         future_target\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture_target\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m     )\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m         train_loss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m         prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/torch/model/deepar/module.py:548\u001b[0m, in \u001b[0;36mDeepARModel.loss\u001b[0;34m(self, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target, future_observed_values, future_only, aggregate_by)\u001b[0m\n\u001b[1;32m    539\u001b[0m future_target_reshaped \u001b[38;5;241m=\u001b[39m future_target\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39mfuture_target\u001b[38;5;241m.\u001b[39mshape[extra_dims \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :],\n\u001b[1;32m    542\u001b[0m )\n\u001b[1;32m    543\u001b[0m future_observed_reshaped \u001b[38;5;241m=\u001b[39m future_observed_values\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39mfuture_observed_values\u001b[38;5;241m.\u001b[39mshape[extra_dims \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :],\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 548\u001b[0m params, scale, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munroll_lagged_rnn(\n\u001b[1;32m    549\u001b[0m     feat_static_cat,\n\u001b[1;32m    550\u001b[0m     feat_static_real,\n\u001b[1;32m    551\u001b[0m     past_time_feat,\n\u001b[1;32m    552\u001b[0m     past_target,\n\u001b[1;32m    553\u001b[0m     past_observed_values,\n\u001b[1;32m    554\u001b[0m     future_time_feat,\n\u001b[1;32m    555\u001b[0m     future_target_reshaped,\n\u001b[1;32m    556\u001b[0m )\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m future_only:\n\u001b[1;32m    559\u001b[0m     sliced_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    560\u001b[0m         [p[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_length :] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/gluonts/torch/model/deepar/module.py:330\u001b[0m, in \u001b[0;36mDeepARModel.unroll_lagged_rnn\u001b[0;34m(self, feat_static_cat, feat_static_real, past_time_feat, past_target, past_observed_values, future_time_feat, future_target)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mApplies the underlying RNN to the provided target data and covariates.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    - Output state from the RNN\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    320\u001b[0m rnn_input, scale, static_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_rnn_input(\n\u001b[1;32m    321\u001b[0m     feat_static_cat,\n\u001b[1;32m    322\u001b[0m     feat_static_real,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m     future_target,\n\u001b[1;32m    328\u001b[0m )\n\u001b[0;32m--> 330\u001b[0m output, new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(rnn_input)\n\u001b[1;32m    332\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_proj(output)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m params, scale, output, static_feat, new_state\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/baseline/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1125\u001b[0m         hx,\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1145\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 60.31 MiB is free. Process 1230636 has 7.87 GiB memory in use. Process 1435290 has 3.67 GiB memory in use. Including non-PyTorch memory, this process has 154.00 MiB memory in use. Of the allocated memory 18.48 MiB is allocated by PyTorch, and 5.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.evaluation import Evaluator, make_evaluation_predictions\n",
    "from gluonts.torch import DeepAREstimator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "\n",
    "def load_and_process_data(\n",
    "    filename,\n",
    "    date_column_name,\n",
    "    index_timezone=\"America/New_York\",\n",
    "    fillna_method=\"ffill\",\n",
    "):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Ensure the Date column is a datetime object\n",
    "    df[date_column_name] = pd.to_datetime(df[date_column_name], utc=True)\n",
    "\n",
    "    # Convert the timezone of the Date column to the specified timezone\n",
    "    df[date_column_name] = df[date_column_name].dt.tz_convert(index_timezone)\n",
    "\n",
    "    # Set Date column as the index\n",
    "    df.set_index(date_column_name, inplace=True)\n",
    "\n",
    "    # If Adj Close column exists, keep only this column and drop others\n",
    "    if \"Adj Close\" in df.columns:\n",
    "        df = df[[\"Adj Close\"]]\n",
    "        # Rename 'Adj Close' to be more specific based on the filename\n",
    "        new_column_name = filename.split(\"/\")[-1].split(\"_\")[0] + \"_Adj_Close\"\n",
    "        df.rename(columns={\"Adj Close\": new_column_name}, inplace=True)\n",
    "\n",
    "    # If fillna method is specified, fill the missing values\n",
    "    if fillna_method:\n",
    "        df.fillna(method=fillna_method, inplace=True)\n",
    "\n",
    "    # Check if there are still NaN values and backfill if needed\n",
    "    if df.isnull().sum().any():\n",
    "        df.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_process_data(\n",
    "    \"/home/yogi/chronos-research/dataset/LQ45-daily/ANTM.csv\", \"timestamp\"\n",
    ")\n",
    "\n",
    "# Create GluonTS datasets and split it\n",
    "def split_data(df, prediction_length, windows=1):\n",
    "    # Ensure the df index is datetime type\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Create GluonTS format dataset\n",
    "    start_timestamp = pd.Period(df.index[0], \"D\")\n",
    "    dataset = [\n",
    "        {\n",
    "            \"start\": start_timestamp,\n",
    "            \"target\": df[\"close\"].values.astype(np.float32),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Split the data (!note the negative symbol in front of prediction length!)\n",
    "    train_data, test_gen = split(dataset, offset=-prediction_length)\n",
    "\n",
    "    # Generate test instances\n",
    "    test_data = test_gen.generate_instances(prediction_length, windows=windows)\n",
    "\n",
    "    return dataset, train_data, test_data\n",
    "\n",
    "\n",
    "prediction_length = 64\n",
    "\n",
    "# Create the estimator\n",
    "\n",
    "estimator = DeepAREstimator(freq=\"D\",\n",
    "                            # context length is number of time steps will look back(5 days in a week)\n",
    "                            context_length=512,\n",
    "                            prediction_length=prediction_length,\n",
    "                            num_layers=2,\n",
    "                            lr= 0.001,  # Sama dengan learning rate pada Chronos\n",
    "                            num_parallel_samples= 20,  # Sama dengan jumlah sampel Chronos\n",
    "                            trainer_kwargs={\"max_epochs\": 50,\"ctx\": gpu(0)}\n",
    "                            )\n",
    "\n",
    "# Train on training dataset\n",
    "dataset, training_data, test_data = split_data(df, prediction_length)\n",
    "\n",
    "# train model dataset\n",
    "train_model_data = ListDataset(\n",
    "    dataset_train_model,\n",
    "    freq = \"D\"\n",
    ")\n",
    "\n",
    "model = estimator.train(train_model_data, num_workers=0)\n",
    "\n",
    "\n",
    "# Make forecast\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset, predictor=model\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "evaluator = Evaluator()\n",
    "evaluator(tss, forecasts)\n",
    "agg_metrics, item_metrics = evaluator(\n",
    "    iter(tss), iter(forecasts), num_series=len(dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.split import split\n",
    "\n",
    "\n",
    "from gluonts.ev.metrics import MASE, MeanWeightedSumQuantileLoss\n",
    "from gluonts.model.evaluation import evaluate_forecasts\n",
    "\n",
    "metrics_df = evaluate_forecasts(\n",
    "    forecasts,\n",
    "    test_data=test_data,\n",
    "    metrics=[\n",
    "        MASE(),\n",
    "        MeanWeightedSumQuantileLoss(np.arange(0.1, 1.0, 0.1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
