{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\chronosenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding outputs:\n",
      "tensor([[[ -9.0625,  -3.7656,  -2.5156,  ...,  -7.6562,  15.9375,  20.1250],\n",
      "         [ 11.6875,   3.5781,  18.8750,  ..., -11.2500,  -9.8125,   3.2344],\n",
      "         [ 11.6875,   3.5781,  18.8750,  ..., -11.2500,  -9.8125,   3.2344],\n",
      "         ...,\n",
      "         [ -9.0625,  -3.7656,  -2.5156,  ...,  -7.6562,  15.9375,  20.1250],\n",
      "         [ 11.6875,   3.5781,  18.8750,  ..., -11.2500,  -9.8125,   3.2344],\n",
      "         [ 11.6875,   3.5781,  18.8750,  ..., -11.2500,  -9.8125,   3.2344]]])\n",
      "=====================================\n",
      "Projected embeddings:\n",
      "tensor([[[16.4911, -7.7782, -7.2378,  6.0241],\n",
      "         [ 0.3229,  0.2022, -3.6999, -7.6459],\n",
      "         [ 0.3229,  0.2022, -3.6999, -7.6459],\n",
      "         [ 6.4316,  0.1444, -4.9361, -2.1015],\n",
      "         [16.4911, -7.7782, -7.2378,  6.0241],\n",
      "         [ 6.4316,  0.1444, -4.9361, -2.1015],\n",
      "         [ 6.4316,  0.1444, -4.9361, -2.1015],\n",
      "         [16.4911, -7.7782, -7.2378,  6.0241],\n",
      "         [ 0.3229,  0.2022, -3.6999, -7.6459],\n",
      "         [ 0.3229,  0.2022, -3.6999, -7.6459]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Inisialisasi tokenizer dan model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5Model.from_pretrained('t5-small')\n",
    "\n",
    "# Token ID yang ingin dicek\n",
    "token_ids = torch.tensor([510, 511, 511, 508, 510, 508, 508, 510, 511, 511]).unsqueeze(0)  # Tambahkan batch dimension\n",
    "\n",
    "# Dapatkan embedding\n",
    "with torch.no_grad():\n",
    "    outputs = model.encoder.embed_tokens(token_ids)\n",
    "    print(\"Embedding outputs:\")\n",
    "    print(outputs)  # Menampilkan embedding untuk token IDs\n",
    "\n",
    "print(\"=====================================\")\n",
    "# Proyeksi ke dimensi 4\n",
    "linear_projection = nn.Linear(outputs.size(-1), 4)\n",
    "projected_embeddings = linear_projection(outputs)\n",
    "\n",
    "print(\"Projected embeddings:\")\n",
    "print(projected_embeddings)  # Embedding dengan dimensi 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoding (dim 4):\n",
      "tensor([[ 0.0000,  0.0000,  1.0000,  1.0000],\n",
      "        [ 0.8415,  0.0100,  0.5403,  0.9999],\n",
      "        [ 0.9093,  0.0200, -0.4161,  0.9998],\n",
      "        [ 0.1411,  0.0300, -0.9900,  0.9996],\n",
      "        [-0.7568,  0.0400, -0.6536,  0.9992],\n",
      "        [-0.9589,  0.0500,  0.2837,  0.9988],\n",
      "        [-0.2794,  0.0600,  0.9602,  0.9982],\n",
      "        [ 0.6570,  0.0699,  0.7539,  0.9976],\n",
      "        [ 0.9894,  0.0799, -0.1455,  0.9968],\n",
      "        [ 0.4121,  0.0899, -0.9111,  0.9960]])\n"
     ]
    }
   ],
   "source": [
    "# Fungsi Positional Encoding dengan dimensi 4\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    return torch.tensor(pos_encoding[:, :d_model], dtype=torch.float32)  # Hanya ambil dimensi 4\n",
    "\n",
    "# Contoh penggunaan\n",
    "seq_length = token_ids.shape[1]\n",
    "pos_enc = positional_encoding(position=seq_length, d_model=4)\n",
    "\n",
    "print(\"Positional encoding (dim 4):\")\n",
    "print(pos_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "tensor([[[-1.0876,  1.6066, -2.7136, -0.2860],\n",
      "         [-0.9635,  1.7421, -2.6983, -0.0204],\n",
      "         [-1.0080,  1.6875, -2.7078, -0.1422],\n",
      "         [-1.0568,  1.6246, -2.7222, -0.2939],\n",
      "         [-1.1092,  1.5945, -2.7068, -0.2760],\n",
      "         [-1.0172,  1.6491, -2.7314, -0.2973],\n",
      "         [-1.0090,  1.6545, -2.7333, -0.2973],\n",
      "         [-1.0970,  1.6014, -2.7107, -0.2815],\n",
      "         [-0.9976,  1.6998, -2.7058, -0.1156],\n",
      "         [-1.0151,  1.6806, -2.7077, -0.1522]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Attention Weights:\n",
      "tensor([[[2.8517e-28, 6.6965e-02, 1.3702e-01, 8.8998e-08, 6.0971e-27,\n",
      "          2.8756e-07, 4.1631e-08, 1.1199e-28, 1.0769e-01, 6.8833e-01],\n",
      "         [3.3426e-06, 1.0159e-01, 1.7548e-01, 2.3967e-02, 3.0431e-05,\n",
      "          4.9891e-02, 1.1903e-02, 1.5449e-06, 1.3139e-01, 5.0574e-01],\n",
      "         [3.2668e-07, 9.3034e-02, 1.7083e-01, 1.2669e-02, 3.9210e-06,\n",
      "          2.9606e-02, 5.8738e-03, 1.3566e-07, 1.2350e-01, 5.6448e-01],\n",
      "         [1.2360e-13, 8.5434e-02, 1.6420e-01, 4.2931e-04, 1.8304e-12,\n",
      "          1.1050e-03, 1.9580e-04, 5.0424e-14, 1.2163e-01, 6.2701e-01],\n",
      "         [3.4163e-29, 5.5337e-02, 1.2191e-01, 3.8464e-08, 1.0512e-27,\n",
      "          1.4788e-07, 1.6646e-08, 1.1430e-29, 9.0894e-02, 7.3185e-01],\n",
      "         [2.9400e-11, 1.0894e-01, 1.8599e-01, 1.6188e-03, 2.6411e-10,\n",
      "          3.4646e-03, 8.4825e-04, 1.4236e-11, 1.4572e-01, 5.5342e-01],\n",
      "         [3.1390e-11, 1.1320e-01, 1.9001e-01, 1.9656e-03, 2.5448e-10,\n",
      "          3.9565e-03, 1.0450e-03, 1.6074e-11, 1.5163e-01, 5.3820e-01],\n",
      "         [4.0678e-29, 6.1377e-02, 1.3052e-01, 5.9271e-08, 1.0106e-27,\n",
      "          2.0049e-07, 2.6434e-08, 1.5293e-29, 1.0110e-01, 7.0700e-01],\n",
      "         [5.5999e-07, 9.5160e-02, 1.7229e-01, 1.5126e-02, 6.2768e-06,\n",
      "          3.4232e-02, 7.1172e-03, 2.3941e-07, 1.2572e-01, 5.5035e-01],\n",
      "         [3.5253e-07, 9.0663e-02, 1.6849e-01, 1.1461e-02, 4.5448e-06,\n",
      "          2.7938e-02, 5.2574e-03, 1.4079e-07, 1.2021e-01, 5.7597e-01]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tambahkan Positional Encoding ke Projected Embeddings\n",
    "projected_embeddings_with_pos = projected_embeddings + pos_enc.unsqueeze(0)\n",
    "\n",
    "batch_size, seq_length, d_model = projected_embeddings_with_pos.size()\n",
    "\n",
    "# Inisialisasi bobot untuk Q, K, V (dimensi 4x4)\n",
    "w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# Hitung Q, K, V\n",
    "Q = w_q(projected_embeddings_with_pos)\n",
    "K = w_k(projected_embeddings_with_pos)\n",
    "V = w_v(projected_embeddings_with_pos)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Hitung Self-Attention\n",
    "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "tensor([[[-0.1749,  0.1441, -0.1272,  0.0869],\n",
      "         [-0.1751,  0.1439, -0.1273,  0.0864],\n",
      "         [-0.1749,  0.1439, -0.1271,  0.0868],\n",
      "         [-0.1745,  0.1442, -0.1270,  0.0875],\n",
      "         [-0.1744,  0.1443, -0.1270,  0.0877],\n",
      "         [-0.1749,  0.1442, -0.1272,  0.0871],\n",
      "         [-0.1750,  0.1441, -0.1273,  0.0867],\n",
      "         [-0.1748,  0.1440, -0.1271,  0.0870]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Attention Weights:\n",
      "tensor([[[0.1249, 0.1256, 0.1253, 0.1248, 0.1241, 0.1249, 0.1254, 0.1250],\n",
      "         [0.1252, 0.1265, 0.1259, 0.1242, 0.1233, 0.1240, 0.1252, 0.1258],\n",
      "         [0.1250, 0.1257, 0.1255, 0.1248, 0.1242, 0.1244, 0.1250, 0.1254],\n",
      "         [0.1245, 0.1245, 0.1247, 0.1257, 0.1252, 0.1257, 0.1253, 0.1244],\n",
      "         [0.1244, 0.1242, 0.1245, 0.1258, 0.1255, 0.1261, 0.1254, 0.1241],\n",
      "         [0.1246, 0.1254, 0.1251, 0.1250, 0.1241, 0.1255, 0.1257, 0.1245],\n",
      "         [0.1249, 0.1260, 0.1255, 0.1246, 0.1235, 0.1249, 0.1256, 0.1251],\n",
      "         [0.1249, 0.1254, 0.1253, 0.1249, 0.1244, 0.1248, 0.1252, 0.1252]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Contoh input manual untuk projected_embeddings (batch_size=1, seq_length=10, d_model=4)\n",
    "manual_projected_embeddings = torch.tensor([[\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727],\n",
    "    [0.64158,\t0.73192,\t-0.20439,\t0.38998],\n",
    "    [0.64158,\t0.73192,\t-0.20439,\t0.38998],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "# Contoh input manual untuk pos_enc (seq_length=10, d_model=4)\n",
    "manual_pos_enc = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [0.8415, 0.5403, 0.01, 0.99995],\n",
    "    [0.9093, -0.4161, 0.02, 0.9998],\n",
    "    [0.1411, -0.99, 0.03, 0.9995],\n",
    "    [-0.7568, -0.6536, 0.04, 0.9992],\n",
    "    [-0.9589, 0.2837, 0.05, 0.99875],\n",
    "    [-0.2794, 0.9602, 0.06, 0.9982],\n",
    "    [0.657, 0.7539, 0.07, 0.99755]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Tambahkan Positional Encoding ke Projected Embeddings\n",
    "projected_embeddings_with_pos = manual_projected_embeddings + manual_pos_enc.unsqueeze(0)\n",
    "\n",
    "# Inisialisasi bobot untuk Q, K, V (d_model=4)\n",
    "w_q = torch.nn.Linear(4, 4, bias=False)\n",
    "w_k = torch.nn.Linear(4, 4, bias=False)\n",
    "w_v = torch.nn.Linear(4, 4, bias=False)\n",
    "\n",
    "# Random inisialisasi bobot (untuk mereplikasi perhitungan manual, tetapkan nilai spesifik)\n",
    "torch.manual_seed(0)\n",
    "w_q.weight.data.uniform_(-0.1, 0.1)\n",
    "w_k.weight.data.uniform_(-0.1, 0.1)\n",
    "w_v.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "# Hitung Q, K, V\n",
    "Q = w_q(projected_embeddings_with_pos)\n",
    "K = w_k(projected_embeddings_with_pos)\n",
    "V = w_v(projected_embeddings_with_pos)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Hitung Self-Attention\n",
    "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "tensor([[[-1.2809,  0.3143, -0.0739, -0.5383],\n",
      "         [-1.3620,  0.2744, -0.0682, -0.5714],\n",
      "         [-1.0980,  0.3086, -0.1497, -0.4849],\n",
      "         [-0.5932,  0.3605, -0.2805, -0.2971],\n",
      "         [-0.4532,  0.2758, -0.3395, -0.2153],\n",
      "         [-1.1137,  0.3325, -0.1227, -0.4674],\n",
      "         [-1.3515,  0.3571, -0.0406, -0.5681],\n",
      "         [-1.1968,  0.3628, -0.0942, -0.5277]]])\n",
      "Attention Weights:\n",
      "tensor([[[0.1496, 0.0680, 0.0543, 0.1119, 0.1159, 0.1769, 0.1893, 0.1341],\n",
      "         [0.1683, 0.0364, 0.0302, 0.1191, 0.1258, 0.1596, 0.1858, 0.1747],\n",
      "         [0.1826, 0.0943, 0.0599, 0.0789, 0.0713, 0.1214, 0.1964, 0.1955],\n",
      "         [0.1423, 0.2723, 0.1171, 0.0360, 0.0262, 0.0845, 0.1862, 0.1354],\n",
      "         [0.1056, 0.3003, 0.1806, 0.0517, 0.0392, 0.0891, 0.1375, 0.0960],\n",
      "         [0.1302, 0.1144, 0.0862, 0.1037, 0.0886, 0.1707, 0.1924, 0.1138],\n",
      "         [0.1477, 0.0532, 0.0410, 0.1126, 0.1113, 0.1948, 0.2093, 0.1301],\n",
      "         [0.1787, 0.0795, 0.0498, 0.0793, 0.0813, 0.1524, 0.2136, 0.1653]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Contoh nilai manual untuk Q, K, dan V (batch_size=1, seq_length=10, d_model=4)\n",
    "manual_Q = torch.tensor([[\n",
    "    [-0.8271741358,\t-0.4473737201\t,-0.1990115814\t,0.9793545112],\n",
    "    [-2.100874688,\t-2.485958248\t,-0.565244542\t,1.721133579],\n",
    "    [-1.230773659,\t-1.938179401\t,-0.9498562648\t,1.496905623],\n",
    "    [0.5144164915,\t0.1266056896\t,-1.04429923\t,1.116860739],\n",
    "    [1.317814715\t ,   1.362316101\t,-0.2696524529\t,0.2781227762],\n",
    "    [0.0940285007,\t0.3925437502\t,0.1478104924\t,1.056014863],\n",
    "    [-1.063412928,\t-0.7184953314\t,-0.108434038\t,1.490215993],\n",
    "    [-1.09268619\t ,   -0.9507493102\t,-0.8114003037\t,1.185122719]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "manual_K = torch.tensor([[\n",
    "    [0.0377749364\t,-0.1641118435\t,0.3524523188\t,1.074776842 ],\n",
    "    [2.17950715\t    ,-0.7541049303\t,-0.0753102742\t,0.9171164008],\n",
    "    [2.029081955\t    ,-1.06142199\t,0.8538343315\t,0.3785391614],\n",
    "    [-0.1718228584\t,-0.5201197879\t,2.255573135\t,0.5279247974],\n",
    "    [-0.773297439\t,-0.2821082053\t,2.240955121\t,0.1969217446],\n",
    "    [-0.6138309323\t,0.3659621834\t,1.344578588\t,1.309497533],\n",
    "    [-0.0784432281\t,0.2364181243\t,0.4556403745\t,1.660645486],\n",
    "    [0.3330865066\t,-0.5524778762\t,0.3919968475\t,0.931139843]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "manual_V = torch.tensor([[\n",
    "    [-1.194609282\t,0.4435478073\t,0.0269536717\t,-0.7814393057],\n",
    "    [0.6912948894\t,0.4351609871\t,-0.6019309471\t,0.1599279749],\n",
    "    [0.5374930809\t,-0.4033286063\t,-0.8513755243\t,0.3570918432],\n",
    "    [-1.562628739\t,-0.7370527331\t,-0.5455516675\t,-0.0945005508],\n",
    "    [-2.097548046\t,-0.3853457318\t,0.2409020575\t,-0.9469289598],\n",
    "    [-2.097679945\t,1.090307844\t,0.5265317096\t,-0.9391367871],\n",
    "    [-1.479237209\t,1.134933923\t,0.1383826489\t,-0.6422500574],\n",
    "    [-0.8159628809\t,-0.3007714649\t,-0.5737610414\t,-0.3254933775]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Hitung Attention dengan input manual\n",
    "attention_output, attention_weights = scaled_dot_product_attention(manual_Q, manual_K, manual_V)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Embeddings with Positional Encoding: tensor([[[-0.5416,  1.0535,  0.8795,  0.7827],\n",
      "         [ 1.4831,  1.2722, -0.1944,  1.3899],\n",
      "         [ 1.5509,  0.3158, -0.1844,  1.3898],\n",
      "         [-0.2386, -0.4227,  0.9933,  1.3493],\n",
      "         [-1.2984, -0.6001,  0.9195,  0.7819],\n",
      "         [-1.3386,  0.8510,  1.0133,  1.3485],\n",
      "         [-0.6591,  1.5275,  1.0233,  1.3480],\n",
      "         [ 0.1154,  0.8074,  0.9495,  0.7803]]])\n"
     ]
    }
   ],
   "source": [
    "# Embedding untuk token IDs & positional encoding\n",
    "\n",
    "# Contoh input manual untuk projected_embeddings (batch_size=1, seq_length=10, d_model=4)\n",
    "manual_projected_embeddings = torch.tensor([[\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727],\n",
    "    [0.64158,\t0.73192,\t-0.20439,\t0.38998],\n",
    "    [0.64158,\t0.73192,\t-0.20439,\t0.38998],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.37968,\t0.56734,\t0.96329,\t0.34976],\n",
    "    [-0.54163,\t0.05354,\t0.87951,\t-0.21727]\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "# Contoh input manual untuk pos_enc (seq_length=10, d_model=4)\n",
    "manual_pos_enc = torch.tensor([\n",
    "    [0.0, 1.0, 0.0, 1.0],\n",
    "    [0.8415, 0.5403, 0.01, 0.99995],\n",
    "    [0.9093, -0.4161, 0.02, 0.9998],\n",
    "    [0.1411, -0.99, 0.03, 0.9995],\n",
    "    [-0.7568, -0.6536, 0.04, 0.9992],\n",
    "    [-0.9589, 0.2837, 0.05, 0.99875],\n",
    "    [-0.2794, 0.9602, 0.06, 0.9982],\n",
    "    [0.657, 0.7539, 0.07, 0.99755]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Tambahkan Positional Encoding ke Projected Embeddings\n",
    "projected_embeddings_with_pos = manual_projected_embeddings + manual_pos_enc.unsqueeze(0)\n",
    "\n",
    "print(\"Projected Embeddings with Positional Encoding:\", projected_embeddings_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Output:\n",
      "tensor([[[-1.2809,  0.3143, -0.0739, -0.5383],\n",
      "         [-1.3620,  0.2744, -0.0683, -0.5714],\n",
      "         [-1.0981,  0.3086, -0.1497, -0.4849],\n",
      "         [-0.5932,  0.3605, -0.2805, -0.2971],\n",
      "         [-0.4533,  0.2758, -0.3396, -0.2153],\n",
      "         [-1.1137,  0.3325, -0.1227, -0.4674],\n",
      "         [-1.3515,  0.3571, -0.0406, -0.5681],\n",
      "         [-1.1968,  0.3628, -0.0942, -0.5277]]])\n",
      "Attention Weights:\n",
      "tensor([[[0.1496, 0.0680, 0.0543, 0.1119, 0.1159, 0.1769, 0.1893, 0.1341],\n",
      "         [0.1683, 0.0364, 0.0302, 0.1191, 0.1258, 0.1596, 0.1858, 0.1748],\n",
      "         [0.1825, 0.0942, 0.0599, 0.0789, 0.0713, 0.1214, 0.1964, 0.1955],\n",
      "         [0.1423, 0.2723, 0.1171, 0.0360, 0.0262, 0.0845, 0.1862, 0.1354],\n",
      "         [0.1056, 0.3003, 0.1806, 0.0517, 0.0392, 0.0891, 0.1375, 0.0960],\n",
      "         [0.1302, 0.1144, 0.0862, 0.1037, 0.0886, 0.1707, 0.1924, 0.1138],\n",
      "         [0.1477, 0.0532, 0.0410, 0.1126, 0.1113, 0.1948, 0.2093, 0.1301],\n",
      "         [0.1787, 0.0795, 0.0498, 0.0793, 0.0813, 0.1524, 0.2136, 0.1653]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Inisialisasi bobot dan bias untuk Q, K, V secara manual (d_model=4)\n",
    "manual_weights_q = torch.tensor([\n",
    "    [-0.73925\t,-0.96466\t,-0.23601\t,0.29734 ],\n",
    "    [-0.99099\t,-0.64466\t,-0.1603\t,-0.10428],\n",
    "    [-0.70426\t,0.34239    ,-0.93681\t,-0.03499],\n",
    "    [0.36942\t    ,0.265\t    ,0.4271\t    ,0.64976]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "manual_bias_q = torch.tensor([-0.23643,-0.08234,-0.08986,0.01603], dtype=torch.float32)\n",
    "\n",
    "manual_weights_k = torch.tensor([\n",
    "    [0.60899\t    ,0.19185\t,-0.82426\t,-0.03031],\n",
    "    [-0.47491\t,0.28685\t,-0.06882\t,0.38688],\n",
    "    [-0.32036\t,-0.99314\t,0.11239\t,0.92019],\n",
    "    [-0.05038\t,0.56384\t,0.41918\t,0.44664]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "manual_bias_k = torch.tensor([0.91417,-0.96584,0.40614,-0.26481], dtype=torch.float32)\n",
    "\n",
    "manual_weights_v = torch.tensor([\n",
    "    [0.7139\t,   0.2054\t    ,-0.58277\t,-0.37923],\n",
    "    [-0.73308,\t0.81554\t    ,-0.86505\t,0.60696],\n",
    "    [-0.75809,\t0.19903\t    ,-0.7651    ,0.06726],\n",
    "    [0.59016\t,   -0.15989\t,0.42657    ,0.3945]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "manual_bias_v = torch.tensor([-0.21495,-0.52698,0.02693,-0.9773], dtype=torch.float32)\n",
    "\n",
    "# Buat fungsi untuk menghitung Q, K, V secara manual\n",
    "def linear_transform(x, weight, bias):\n",
    "    return torch.matmul(x, weight.transpose(-2, -1)) + bias\n",
    "\n",
    "# Hitung Q, K, V dengan nilai manual\n",
    "Q = linear_transform(projected_embeddings_with_pos, manual_weights_q, manual_bias_q)\n",
    "K = linear_transform(projected_embeddings_with_pos, manual_weights_k, manual_bias_k)\n",
    "V = linear_transform(projected_embeddings_with_pos, manual_weights_v, manual_bias_v)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights\n",
    "\n",
    "# Hitung Self-Attention\n",
    "attention_output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Attention Output:\")\n",
    "print(attention_output)\n",
    "\n",
    "print(\"Attention Weights:\")\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Connection Output:\n",
      "tensor([[[-1.8225,  1.3678,  0.8056,  0.2444],\n",
      "         [ 0.1211,  1.5466, -0.2627,  0.8185],\n",
      "         [ 0.4528,  0.6244, -0.3341,  0.9049],\n",
      "         [-0.8318, -0.0621,  0.7128,  1.0522],\n",
      "         [-1.7517, -0.3243,  0.5800,  0.5667],\n",
      "         [-2.4523,  1.1835,  0.8906,  0.8812],\n",
      "         [-2.0106,  1.8847,  0.9827,  0.7799],\n",
      "         [-1.0814,  1.1703,  0.8553,  0.2525]]])\n",
      "Layer Normalization Output:\n",
      "tensor([[[-1.6353,  1.0112,  0.5448,  0.0793],\n",
      "         [-0.6293,  1.4339, -1.1847,  0.3801],\n",
      "         [ 0.0887,  0.4618, -1.6220,  1.0715],\n",
      "         [-1.4413, -0.3843,  0.6798,  1.1458],\n",
      "         [-1.5982, -0.0967,  0.8544,  0.8405],\n",
      "         [-1.7263,  0.7083,  0.5122,  0.5058],\n",
      "         [-1.6601,  1.0123,  0.3934,  0.2543],\n",
      "         [-1.6005,  1.0099,  0.6447, -0.0541]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. Residual Connection\n",
    "residual_output = attention_output + projected_embeddings_with_pos\n",
    "print(\"Residual Connection Output:\")\n",
    "print(residual_output)\n",
    "\n",
    "# 2. Layer Normalization\n",
    "# Menggunakan LayerNorm dengan epsilon kecil seperti pada perhitungan manual\n",
    "# Pastikan normalized_shape adalah (4,) karena kita ingin menghitung rata-rata per fitur\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=(4,), eps=1e-10)\n",
    "\n",
    "# Apply Layer Normalization\n",
    "normalized_output = layer_norm(residual_output)\n",
    "print(\"Layer Normalization Output:\")\n",
    "print(normalized_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Output (Final Output):\n",
      "tensor([[-0.4198,  0.3524, -0.3775, -0.0832],\n",
      "        [-0.4382,  0.3698, -0.3086, -0.0521],\n",
      "        [-0.4080,  0.3377, -0.3695,  0.0232],\n",
      "        [-0.3700,  0.2974, -0.4459,  0.1176],\n",
      "        [-0.4030,  0.3324, -0.3796,  0.0356],\n",
      "        [-0.4304,  0.3624, -0.3379, -0.0653],\n",
      "        [-0.4266,  0.3588, -0.3520, -0.0717],\n",
      "        [-0.4153,  0.3482, -0.3941, -0.0906]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "# Data input dari Layer Normalization dengan dimensi 4 (contoh data dari gambar)\n",
    "normalized_output = torch.tensor([\n",
    "    [-1.6353,  1.0112,  0.5448,  0.0793],\n",
    "    [-0.6293,  1.4339, -1.1847,  0.3801],\n",
    "    [ 0.0887,  0.4618, -1.6220,  1.0715],\n",
    "    [-1.4413, -0.3843,  0.6798,  1.1458],\n",
    "    [-1.5982, -0.0967,  0.8544,  0.8405],\n",
    "    [-1.7263,  0.7083,  0.5122,  0.5058],\n",
    "    [-1.6601,  1.0123,  0.3934,  0.2543],\n",
    "    [-1.6005,  1.0099,  0.6447, -0.0541]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Inisialisasi Linear Layer dengan dimensi sesuai dari gambar\n",
    "linear1 = nn.Linear(in_features=4, out_features=3, bias=True)  # Input 4, output 3\n",
    "linear2 = nn.Linear(in_features=3, out_features=4, bias=True)  # Input 3, output 4\n",
    "\n",
    "# Menetapkan bobot dan bias untuk Linear Layer 1 dari gambar\n",
    "manual_weights1 = torch.tensor([\n",
    "    [-0.28604, -0.29261, -0.2221, 0.30325],\n",
    "    [-0.261, 0.04918, 0.18259, -0.23019],\n",
    "    [0.19887, -0.38437, -0.32183, -0.47807]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "manual_bias1 = torch.tensor([-0.36804, -0.39214, 0.07934], dtype=torch.float32)\n",
    "\n",
    "linear1.weight.data = manual_weights1\n",
    "linear1.bias.data = manual_bias1\n",
    "\n",
    "# Menetapkan bobot dan bias untuk Linear Layer 2 dari gambar\n",
    "manual_weights2 = torch.tensor([\n",
    "    [0.19303\t,0.11128\t,   -0.47795],\n",
    "    [-0.20505,\t-0.10522,\t0.44491],\n",
    "    [-0.38888,\t-0.41619,\t0.09184],\n",
    "    [0.48059\t,-0.18764\t,0.12954]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "manual_bias2 = torch.tensor([-0.43819, 0.3698, -0.30859, -0.0521], dtype=torch.float32)\n",
    "\n",
    "linear2.weight.data = manual_weights2\n",
    "linear2.bias.data = manual_bias2\n",
    "\n",
    "# Definisikan Feed Forward Network (FFN) dengan nn.Linear()\n",
    "def feed_forward_network(x):\n",
    "    x = F.relu(linear1(x))  # Linear Layer 1 + ReLU\n",
    "    x = linear2(x)          # Linear Layer 2\n",
    "    return x\n",
    "\n",
    "# Mengaplikasikan FFN ke normalized_output\n",
    "ffn_output = feed_forward_network(normalized_output)\n",
    "\n",
    "print(\"FFN Output (Final Output):\")\n",
    "print(ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronosenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
