{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.16666666666666666,
  "eval_steps": 500,
  "global_step": 50000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.47432389855384827,
      "learning_rate": 0.0009983333333333333,
      "loss": 2.4273,
      "step": 500
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.6223739981651306,
      "learning_rate": 0.0009966666666666668,
      "loss": 2.3662,
      "step": 1000
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.5113865733146667,
      "learning_rate": 0.000995,
      "loss": 2.3413,
      "step": 1500
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.4166161119937897,
      "learning_rate": 0.0009933333333333333,
      "loss": 2.3489,
      "step": 2000
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.5026922225952148,
      "learning_rate": 0.0009916666666666667,
      "loss": 2.3092,
      "step": 2500
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.571026623249054,
      "learning_rate": 0.00099,
      "loss": 2.2846,
      "step": 3000
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.6771647334098816,
      "learning_rate": 0.0009883333333333333,
      "loss": 2.2868,
      "step": 3500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.6388105154037476,
      "learning_rate": 0.0009866666666666667,
      "loss": 2.2497,
      "step": 4000
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.6788287162780762,
      "learning_rate": 0.000985,
      "loss": 2.2401,
      "step": 4500
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4822697639465332,
      "learning_rate": 0.0009833333333333332,
      "loss": 2.2257,
      "step": 5000
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5764846801757812,
      "learning_rate": 0.0009816666666666667,
      "loss": 2.2096,
      "step": 5500
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.6190088391304016,
      "learning_rate": 0.00098,
      "loss": 2.2068,
      "step": 6000
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.6824941635131836,
      "learning_rate": 0.0009783333333333334,
      "loss": 2.1722,
      "step": 6500
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.6143842935562134,
      "learning_rate": 0.0009766666666666667,
      "loss": 2.1581,
      "step": 7000
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.5903660655021667,
      "learning_rate": 0.000975,
      "loss": 2.1295,
      "step": 7500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.6020989418029785,
      "learning_rate": 0.0009733333333333334,
      "loss": 2.1147,
      "step": 8000
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.713334858417511,
      "learning_rate": 0.0009716666666666667,
      "loss": 2.1085,
      "step": 8500
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.783708393573761,
      "learning_rate": 0.0009699999999999999,
      "loss": 2.0945,
      "step": 9000
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.6970427632331848,
      "learning_rate": 0.0009683333333333334,
      "loss": 2.0517,
      "step": 9500
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.6578370928764343,
      "learning_rate": 0.0009666666666666667,
      "loss": 2.0723,
      "step": 10000
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.8710381984710693,
      "learning_rate": 0.000965,
      "loss": 2.0495,
      "step": 10500
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.7660230994224548,
      "learning_rate": 0.0009633333333333334,
      "loss": 2.0189,
      "step": 11000
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.7910047769546509,
      "learning_rate": 0.0009616666666666667,
      "loss": 2.0054,
      "step": 11500
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.8109735250473022,
      "learning_rate": 0.00096,
      "loss": 1.9984,
      "step": 12000
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.8906494379043579,
      "learning_rate": 0.0009583333333333334,
      "loss": 1.9742,
      "step": 12500
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.7669569253921509,
      "learning_rate": 0.0009566666666666666,
      "loss": 1.9693,
      "step": 13000
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.8781254887580872,
      "learning_rate": 0.000955,
      "loss": 1.9649,
      "step": 13500
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.860998809337616,
      "learning_rate": 0.0009533333333333334,
      "loss": 1.9366,
      "step": 14000
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.8149706125259399,
      "learning_rate": 0.0009516666666666666,
      "loss": 1.9196,
      "step": 14500
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.8580487966537476,
      "learning_rate": 0.00095,
      "loss": 1.9206,
      "step": 15000
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.8429140448570251,
      "learning_rate": 0.0009483333333333334,
      "loss": 1.9082,
      "step": 15500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.778444230556488,
      "learning_rate": 0.0009466666666666667,
      "loss": 1.8978,
      "step": 16000
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.842940628528595,
      "learning_rate": 0.000945,
      "loss": 1.8788,
      "step": 16500
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.9122191667556763,
      "learning_rate": 0.0009433333333333334,
      "loss": 1.8664,
      "step": 17000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.789079487323761,
      "learning_rate": 0.0009416666666666667,
      "loss": 1.8447,
      "step": 17500
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.8142719864845276,
      "learning_rate": 0.00094,
      "loss": 1.8304,
      "step": 18000
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.8323712348937988,
      "learning_rate": 0.0009383333333333333,
      "loss": 1.8276,
      "step": 18500
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.7962679266929626,
      "learning_rate": 0.0009366666666666667,
      "loss": 1.8082,
      "step": 19000
    },
    {
      "epoch": 0.065,
      "grad_norm": 1.1406816244125366,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.8108,
      "step": 19500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.8697605133056641,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.8088,
      "step": 20000
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.9081452488899231,
      "learning_rate": 0.0009316666666666667,
      "loss": 1.8115,
      "step": 20500
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.4291996955871582,
      "learning_rate": 0.00093,
      "loss": 1.7839,
      "step": 21000
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 1.0401231050491333,
      "learning_rate": 0.0009283333333333333,
      "loss": 1.7661,
      "step": 21500
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.8535482883453369,
      "learning_rate": 0.0009266666666666667,
      "loss": 1.7532,
      "step": 22000
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.9193390011787415,
      "learning_rate": 0.000925,
      "loss": 1.7455,
      "step": 22500
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.9006972908973694,
      "learning_rate": 0.0009233333333333334,
      "loss": 1.7404,
      "step": 23000
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 1.897673487663269,
      "learning_rate": 0.0009216666666666667,
      "loss": 1.7413,
      "step": 23500
    },
    {
      "epoch": 0.08,
      "grad_norm": 1.0200986862182617,
      "learning_rate": 0.00092,
      "loss": 1.7321,
      "step": 24000
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.8214194774627686,
      "learning_rate": 0.0009183333333333334,
      "loss": 1.7145,
      "step": 24500
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.9754721522331238,
      "learning_rate": 0.0009166666666666666,
      "loss": 1.7167,
      "step": 25000
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.9388805627822876,
      "learning_rate": 0.000915,
      "loss": 1.7252,
      "step": 25500
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.95396488904953,
      "learning_rate": 0.0009133333333333334,
      "loss": 1.7009,
      "step": 26000
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 1.13447904586792,
      "learning_rate": 0.0009116666666666666,
      "loss": 1.6962,
      "step": 26500
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.8911749124526978,
      "learning_rate": 0.00091,
      "loss": 1.675,
      "step": 27000
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.9429997205734253,
      "learning_rate": 0.0009083333333333334,
      "loss": 1.6915,
      "step": 27500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.8621017932891846,
      "learning_rate": 0.0009066666666666666,
      "loss": 1.6686,
      "step": 28000
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.955685019493103,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.6582,
      "step": 28500
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.944513201713562,
      "learning_rate": 0.0009033333333333334,
      "loss": 1.6931,
      "step": 29000
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.8557716012001038,
      "learning_rate": 0.0009016666666666666,
      "loss": 1.653,
      "step": 29500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.9315153956413269,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.6411,
      "step": 30000
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 1.039352297782898,
      "learning_rate": 0.0008983333333333333,
      "loss": 1.6326,
      "step": 30500
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 1.0800373554229736,
      "learning_rate": 0.0008966666666666666,
      "loss": 1.632,
      "step": 31000
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.868090033531189,
      "learning_rate": 0.0008950000000000001,
      "loss": 1.6132,
      "step": 31500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.9386665225028992,
      "learning_rate": 0.0008933333333333333,
      "loss": 1.6258,
      "step": 32000
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.9881996512413025,
      "learning_rate": 0.0008916666666666667,
      "loss": 1.5891,
      "step": 32500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.9697954058647156,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.5807,
      "step": 33000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.9628074765205383,
      "learning_rate": 0.0008883333333333333,
      "loss": 1.605,
      "step": 33500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 1.0503994226455688,
      "learning_rate": 0.0008866666666666667,
      "loss": 1.5958,
      "step": 34000
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.9546058773994446,
      "learning_rate": 0.000885,
      "loss": 1.5948,
      "step": 34500
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.0694688558578491,
      "learning_rate": 0.0008833333333333333,
      "loss": 1.5629,
      "step": 35000
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.9664521217346191,
      "learning_rate": 0.0008816666666666668,
      "loss": 1.5621,
      "step": 35500
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9310898184776306,
      "learning_rate": 0.00088,
      "loss": 1.5742,
      "step": 36000
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 1.0388662815093994,
      "learning_rate": 0.0008783333333333333,
      "loss": 1.5645,
      "step": 36500
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.9525671005249023,
      "learning_rate": 0.0008766666666666668,
      "loss": 1.5835,
      "step": 37000
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.9514592289924622,
      "learning_rate": 0.000875,
      "loss": 1.558,
      "step": 37500
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.9368456602096558,
      "learning_rate": 0.0008733333333333333,
      "loss": 1.5394,
      "step": 38000
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.9298399686813354,
      "learning_rate": 0.0008716666666666667,
      "loss": 1.5485,
      "step": 38500
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.0405867099761963,
      "learning_rate": 0.00087,
      "loss": 1.5483,
      "step": 39000
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.9292435646057129,
      "learning_rate": 0.0008683333333333333,
      "loss": 1.5476,
      "step": 39500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.9674205780029297,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.542,
      "step": 40000
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.9190806150436401,
      "learning_rate": 0.000865,
      "loss": 1.5055,
      "step": 40500
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.9253895878791809,
      "learning_rate": 0.0008633333333333334,
      "loss": 1.5233,
      "step": 41000
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.8696349859237671,
      "learning_rate": 0.0008616666666666667,
      "loss": 1.5071,
      "step": 41500
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9785268902778625,
      "learning_rate": 0.00086,
      "loss": 1.5086,
      "step": 42000
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.8841969966888428,
      "learning_rate": 0.0008583333333333333,
      "loss": 1.5031,
      "step": 42500
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.9725237488746643,
      "learning_rate": 0.0008566666666666667,
      "loss": 1.4819,
      "step": 43000
    },
    {
      "epoch": 0.145,
      "grad_norm": 1.118957281112671,
      "learning_rate": 0.000855,
      "loss": 1.4851,
      "step": 43500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.9113378524780273,
      "learning_rate": 0.0008533333333333334,
      "loss": 1.484,
      "step": 44000
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.9779229164123535,
      "learning_rate": 0.0008516666666666667,
      "loss": 1.4741,
      "step": 44500
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.8975793719291687,
      "learning_rate": 0.00085,
      "loss": 1.4664,
      "step": 45000
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.9764734506607056,
      "learning_rate": 0.0008483333333333334,
      "loss": 1.4831,
      "step": 45500
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 0.9981516599655151,
      "learning_rate": 0.0008466666666666667,
      "loss": 1.4685,
      "step": 46000
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.936066210269928,
      "learning_rate": 0.0008449999999999999,
      "loss": 1.4645,
      "step": 46500
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.9116196036338806,
      "learning_rate": 0.0008433333333333334,
      "loss": 1.4616,
      "step": 47000
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.0406516790390015,
      "learning_rate": 0.0008416666666666667,
      "loss": 1.458,
      "step": 47500
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.019428014755249,
      "learning_rate": 0.00084,
      "loss": 1.4729,
      "step": 48000
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 1.003953218460083,
      "learning_rate": 0.0008383333333333334,
      "loss": 1.455,
      "step": 48500
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 1.0243335962295532,
      "learning_rate": 0.0008366666666666667,
      "loss": 1.4355,
      "step": 49000
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.9076284766197205,
      "learning_rate": 0.000835,
      "loss": 1.4439,
      "step": 49500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.0536261796951294,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.4254,
      "step": 50000
    }
  ],
  "logging_steps": 500,
  "max_steps": 300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 50000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.169698254848e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
