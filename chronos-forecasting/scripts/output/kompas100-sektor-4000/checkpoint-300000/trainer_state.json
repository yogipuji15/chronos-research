{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 300000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.3355211615562439,
      "learning_rate": 0.0009983333333333333,
      "loss": 2.0673,
      "step": 500
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.37550294399261475,
      "learning_rate": 0.0009966666666666668,
      "loss": 2.0448,
      "step": 1000
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.2985720634460449,
      "learning_rate": 0.000995,
      "loss": 2.0235,
      "step": 1500
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.4061012864112854,
      "learning_rate": 0.0009933333333333333,
      "loss": 1.9993,
      "step": 2000
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.3289767801761627,
      "learning_rate": 0.0009916666666666667,
      "loss": 2.0047,
      "step": 2500
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.3406992554664612,
      "learning_rate": 0.00099,
      "loss": 1.9933,
      "step": 3000
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.32093068957328796,
      "learning_rate": 0.0009883333333333333,
      "loss": 1.9825,
      "step": 3500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.3466475307941437,
      "learning_rate": 0.0009866666666666667,
      "loss": 1.9613,
      "step": 4000
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.3956133723258972,
      "learning_rate": 0.000985,
      "loss": 1.9562,
      "step": 4500
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4207194149494171,
      "learning_rate": 0.0009833333333333332,
      "loss": 1.9328,
      "step": 5000
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.5304854512214661,
      "learning_rate": 0.0009816666666666667,
      "loss": 1.9249,
      "step": 5500
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.457515686750412,
      "learning_rate": 0.00098,
      "loss": 1.9151,
      "step": 6000
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.6633057594299316,
      "learning_rate": 0.0009783333333333334,
      "loss": 1.9,
      "step": 6500
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.4478442072868347,
      "learning_rate": 0.0009766666666666667,
      "loss": 1.8836,
      "step": 7000
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.4768136143684387,
      "learning_rate": 0.000975,
      "loss": 1.888,
      "step": 7500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.502360463142395,
      "learning_rate": 0.0009733333333333334,
      "loss": 1.8593,
      "step": 8000
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.46312302350997925,
      "learning_rate": 0.0009716666666666667,
      "loss": 1.8546,
      "step": 8500
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.48460450768470764,
      "learning_rate": 0.0009699999999999999,
      "loss": 1.8411,
      "step": 9000
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.7103081941604614,
      "learning_rate": 0.0009683333333333334,
      "loss": 1.8385,
      "step": 9500
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.4974411129951477,
      "learning_rate": 0.0009666666666666667,
      "loss": 1.8185,
      "step": 10000
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.5890368223190308,
      "learning_rate": 0.000965,
      "loss": 1.8081,
      "step": 10500
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.5818201899528503,
      "learning_rate": 0.0009633333333333334,
      "loss": 1.7835,
      "step": 11000
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.6060959100723267,
      "learning_rate": 0.0009616666666666667,
      "loss": 1.7784,
      "step": 11500
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.5379834771156311,
      "learning_rate": 0.00096,
      "loss": 1.7518,
      "step": 12000
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.6572637557983398,
      "learning_rate": 0.0009583333333333334,
      "loss": 1.7467,
      "step": 12500
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.7304471731185913,
      "learning_rate": 0.0009566666666666666,
      "loss": 1.7383,
      "step": 13000
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.6181036233901978,
      "learning_rate": 0.000955,
      "loss": 1.7267,
      "step": 13500
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.6775724291801453,
      "learning_rate": 0.0009533333333333334,
      "loss": 1.7137,
      "step": 14000
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.6389793157577515,
      "learning_rate": 0.0009516666666666666,
      "loss": 1.7099,
      "step": 14500
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.6595916748046875,
      "learning_rate": 0.00095,
      "loss": 1.6892,
      "step": 15000
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.7719201445579529,
      "learning_rate": 0.0009483333333333334,
      "loss": 1.6796,
      "step": 15500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.5772321820259094,
      "learning_rate": 0.0009466666666666667,
      "loss": 1.6727,
      "step": 16000
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.6944795846939087,
      "learning_rate": 0.000945,
      "loss": 1.6561,
      "step": 16500
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.7987475395202637,
      "learning_rate": 0.0009433333333333334,
      "loss": 1.6509,
      "step": 17000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.7101038694381714,
      "learning_rate": 0.0009416666666666667,
      "loss": 1.6525,
      "step": 17500
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6223574280738831,
      "learning_rate": 0.00094,
      "loss": 1.6332,
      "step": 18000
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.6862496733665466,
      "learning_rate": 0.0009383333333333333,
      "loss": 1.6089,
      "step": 18500
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.733048141002655,
      "learning_rate": 0.0009366666666666667,
      "loss": 1.6074,
      "step": 19000
    },
    {
      "epoch": 0.065,
      "grad_norm": 0.7021152377128601,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.5973,
      "step": 19500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.7779088616371155,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.5806,
      "step": 20000
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.7145970463752747,
      "learning_rate": 0.0009316666666666667,
      "loss": 1.5742,
      "step": 20500
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.8040848970413208,
      "learning_rate": 0.00093,
      "loss": 1.5551,
      "step": 21000
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.7681021094322205,
      "learning_rate": 0.0009283333333333333,
      "loss": 1.5647,
      "step": 21500
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.717124342918396,
      "learning_rate": 0.0009266666666666667,
      "loss": 1.5475,
      "step": 22000
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.7583231329917908,
      "learning_rate": 0.000925,
      "loss": 1.5458,
      "step": 22500
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.7346726059913635,
      "learning_rate": 0.0009233333333333334,
      "loss": 1.5441,
      "step": 23000
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.6872162222862244,
      "learning_rate": 0.0009216666666666667,
      "loss": 1.524,
      "step": 23500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.868769109249115,
      "learning_rate": 0.00092,
      "loss": 1.5086,
      "step": 24000
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.7928684949874878,
      "learning_rate": 0.0009183333333333334,
      "loss": 1.509,
      "step": 24500
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.8284257054328918,
      "learning_rate": 0.0009166666666666666,
      "loss": 1.5067,
      "step": 25000
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.8000340461730957,
      "learning_rate": 0.000915,
      "loss": 1.4875,
      "step": 25500
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.8220829963684082,
      "learning_rate": 0.0009133333333333334,
      "loss": 1.501,
      "step": 26000
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.7681417465209961,
      "learning_rate": 0.0009116666666666666,
      "loss": 1.4751,
      "step": 26500
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.8356546759605408,
      "learning_rate": 0.00091,
      "loss": 1.4736,
      "step": 27000
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.8704735040664673,
      "learning_rate": 0.0009083333333333334,
      "loss": 1.4582,
      "step": 27500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.7940770387649536,
      "learning_rate": 0.0009066666666666666,
      "loss": 1.4569,
      "step": 28000
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.9029849171638489,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.4531,
      "step": 28500
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.8514230847358704,
      "learning_rate": 0.0009033333333333334,
      "loss": 1.4337,
      "step": 29000
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.7928577065467834,
      "learning_rate": 0.0009016666666666666,
      "loss": 1.4334,
      "step": 29500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.819105327129364,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.4233,
      "step": 30000
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.7964473962783813,
      "learning_rate": 0.0008983333333333333,
      "loss": 1.4298,
      "step": 30500
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.8431414365768433,
      "learning_rate": 0.0008966666666666666,
      "loss": 1.4117,
      "step": 31000
    },
    {
      "epoch": 0.105,
      "grad_norm": 0.8380269408226013,
      "learning_rate": 0.0008950000000000001,
      "loss": 1.4037,
      "step": 31500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.8863115310668945,
      "learning_rate": 0.0008933333333333333,
      "loss": 1.4005,
      "step": 32000
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.8764029741287231,
      "learning_rate": 0.0008916666666666667,
      "loss": 1.3905,
      "step": 32500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.8948859572410583,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.3843,
      "step": 33000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.8076921701431274,
      "learning_rate": 0.0008883333333333333,
      "loss": 1.3848,
      "step": 33500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.8373511433601379,
      "learning_rate": 0.0008866666666666667,
      "loss": 1.3655,
      "step": 34000
    },
    {
      "epoch": 0.115,
      "grad_norm": 0.8531045913696289,
      "learning_rate": 0.000885,
      "loss": 1.375,
      "step": 34500
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.7587197422981262,
      "learning_rate": 0.0008833333333333333,
      "loss": 1.3496,
      "step": 35000
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.9257946610450745,
      "learning_rate": 0.0008816666666666668,
      "loss": 1.3579,
      "step": 35500
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8915951251983643,
      "learning_rate": 0.00088,
      "loss": 1.3515,
      "step": 36000
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.9003289341926575,
      "learning_rate": 0.0008783333333333333,
      "loss": 1.3419,
      "step": 36500
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.8823502063751221,
      "learning_rate": 0.0008766666666666668,
      "loss": 1.3341,
      "step": 37000
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.8118137717247009,
      "learning_rate": 0.000875,
      "loss": 1.3396,
      "step": 37500
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.9283266663551331,
      "learning_rate": 0.0008733333333333333,
      "loss": 1.3269,
      "step": 38000
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 1.0429970026016235,
      "learning_rate": 0.0008716666666666667,
      "loss": 1.3305,
      "step": 38500
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.8878975510597229,
      "learning_rate": 0.00087,
      "loss": 1.3142,
      "step": 39000
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.7893909215927124,
      "learning_rate": 0.0008683333333333333,
      "loss": 1.3048,
      "step": 39500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.9881820678710938,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.2989,
      "step": 40000
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.8750188946723938,
      "learning_rate": 0.000865,
      "loss": 1.2905,
      "step": 40500
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.8218732476234436,
      "learning_rate": 0.0008633333333333334,
      "loss": 1.2853,
      "step": 41000
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.7952191829681396,
      "learning_rate": 0.0008616666666666667,
      "loss": 1.2896,
      "step": 41500
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9972625970840454,
      "learning_rate": 0.00086,
      "loss": 1.274,
      "step": 42000
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 1.0251621007919312,
      "learning_rate": 0.0008583333333333333,
      "loss": 1.2763,
      "step": 42500
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.8515802025794983,
      "learning_rate": 0.0008566666666666667,
      "loss": 1.2736,
      "step": 43000
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.9712474346160889,
      "learning_rate": 0.000855,
      "loss": 1.2577,
      "step": 43500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.9134600758552551,
      "learning_rate": 0.0008533333333333334,
      "loss": 1.2615,
      "step": 44000
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.9721280932426453,
      "learning_rate": 0.0008516666666666667,
      "loss": 1.2506,
      "step": 44500
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.8719412684440613,
      "learning_rate": 0.00085,
      "loss": 1.2493,
      "step": 45000
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.900425910949707,
      "learning_rate": 0.0008483333333333334,
      "loss": 1.2444,
      "step": 45500
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 1.0517330169677734,
      "learning_rate": 0.0008466666666666667,
      "loss": 1.2332,
      "step": 46000
    },
    {
      "epoch": 0.155,
      "grad_norm": 1.0166348218917847,
      "learning_rate": 0.0008449999999999999,
      "loss": 1.246,
      "step": 46500
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 0.9865723252296448,
      "learning_rate": 0.0008433333333333334,
      "loss": 1.2204,
      "step": 47000
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.8766039609909058,
      "learning_rate": 0.0008416666666666667,
      "loss": 1.2317,
      "step": 47500
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.941115140914917,
      "learning_rate": 0.00084,
      "loss": 1.2136,
      "step": 48000
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 0.9155890941619873,
      "learning_rate": 0.0008383333333333334,
      "loss": 1.2125,
      "step": 48500
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.8064982295036316,
      "learning_rate": 0.0008366666666666667,
      "loss": 1.2173,
      "step": 49000
    },
    {
      "epoch": 0.165,
      "grad_norm": 0.9369194507598877,
      "learning_rate": 0.000835,
      "loss": 1.2047,
      "step": 49500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.8538953065872192,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.1958,
      "step": 50000
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 1.0207866430282593,
      "learning_rate": 0.0008316666666666666,
      "loss": 1.1986,
      "step": 50500
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.9902206659317017,
      "learning_rate": 0.00083,
      "loss": 1.1868,
      "step": 51000
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 1.009329915046692,
      "learning_rate": 0.0008283333333333334,
      "loss": 1.1944,
      "step": 51500
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.9192966222763062,
      "learning_rate": 0.0008266666666666666,
      "loss": 1.1675,
      "step": 52000
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.0058118104934692,
      "learning_rate": 0.000825,
      "loss": 1.1702,
      "step": 52500
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.9155881404876709,
      "learning_rate": 0.0008233333333333334,
      "loss": 1.1773,
      "step": 53000
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 1.1042126417160034,
      "learning_rate": 0.0008216666666666667,
      "loss": 1.1629,
      "step": 53500
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.8705521821975708,
      "learning_rate": 0.00082,
      "loss": 1.1615,
      "step": 54000
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.9926793575286865,
      "learning_rate": 0.0008183333333333333,
      "loss": 1.1688,
      "step": 54500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.996634840965271,
      "learning_rate": 0.0008166666666666667,
      "loss": 1.1609,
      "step": 55000
    },
    {
      "epoch": 0.185,
      "grad_norm": 1.0290559530258179,
      "learning_rate": 0.000815,
      "loss": 1.1509,
      "step": 55500
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.9593789577484131,
      "learning_rate": 0.0008133333333333333,
      "loss": 1.151,
      "step": 56000
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 1.0978747606277466,
      "learning_rate": 0.0008116666666666667,
      "loss": 1.1444,
      "step": 56500
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.9745413064956665,
      "learning_rate": 0.0008100000000000001,
      "loss": 1.1367,
      "step": 57000
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.1057230234146118,
      "learning_rate": 0.0008083333333333333,
      "loss": 1.1402,
      "step": 57500
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.8359063863754272,
      "learning_rate": 0.0008066666666666667,
      "loss": 1.1355,
      "step": 58000
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.920087993144989,
      "learning_rate": 0.000805,
      "loss": 1.1238,
      "step": 58500
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.8549920320510864,
      "learning_rate": 0.0008033333333333333,
      "loss": 1.1278,
      "step": 59000
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 1.044141173362732,
      "learning_rate": 0.0008016666666666667,
      "loss": 1.1185,
      "step": 59500
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9018813371658325,
      "learning_rate": 0.0008,
      "loss": 1.1151,
      "step": 60000
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.8610502481460571,
      "learning_rate": 0.0007983333333333334,
      "loss": 1.1163,
      "step": 60500
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 0.992937445640564,
      "learning_rate": 0.0007966666666666667,
      "loss": 1.1076,
      "step": 61000
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.9966478943824768,
      "learning_rate": 0.000795,
      "loss": 1.106,
      "step": 61500
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 1.0127167701721191,
      "learning_rate": 0.0007933333333333334,
      "loss": 1.109,
      "step": 62000
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.9203179478645325,
      "learning_rate": 0.0007916666666666666,
      "loss": 1.0903,
      "step": 62500
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.9929056167602539,
      "learning_rate": 0.00079,
      "loss": 1.0903,
      "step": 63000
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 1.018000602722168,
      "learning_rate": 0.0007883333333333334,
      "loss": 1.0884,
      "step": 63500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.8624317049980164,
      "learning_rate": 0.0007866666666666666,
      "loss": 1.0861,
      "step": 64000
    },
    {
      "epoch": 0.215,
      "grad_norm": 1.006980061531067,
      "learning_rate": 0.000785,
      "loss": 1.0903,
      "step": 64500
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.060867190361023,
      "learning_rate": 0.0007833333333333334,
      "loss": 1.0843,
      "step": 65000
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.9617030024528503,
      "learning_rate": 0.0007816666666666666,
      "loss": 1.0781,
      "step": 65500
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.9194023013114929,
      "learning_rate": 0.0007800000000000001,
      "loss": 1.0753,
      "step": 66000
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.9613245129585266,
      "learning_rate": 0.0007783333333333334,
      "loss": 1.0662,
      "step": 66500
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 0.9853022694587708,
      "learning_rate": 0.0007766666666666666,
      "loss": 1.0639,
      "step": 67000
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.9544127583503723,
      "learning_rate": 0.0007750000000000001,
      "loss": 1.0589,
      "step": 67500
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.069939374923706,
      "learning_rate": 0.0007733333333333333,
      "loss": 1.0615,
      "step": 68000
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 1.1555745601654053,
      "learning_rate": 0.0007716666666666666,
      "loss": 1.0605,
      "step": 68500
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.1689273118972778,
      "learning_rate": 0.0007700000000000001,
      "loss": 1.0471,
      "step": 69000
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 1.1077680587768555,
      "learning_rate": 0.0007683333333333333,
      "loss": 1.0591,
      "step": 69500
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.1474120616912842,
      "learning_rate": 0.0007666666666666667,
      "loss": 1.043,
      "step": 70000
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.8902185559272766,
      "learning_rate": 0.0007650000000000001,
      "loss": 1.0404,
      "step": 70500
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 1.2042303085327148,
      "learning_rate": 0.0007633333333333333,
      "loss": 1.0335,
      "step": 71000
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.8724710941314697,
      "learning_rate": 0.0007616666666666667,
      "loss": 1.0352,
      "step": 71500
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9713504910469055,
      "learning_rate": 0.00076,
      "loss": 1.0344,
      "step": 72000
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.942729115486145,
      "learning_rate": 0.0007583333333333333,
      "loss": 1.0204,
      "step": 72500
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 1.0333727598190308,
      "learning_rate": 0.0007566666666666668,
      "loss": 1.0248,
      "step": 73000
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.9333825707435608,
      "learning_rate": 0.000755,
      "loss": 1.0128,
      "step": 73500
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.9878674745559692,
      "learning_rate": 0.0007533333333333333,
      "loss": 1.0253,
      "step": 74000
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 1.0109387636184692,
      "learning_rate": 0.0007516666666666668,
      "loss": 1.0078,
      "step": 74500
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.8716571927070618,
      "learning_rate": 0.00075,
      "loss": 1.0108,
      "step": 75000
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.8785520195960999,
      "learning_rate": 0.0007483333333333333,
      "loss": 1.0067,
      "step": 75500
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.9054956436157227,
      "learning_rate": 0.0007466666666666667,
      "loss": 1.0122,
      "step": 76000
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.9999340176582336,
      "learning_rate": 0.000745,
      "loss": 1.0104,
      "step": 76500
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 0.8989091515541077,
      "learning_rate": 0.0007433333333333333,
      "loss": 0.9938,
      "step": 77000
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.062361717224121,
      "learning_rate": 0.0007416666666666667,
      "loss": 0.9927,
      "step": 77500
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.8743816018104553,
      "learning_rate": 0.00074,
      "loss": 0.9904,
      "step": 78000
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.9940075278282166,
      "learning_rate": 0.0007383333333333334,
      "loss": 0.9937,
      "step": 78500
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.8652383089065552,
      "learning_rate": 0.0007366666666666667,
      "loss": 0.9848,
      "step": 79000
    },
    {
      "epoch": 0.265,
      "grad_norm": 0.9694322347640991,
      "learning_rate": 0.000735,
      "loss": 0.9768,
      "step": 79500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.9644596576690674,
      "learning_rate": 0.0007333333333333333,
      "loss": 0.9794,
      "step": 80000
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 1.0764353275299072,
      "learning_rate": 0.0007316666666666667,
      "loss": 0.9773,
      "step": 80500
    },
    {
      "epoch": 0.27,
      "grad_norm": 1.11562180519104,
      "learning_rate": 0.00073,
      "loss": 0.9673,
      "step": 81000
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.8707228899002075,
      "learning_rate": 0.0007283333333333334,
      "loss": 0.9878,
      "step": 81500
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 0.8687900900840759,
      "learning_rate": 0.0007266666666666667,
      "loss": 0.9676,
      "step": 82000
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.9881256818771362,
      "learning_rate": 0.000725,
      "loss": 0.9597,
      "step": 82500
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 1.2690694332122803,
      "learning_rate": 0.0007233333333333334,
      "loss": 0.957,
      "step": 83000
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 0.9718475937843323,
      "learning_rate": 0.0007216666666666667,
      "loss": 0.959,
      "step": 83500
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.13005793094635,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.9483,
      "step": 84000
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.9922038912773132,
      "learning_rate": 0.0007183333333333334,
      "loss": 0.9594,
      "step": 84500
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.8931356072425842,
      "learning_rate": 0.0007166666666666667,
      "loss": 0.9526,
      "step": 85000
    },
    {
      "epoch": 0.285,
      "grad_norm": 0.9057169556617737,
      "learning_rate": 0.000715,
      "loss": 0.9499,
      "step": 85500
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.003648042678833,
      "learning_rate": 0.0007133333333333334,
      "loss": 0.9421,
      "step": 86000
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 1.104522705078125,
      "learning_rate": 0.0007116666666666667,
      "loss": 0.9391,
      "step": 86500
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.0526796579360962,
      "learning_rate": 0.00071,
      "loss": 0.944,
      "step": 87000
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.9382848143577576,
      "learning_rate": 0.0007083333333333334,
      "loss": 0.9371,
      "step": 87500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 1.0390491485595703,
      "learning_rate": 0.0007066666666666666,
      "loss": 0.9339,
      "step": 88000
    },
    {
      "epoch": 0.295,
      "grad_norm": 1.0137202739715576,
      "learning_rate": 0.000705,
      "loss": 0.9323,
      "step": 88500
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 1.0043389797210693,
      "learning_rate": 0.0007033333333333334,
      "loss": 0.9361,
      "step": 89000
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 1.0641875267028809,
      "learning_rate": 0.0007016666666666666,
      "loss": 0.9219,
      "step": 89500
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.902735710144043,
      "learning_rate": 0.0007,
      "loss": 0.9284,
      "step": 90000
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.9443297982215881,
      "learning_rate": 0.0006983333333333334,
      "loss": 0.9242,
      "step": 90500
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 1.0209081172943115,
      "learning_rate": 0.0006966666666666667,
      "loss": 0.9239,
      "step": 91000
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.9547789692878723,
      "learning_rate": 0.000695,
      "loss": 0.9197,
      "step": 91500
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.9873912930488586,
      "learning_rate": 0.0006933333333333333,
      "loss": 0.9188,
      "step": 92000
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.9573597311973572,
      "learning_rate": 0.0006916666666666667,
      "loss": 0.9061,
      "step": 92500
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.9724090695381165,
      "learning_rate": 0.00069,
      "loss": 0.9087,
      "step": 93000
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 1.476474642753601,
      "learning_rate": 0.0006883333333333333,
      "loss": 0.9058,
      "step": 93500
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 1.0265682935714722,
      "learning_rate": 0.0006866666666666667,
      "loss": 0.9033,
      "step": 94000
    },
    {
      "epoch": 0.315,
      "grad_norm": 1.2041356563568115,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.9097,
      "step": 94500
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.8538944125175476,
      "learning_rate": 0.0006833333333333333,
      "loss": 0.9109,
      "step": 95000
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 0.983893871307373,
      "learning_rate": 0.0006816666666666667,
      "loss": 0.901,
      "step": 95500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.0800694227218628,
      "learning_rate": 0.00068,
      "loss": 0.8976,
      "step": 96000
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 1.022412896156311,
      "learning_rate": 0.0006783333333333333,
      "loss": 0.8955,
      "step": 96500
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.9809665083885193,
      "learning_rate": 0.0006766666666666667,
      "loss": 0.893,
      "step": 97000
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.9918899536132812,
      "learning_rate": 0.000675,
      "loss": 0.8846,
      "step": 97500
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 1.0031393766403198,
      "learning_rate": 0.0006733333333333334,
      "loss": 0.8816,
      "step": 98000
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 0.932267427444458,
      "learning_rate": 0.0006716666666666667,
      "loss": 0.8809,
      "step": 98500
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.047802209854126,
      "learning_rate": 0.00067,
      "loss": 0.8887,
      "step": 99000
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 1.2314518690109253,
      "learning_rate": 0.0006683333333333334,
      "loss": 0.872,
      "step": 99500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.0093920230865479,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.8771,
      "step": 100000
    },
    {
      "epoch": 0.335,
      "grad_norm": 1.0020400285720825,
      "learning_rate": 0.000665,
      "loss": 0.8735,
      "step": 100500
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.8614381551742554,
      "learning_rate": 0.0006633333333333334,
      "loss": 0.8629,
      "step": 101000
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 1.0248829126358032,
      "learning_rate": 0.0006616666666666666,
      "loss": 0.8618,
      "step": 101500
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.035898208618164,
      "learning_rate": 0.00066,
      "loss": 0.873,
      "step": 102000
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 1.0885359048843384,
      "learning_rate": 0.0006583333333333334,
      "loss": 0.8595,
      "step": 102500
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 1.037429690361023,
      "learning_rate": 0.0006566666666666666,
      "loss": 0.8671,
      "step": 103000
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.8310818076133728,
      "learning_rate": 0.0006550000000000001,
      "loss": 0.8669,
      "step": 103500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 1.0099736452102661,
      "learning_rate": 0.0006533333333333333,
      "loss": 0.8496,
      "step": 104000
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 0.9272271394729614,
      "learning_rate": 0.0006516666666666666,
      "loss": 0.8534,
      "step": 104500
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.124229907989502,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.8497,
      "step": 105000
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 1.0174394845962524,
      "learning_rate": 0.0006483333333333333,
      "loss": 0.8461,
      "step": 105500
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.9417256712913513,
      "learning_rate": 0.0006466666666666666,
      "loss": 0.8437,
      "step": 106000
    },
    {
      "epoch": 0.355,
      "grad_norm": 1.0360461473464966,
      "learning_rate": 0.0006450000000000001,
      "loss": 0.8407,
      "step": 106500
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.9436246156692505,
      "learning_rate": 0.0006433333333333333,
      "loss": 0.8434,
      "step": 107000
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.9443448185920715,
      "learning_rate": 0.0006416666666666667,
      "loss": 0.8391,
      "step": 107500
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.957114577293396,
      "learning_rate": 0.00064,
      "loss": 0.8426,
      "step": 108000
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 0.9168537259101868,
      "learning_rate": 0.0006383333333333333,
      "loss": 0.8318,
      "step": 108500
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 1.0700068473815918,
      "learning_rate": 0.0006366666666666667,
      "loss": 0.8382,
      "step": 109000
    },
    {
      "epoch": 0.365,
      "grad_norm": 0.9874843955039978,
      "learning_rate": 0.000635,
      "loss": 0.8302,
      "step": 109500
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.1763062477111816,
      "learning_rate": 0.0006333333333333333,
      "loss": 0.8313,
      "step": 110000
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.9994420409202576,
      "learning_rate": 0.0006316666666666668,
      "loss": 0.8195,
      "step": 110500
    },
    {
      "epoch": 0.37,
      "grad_norm": 1.0331600904464722,
      "learning_rate": 0.00063,
      "loss": 0.8187,
      "step": 111000
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.9770028591156006,
      "learning_rate": 0.0006283333333333333,
      "loss": 0.8295,
      "step": 111500
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.1105104684829712,
      "learning_rate": 0.0006266666666666668,
      "loss": 0.8224,
      "step": 112000
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.9869806170463562,
      "learning_rate": 0.000625,
      "loss": 0.8158,
      "step": 112500
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 0.9631744623184204,
      "learning_rate": 0.0006233333333333333,
      "loss": 0.8213,
      "step": 113000
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 1.0372138023376465,
      "learning_rate": 0.0006216666666666667,
      "loss": 0.8145,
      "step": 113500
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.9766979813575745,
      "learning_rate": 0.00062,
      "loss": 0.8196,
      "step": 114000
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 1.034812092781067,
      "learning_rate": 0.0006183333333333333,
      "loss": 0.8095,
      "step": 114500
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.0773663520812988,
      "learning_rate": 0.0006166666666666667,
      "loss": 0.8009,
      "step": 115000
    },
    {
      "epoch": 0.385,
      "grad_norm": 1.0388226509094238,
      "learning_rate": 0.000615,
      "loss": 0.8092,
      "step": 115500
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.9744459390640259,
      "learning_rate": 0.0006133333333333334,
      "loss": 0.7996,
      "step": 116000
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 1.0580083131790161,
      "learning_rate": 0.0006116666666666667,
      "loss": 0.7991,
      "step": 116500
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.9228935241699219,
      "learning_rate": 0.00061,
      "loss": 0.8016,
      "step": 117000
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.0258142948150635,
      "learning_rate": 0.0006083333333333333,
      "loss": 0.795,
      "step": 117500
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 0.972880482673645,
      "learning_rate": 0.0006066666666666667,
      "loss": 0.7951,
      "step": 118000
    },
    {
      "epoch": 0.395,
      "grad_norm": 1.1585173606872559,
      "learning_rate": 0.000605,
      "loss": 0.8041,
      "step": 118500
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 1.2910411357879639,
      "learning_rate": 0.0006033333333333334,
      "loss": 0.7906,
      "step": 119000
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 1.1908948421478271,
      "learning_rate": 0.0006016666666666667,
      "loss": 0.7883,
      "step": 119500
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.2029162645339966,
      "learning_rate": 0.0006,
      "loss": 0.7833,
      "step": 120000
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 1.0321658849716187,
      "learning_rate": 0.0005983333333333334,
      "loss": 0.7784,
      "step": 120500
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 0.9472991824150085,
      "learning_rate": 0.0005966666666666667,
      "loss": 0.7852,
      "step": 121000
    },
    {
      "epoch": 0.405,
      "grad_norm": 1.1335678100585938,
      "learning_rate": 0.0005949999999999999,
      "loss": 0.7911,
      "step": 121500
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 0.982886552810669,
      "learning_rate": 0.0005933333333333334,
      "loss": 0.7801,
      "step": 122000
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.0921733379364014,
      "learning_rate": 0.0005916666666666667,
      "loss": 0.7774,
      "step": 122500
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.9958335757255554,
      "learning_rate": 0.00059,
      "loss": 0.7782,
      "step": 123000
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 1.0095781087875366,
      "learning_rate": 0.0005883333333333334,
      "loss": 0.7683,
      "step": 123500
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.0680850744247437,
      "learning_rate": 0.0005866666666666667,
      "loss": 0.7852,
      "step": 124000
    },
    {
      "epoch": 0.415,
      "grad_norm": 0.9806045889854431,
      "learning_rate": 0.000585,
      "loss": 0.7676,
      "step": 124500
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.1267122030258179,
      "learning_rate": 0.0005833333333333334,
      "loss": 0.7585,
      "step": 125000
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 1.0163002014160156,
      "learning_rate": 0.0005816666666666666,
      "loss": 0.7647,
      "step": 125500
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.9769288301467896,
      "learning_rate": 0.00058,
      "loss": 0.7636,
      "step": 126000
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 1.1067560911178589,
      "learning_rate": 0.0005783333333333334,
      "loss": 0.7595,
      "step": 126500
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.9539453983306885,
      "learning_rate": 0.0005766666666666666,
      "loss": 0.7589,
      "step": 127000
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.006622314453125,
      "learning_rate": 0.000575,
      "loss": 0.7537,
      "step": 127500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.050970196723938,
      "learning_rate": 0.0005733333333333334,
      "loss": 0.7618,
      "step": 128000
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.8594112992286682,
      "learning_rate": 0.0005716666666666667,
      "loss": 0.7528,
      "step": 128500
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.1174187660217285,
      "learning_rate": 0.00057,
      "loss": 0.7541,
      "step": 129000
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 0.944283664226532,
      "learning_rate": 0.0005683333333333333,
      "loss": 0.7485,
      "step": 129500
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.011928677558899,
      "learning_rate": 0.0005666666666666667,
      "loss": 0.7412,
      "step": 130000
    },
    {
      "epoch": 0.435,
      "grad_norm": 1.0318036079406738,
      "learning_rate": 0.000565,
      "loss": 0.7541,
      "step": 130500
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 0.9212091565132141,
      "learning_rate": 0.0005633333333333333,
      "loss": 0.738,
      "step": 131000
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 1.000378966331482,
      "learning_rate": 0.0005616666666666667,
      "loss": 0.7436,
      "step": 131500
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.065245270729065,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.7402,
      "step": 132000
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.909734308719635,
      "learning_rate": 0.0005583333333333333,
      "loss": 0.736,
      "step": 132500
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 1.4717170000076294,
      "learning_rate": 0.0005566666666666667,
      "loss": 0.7386,
      "step": 133000
    },
    {
      "epoch": 0.445,
      "grad_norm": 1.1412193775177002,
      "learning_rate": 0.000555,
      "loss": 0.7373,
      "step": 133500
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 0.8055218458175659,
      "learning_rate": 0.0005533333333333333,
      "loss": 0.7365,
      "step": 134000
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 1.0483664274215698,
      "learning_rate": 0.0005516666666666667,
      "loss": 0.7418,
      "step": 134500
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.8856220841407776,
      "learning_rate": 0.00055,
      "loss": 0.7324,
      "step": 135000
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.9488560557365417,
      "learning_rate": 0.0005483333333333334,
      "loss": 0.7315,
      "step": 135500
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 1.112248420715332,
      "learning_rate": 0.0005466666666666667,
      "loss": 0.7266,
      "step": 136000
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.9045359492301941,
      "learning_rate": 0.000545,
      "loss": 0.7175,
      "step": 136500
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 1.222853422164917,
      "learning_rate": 0.0005433333333333334,
      "loss": 0.7259,
      "step": 137000
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 1.0618685483932495,
      "learning_rate": 0.0005416666666666666,
      "loss": 0.7261,
      "step": 137500
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.9435228109359741,
      "learning_rate": 0.00054,
      "loss": 0.7194,
      "step": 138000
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 1.037062406539917,
      "learning_rate": 0.0005383333333333334,
      "loss": 0.7215,
      "step": 138500
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 1.0491989850997925,
      "learning_rate": 0.0005366666666666666,
      "loss": 0.7172,
      "step": 139000
    },
    {
      "epoch": 0.465,
      "grad_norm": 0.9729234576225281,
      "learning_rate": 0.000535,
      "loss": 0.7137,
      "step": 139500
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.1600698232650757,
      "learning_rate": 0.0005333333333333334,
      "loss": 0.7198,
      "step": 140000
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 1.190032958984375,
      "learning_rate": 0.0005316666666666666,
      "loss": 0.7149,
      "step": 140500
    },
    {
      "epoch": 0.47,
      "grad_norm": 1.0801777839660645,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.7039,
      "step": 141000
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 1.0362677574157715,
      "learning_rate": 0.0005283333333333333,
      "loss": 0.7134,
      "step": 141500
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 0.9686642289161682,
      "learning_rate": 0.0005266666666666666,
      "loss": 0.714,
      "step": 142000
    },
    {
      "epoch": 0.475,
      "grad_norm": 1.0280483961105347,
      "learning_rate": 0.0005250000000000001,
      "loss": 0.7117,
      "step": 142500
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 1.045782446861267,
      "learning_rate": 0.0005233333333333333,
      "loss": 0.7099,
      "step": 143000
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 1.329829454421997,
      "learning_rate": 0.0005216666666666666,
      "loss": 0.7056,
      "step": 143500
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.1091256141662598,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.7014,
      "step": 144000
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 0.9480507373809814,
      "learning_rate": 0.0005183333333333333,
      "loss": 0.6979,
      "step": 144500
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.988597571849823,
      "learning_rate": 0.0005166666666666667,
      "loss": 0.6952,
      "step": 145000
    },
    {
      "epoch": 0.485,
      "grad_norm": 1.0719892978668213,
      "learning_rate": 0.000515,
      "loss": 0.7002,
      "step": 145500
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 1.0806583166122437,
      "learning_rate": 0.0005133333333333333,
      "loss": 0.6992,
      "step": 146000
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.8975508809089661,
      "learning_rate": 0.0005116666666666667,
      "loss": 0.6934,
      "step": 146500
    },
    {
      "epoch": 0.49,
      "grad_norm": 1.0331060886383057,
      "learning_rate": 0.00051,
      "loss": 0.692,
      "step": 147000
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 1.0725127458572388,
      "learning_rate": 0.0005083333333333333,
      "loss": 0.6903,
      "step": 147500
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 1.0267566442489624,
      "learning_rate": 0.0005066666666666668,
      "loss": 0.6883,
      "step": 148000
    },
    {
      "epoch": 0.495,
      "grad_norm": 1.158088207244873,
      "learning_rate": 0.000505,
      "loss": 0.684,
      "step": 148500
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 1.114195466041565,
      "learning_rate": 0.0005033333333333333,
      "loss": 0.6899,
      "step": 149000
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 0.9512441158294678,
      "learning_rate": 0.0005016666666666668,
      "loss": 0.6808,
      "step": 149500
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9842698574066162,
      "learning_rate": 0.0005,
      "loss": 0.6834,
      "step": 150000
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 1.0161007642745972,
      "learning_rate": 0.0004983333333333334,
      "loss": 0.681,
      "step": 150500
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 1.0379488468170166,
      "learning_rate": 0.0004966666666666666,
      "loss": 0.6801,
      "step": 151000
    },
    {
      "epoch": 0.505,
      "grad_norm": 1.1122560501098633,
      "learning_rate": 0.000495,
      "loss": 0.6784,
      "step": 151500
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 1.1024196147918701,
      "learning_rate": 0.0004933333333333334,
      "loss": 0.6778,
      "step": 152000
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.0943552255630493,
      "learning_rate": 0.0004916666666666666,
      "loss": 0.6769,
      "step": 152500
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.021329402923584,
      "learning_rate": 0.00049,
      "loss": 0.674,
      "step": 153000
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 0.9988512396812439,
      "learning_rate": 0.0004883333333333333,
      "loss": 0.6662,
      "step": 153500
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.9681910872459412,
      "learning_rate": 0.0004866666666666667,
      "loss": 0.6749,
      "step": 154000
    },
    {
      "epoch": 0.515,
      "grad_norm": 1.0706475973129272,
      "learning_rate": 0.00048499999999999997,
      "loss": 0.6745,
      "step": 154500
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.9415563941001892,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.6732,
      "step": 155000
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 1.26386559009552,
      "learning_rate": 0.0004816666666666667,
      "loss": 0.6673,
      "step": 155500
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0171470642089844,
      "learning_rate": 0.00048,
      "loss": 0.6659,
      "step": 156000
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.9921004176139832,
      "learning_rate": 0.0004783333333333333,
      "loss": 0.6674,
      "step": 156500
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 1.1689203977584839,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.6661,
      "step": 157000
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.966751754283905,
      "learning_rate": 0.000475,
      "loss": 0.6596,
      "step": 157500
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.9317200183868408,
      "learning_rate": 0.00047333333333333336,
      "loss": 0.6564,
      "step": 158000
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 1.0294374227523804,
      "learning_rate": 0.0004716666666666667,
      "loss": 0.6536,
      "step": 158500
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.9762964248657227,
      "learning_rate": 0.00047,
      "loss": 0.6539,
      "step": 159000
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 1.0530881881713867,
      "learning_rate": 0.00046833333333333335,
      "loss": 0.65,
      "step": 159500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.8658444881439209,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.6525,
      "step": 160000
    },
    {
      "epoch": 0.535,
      "grad_norm": 1.0342484712600708,
      "learning_rate": 0.000465,
      "loss": 0.6565,
      "step": 160500
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 1.0554834604263306,
      "learning_rate": 0.00046333333333333334,
      "loss": 0.6553,
      "step": 161000
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 0.9937978386878967,
      "learning_rate": 0.0004616666666666667,
      "loss": 0.653,
      "step": 161500
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.9663102626800537,
      "learning_rate": 0.00046,
      "loss": 0.6496,
      "step": 162000
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.9728264212608337,
      "learning_rate": 0.0004583333333333333,
      "loss": 0.6449,
      "step": 162500
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 1.0759565830230713,
      "learning_rate": 0.0004566666666666667,
      "loss": 0.6471,
      "step": 163000
    },
    {
      "epoch": 0.545,
      "grad_norm": 1.1348121166229248,
      "learning_rate": 0.000455,
      "loss": 0.6403,
      "step": 163500
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.0227446556091309,
      "learning_rate": 0.0004533333333333333,
      "loss": 0.6459,
      "step": 164000
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 1.1330420970916748,
      "learning_rate": 0.0004516666666666667,
      "loss": 0.6364,
      "step": 164500
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.9273430705070496,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.6401,
      "step": 165000
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 1.0526766777038574,
      "learning_rate": 0.0004483333333333333,
      "loss": 0.6401,
      "step": 165500
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 1.051679253578186,
      "learning_rate": 0.00044666666666666666,
      "loss": 0.6362,
      "step": 166000
    },
    {
      "epoch": 0.555,
      "grad_norm": 1.055321455001831,
      "learning_rate": 0.00044500000000000003,
      "loss": 0.634,
      "step": 166500
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.9224240779876709,
      "learning_rate": 0.00044333333333333334,
      "loss": 0.6356,
      "step": 167000
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.8997524976730347,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.6412,
      "step": 167500
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.0504395961761475,
      "learning_rate": 0.00044,
      "loss": 0.6321,
      "step": 168000
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 0.8518562316894531,
      "learning_rate": 0.0004383333333333334,
      "loss": 0.626,
      "step": 168500
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 1.066597580909729,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.6269,
      "step": 169000
    },
    {
      "epoch": 0.565,
      "grad_norm": 1.0526403188705444,
      "learning_rate": 0.000435,
      "loss": 0.6301,
      "step": 169500
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.9444252252578735,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.6287,
      "step": 170000
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 1.0447901487350464,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.6276,
      "step": 170500
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.101892352104187,
      "learning_rate": 0.00043,
      "loss": 0.629,
      "step": 171000
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 0.9540112614631653,
      "learning_rate": 0.00042833333333333335,
      "loss": 0.6224,
      "step": 171500
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 1.141535758972168,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.6245,
      "step": 172000
    },
    {
      "epoch": 0.575,
      "grad_norm": 1.2189524173736572,
      "learning_rate": 0.000425,
      "loss": 0.621,
      "step": 172500
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 0.9708441495895386,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.6221,
      "step": 173000
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 1.2005950212478638,
      "learning_rate": 0.0004216666666666667,
      "loss": 0.6145,
      "step": 173500
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.1443593502044678,
      "learning_rate": 0.00042,
      "loss": 0.619,
      "step": 174000
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 1.1686246395111084,
      "learning_rate": 0.00041833333333333333,
      "loss": 0.6164,
      "step": 174500
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 1.170094609260559,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.6134,
      "step": 175000
    },
    {
      "epoch": 0.585,
      "grad_norm": 1.019801378250122,
      "learning_rate": 0.000415,
      "loss": 0.6164,
      "step": 175500
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.9313812851905823,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.6049,
      "step": 176000
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 0.8873560428619385,
      "learning_rate": 0.0004116666666666667,
      "loss": 0.615,
      "step": 176500
    },
    {
      "epoch": 0.59,
      "grad_norm": 1.0654759407043457,
      "learning_rate": 0.00041,
      "loss": 0.6115,
      "step": 177000
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 1.0033938884735107,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.6129,
      "step": 177500
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 1.0200433731079102,
      "learning_rate": 0.00040666666666666667,
      "loss": 0.6089,
      "step": 178000
    },
    {
      "epoch": 0.595,
      "grad_norm": 1.074753761291504,
      "learning_rate": 0.00040500000000000003,
      "loss": 0.6085,
      "step": 178500
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 1.0822252035140991,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.605,
      "step": 179000
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 0.9835790991783142,
      "learning_rate": 0.00040166666666666665,
      "loss": 0.6043,
      "step": 179500
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0235002040863037,
      "learning_rate": 0.0004,
      "loss": 0.6065,
      "step": 180000
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 1.0357264280319214,
      "learning_rate": 0.00039833333333333333,
      "loss": 0.6019,
      "step": 180500
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 1.0192182064056396,
      "learning_rate": 0.0003966666666666667,
      "loss": 0.6024,
      "step": 181000
    },
    {
      "epoch": 0.605,
      "grad_norm": 0.9212009310722351,
      "learning_rate": 0.000395,
      "loss": 0.6018,
      "step": 181500
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 1.0817971229553223,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.6033,
      "step": 182000
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 1.1005220413208008,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.6047,
      "step": 182500
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.9889771938323975,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.5998,
      "step": 183000
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 1.1950727701187134,
      "learning_rate": 0.0003883333333333333,
      "loss": 0.5994,
      "step": 183500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.9562946557998657,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.588,
      "step": 184000
    },
    {
      "epoch": 0.615,
      "grad_norm": 1.0180823802947998,
      "learning_rate": 0.00038500000000000003,
      "loss": 0.5901,
      "step": 184500
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 1.1163774728775024,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.5939,
      "step": 185000
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 0.9045707583427429,
      "learning_rate": 0.00038166666666666666,
      "loss": 0.5867,
      "step": 185500
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.1538316011428833,
      "learning_rate": 0.00038,
      "loss": 0.5865,
      "step": 186000
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 1.0306947231292725,
      "learning_rate": 0.0003783333333333334,
      "loss": 0.5889,
      "step": 186500
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 0.8406145572662354,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.5849,
      "step": 187000
    },
    {
      "epoch": 0.625,
      "grad_norm": 1.0539076328277588,
      "learning_rate": 0.000375,
      "loss": 0.5909,
      "step": 187500
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 1.0620521306991577,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.584,
      "step": 188000
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.9584974050521851,
      "learning_rate": 0.00037166666666666663,
      "loss": 0.5903,
      "step": 188500
    },
    {
      "epoch": 0.63,
      "grad_norm": 1.1984639167785645,
      "learning_rate": 0.00037,
      "loss": 0.5862,
      "step": 189000
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 1.0201419591903687,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.584,
      "step": 189500
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.9530481100082397,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.5775,
      "step": 190000
    },
    {
      "epoch": 0.635,
      "grad_norm": 1.0292537212371826,
      "learning_rate": 0.000365,
      "loss": 0.5784,
      "step": 190500
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 1.0272395610809326,
      "learning_rate": 0.00036333333333333335,
      "loss": 0.5784,
      "step": 191000
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 1.1783313751220703,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.5862,
      "step": 191500
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.8683679699897766,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.5783,
      "step": 192000
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.9472211599349976,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.5792,
      "step": 192500
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 0.9748314619064331,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.5762,
      "step": 193000
    },
    {
      "epoch": 0.645,
      "grad_norm": 0.8876405954360962,
      "learning_rate": 0.000355,
      "loss": 0.5792,
      "step": 193500
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 1.058377742767334,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.5818,
      "step": 194000
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.9853877425193787,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.5705,
      "step": 194500
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.9989496469497681,
      "learning_rate": 0.00035,
      "loss": 0.5689,
      "step": 195000
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 1.0058574676513672,
      "learning_rate": 0.00034833333333333336,
      "loss": 0.5739,
      "step": 195500
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 1.0223699808120728,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.567,
      "step": 196000
    },
    {
      "epoch": 0.655,
      "grad_norm": 1.099519968032837,
      "learning_rate": 0.000345,
      "loss": 0.5748,
      "step": 196500
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 1.0735410451889038,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.568,
      "step": 197000
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 1.0452617406845093,
      "learning_rate": 0.00034166666666666666,
      "loss": 0.567,
      "step": 197500
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0193676948547363,
      "learning_rate": 0.00034,
      "loss": 0.5673,
      "step": 198000
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.9510929584503174,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.5671,
      "step": 198500
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 0.9536948204040527,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.5685,
      "step": 199000
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.9571207165718079,
      "learning_rate": 0.000335,
      "loss": 0.5647,
      "step": 199500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.0180803537368774,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.5668,
      "step": 200000
    },
    {
      "epoch": 0.6683333333333333,
      "grad_norm": 0.9530026316642761,
      "learning_rate": 0.0003316666666666667,
      "loss": 0.5629,
      "step": 200500
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.9427794218063354,
      "learning_rate": 0.00033,
      "loss": 0.5612,
      "step": 201000
    },
    {
      "epoch": 0.6716666666666666,
      "grad_norm": 1.0282652378082275,
      "learning_rate": 0.0003283333333333333,
      "loss": 0.5597,
      "step": 201500
    },
    {
      "epoch": 0.6733333333333333,
      "grad_norm": 1.0436697006225586,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.5586,
      "step": 202000
    },
    {
      "epoch": 0.675,
      "grad_norm": 1.036487340927124,
      "learning_rate": 0.00032500000000000004,
      "loss": 0.5612,
      "step": 202500
    },
    {
      "epoch": 0.6766666666666666,
      "grad_norm": 0.9922236800193787,
      "learning_rate": 0.0003233333333333333,
      "loss": 0.5494,
      "step": 203000
    },
    {
      "epoch": 0.6783333333333333,
      "grad_norm": 1.1615581512451172,
      "learning_rate": 0.00032166666666666666,
      "loss": 0.5554,
      "step": 203500
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.9600471258163452,
      "learning_rate": 0.00032,
      "loss": 0.553,
      "step": 204000
    },
    {
      "epoch": 0.6816666666666666,
      "grad_norm": 1.0620126724243164,
      "learning_rate": 0.00031833333333333334,
      "loss": 0.557,
      "step": 204500
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 1.0392429828643799,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.555,
      "step": 205000
    },
    {
      "epoch": 0.685,
      "grad_norm": 0.9563198685646057,
      "learning_rate": 0.000315,
      "loss": 0.5541,
      "step": 205500
    },
    {
      "epoch": 0.6866666666666666,
      "grad_norm": 0.9309865832328796,
      "learning_rate": 0.0003133333333333334,
      "loss": 0.5512,
      "step": 206000
    },
    {
      "epoch": 0.6883333333333334,
      "grad_norm": 0.9538853168487549,
      "learning_rate": 0.00031166666666666663,
      "loss": 0.5507,
      "step": 206500
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.106732726097107,
      "learning_rate": 0.00031,
      "loss": 0.5581,
      "step": 207000
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 1.109008550643921,
      "learning_rate": 0.00030833333333333337,
      "loss": 0.5467,
      "step": 207500
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.8540272116661072,
      "learning_rate": 0.0003066666666666667,
      "loss": 0.5502,
      "step": 208000
    },
    {
      "epoch": 0.695,
      "grad_norm": 0.9316406846046448,
      "learning_rate": 0.000305,
      "loss": 0.5463,
      "step": 208500
    },
    {
      "epoch": 0.6966666666666667,
      "grad_norm": 1.1414551734924316,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.5416,
      "step": 209000
    },
    {
      "epoch": 0.6983333333333334,
      "grad_norm": 1.0686968564987183,
      "learning_rate": 0.0003016666666666667,
      "loss": 0.5465,
      "step": 209500
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.0367543697357178,
      "learning_rate": 0.0003,
      "loss": 0.544,
      "step": 210000
    },
    {
      "epoch": 0.7016666666666667,
      "grad_norm": 0.8706143498420715,
      "learning_rate": 0.00029833333333333334,
      "loss": 0.542,
      "step": 210500
    },
    {
      "epoch": 0.7033333333333334,
      "grad_norm": 1.16378915309906,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.5461,
      "step": 211000
    },
    {
      "epoch": 0.705,
      "grad_norm": 1.0563609600067139,
      "learning_rate": 0.000295,
      "loss": 0.5437,
      "step": 211500
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 1.29408860206604,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.5401,
      "step": 212000
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 1.0532172918319702,
      "learning_rate": 0.0002916666666666667,
      "loss": 0.538,
      "step": 212500
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.9305809140205383,
      "learning_rate": 0.00029,
      "loss": 0.5385,
      "step": 213000
    },
    {
      "epoch": 0.7116666666666667,
      "grad_norm": 1.091171145439148,
      "learning_rate": 0.0002883333333333333,
      "loss": 0.5468,
      "step": 213500
    },
    {
      "epoch": 0.7133333333333334,
      "grad_norm": 0.9899116158485413,
      "learning_rate": 0.0002866666666666667,
      "loss": 0.543,
      "step": 214000
    },
    {
      "epoch": 0.715,
      "grad_norm": 1.0014692544937134,
      "learning_rate": 0.000285,
      "loss": 0.5438,
      "step": 214500
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 1.0533548593521118,
      "learning_rate": 0.00028333333333333335,
      "loss": 0.5389,
      "step": 215000
    },
    {
      "epoch": 0.7183333333333334,
      "grad_norm": 1.0234867334365845,
      "learning_rate": 0.00028166666666666666,
      "loss": 0.533,
      "step": 215500
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.990218460559845,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.534,
      "step": 216000
    },
    {
      "epoch": 0.7216666666666667,
      "grad_norm": 1.0189943313598633,
      "learning_rate": 0.00027833333333333334,
      "loss": 0.5355,
      "step": 216500
    },
    {
      "epoch": 0.7233333333333334,
      "grad_norm": 1.149340271949768,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.5345,
      "step": 217000
    },
    {
      "epoch": 0.725,
      "grad_norm": 1.1403751373291016,
      "learning_rate": 0.000275,
      "loss": 0.5299,
      "step": 217500
    },
    {
      "epoch": 0.7266666666666667,
      "grad_norm": 0.9778065085411072,
      "learning_rate": 0.00027333333333333333,
      "loss": 0.5276,
      "step": 218000
    },
    {
      "epoch": 0.7283333333333334,
      "grad_norm": 0.9148910045623779,
      "learning_rate": 0.0002716666666666667,
      "loss": 0.534,
      "step": 218500
    },
    {
      "epoch": 0.73,
      "grad_norm": 1.0084974765777588,
      "learning_rate": 0.00027,
      "loss": 0.5302,
      "step": 219000
    },
    {
      "epoch": 0.7316666666666667,
      "grad_norm": 0.9354385137557983,
      "learning_rate": 0.0002683333333333333,
      "loss": 0.5236,
      "step": 219500
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.0880112648010254,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.5268,
      "step": 220000
    },
    {
      "epoch": 0.735,
      "grad_norm": 0.9844102263450623,
      "learning_rate": 0.00026500000000000004,
      "loss": 0.5266,
      "step": 220500
    },
    {
      "epoch": 0.7366666666666667,
      "grad_norm": 0.8722320199012756,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.5258,
      "step": 221000
    },
    {
      "epoch": 0.7383333333333333,
      "grad_norm": 1.2910218238830566,
      "learning_rate": 0.00026166666666666667,
      "loss": 0.5199,
      "step": 221500
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.121667742729187,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.5268,
      "step": 222000
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.996799111366272,
      "learning_rate": 0.00025833333333333334,
      "loss": 0.5212,
      "step": 222500
    },
    {
      "epoch": 0.7433333333333333,
      "grad_norm": 1.048876166343689,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.5276,
      "step": 223000
    },
    {
      "epoch": 0.745,
      "grad_norm": 0.9407269954681396,
      "learning_rate": 0.000255,
      "loss": 0.5277,
      "step": 223500
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.9812660813331604,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.5189,
      "step": 224000
    },
    {
      "epoch": 0.7483333333333333,
      "grad_norm": 1.082412600517273,
      "learning_rate": 0.00025166666666666664,
      "loss": 0.5226,
      "step": 224500
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.1323760747909546,
      "learning_rate": 0.00025,
      "loss": 0.523,
      "step": 225000
    },
    {
      "epoch": 0.7516666666666667,
      "grad_norm": 1.2455501556396484,
      "learning_rate": 0.0002483333333333333,
      "loss": 0.5188,
      "step": 225500
    },
    {
      "epoch": 0.7533333333333333,
      "grad_norm": 0.9426478743553162,
      "learning_rate": 0.0002466666666666667,
      "loss": 0.5157,
      "step": 226000
    },
    {
      "epoch": 0.755,
      "grad_norm": 1.1078193187713623,
      "learning_rate": 0.000245,
      "loss": 0.5148,
      "step": 226500
    },
    {
      "epoch": 0.7566666666666667,
      "grad_norm": 1.1167970895767212,
      "learning_rate": 0.00024333333333333336,
      "loss": 0.5213,
      "step": 227000
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.8542302250862122,
      "learning_rate": 0.00024166666666666667,
      "loss": 0.516,
      "step": 227500
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9929975867271423,
      "learning_rate": 0.00024,
      "loss": 0.5151,
      "step": 228000
    },
    {
      "epoch": 0.7616666666666667,
      "grad_norm": 1.1698840856552124,
      "learning_rate": 0.00023833333333333334,
      "loss": 0.5157,
      "step": 228500
    },
    {
      "epoch": 0.7633333333333333,
      "grad_norm": 1.0215872526168823,
      "learning_rate": 0.00023666666666666668,
      "loss": 0.5108,
      "step": 229000
    },
    {
      "epoch": 0.765,
      "grad_norm": 1.016862392425537,
      "learning_rate": 0.000235,
      "loss": 0.5085,
      "step": 229500
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 1.2750895023345947,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.5156,
      "step": 230000
    },
    {
      "epoch": 0.7683333333333333,
      "grad_norm": 1.031913161277771,
      "learning_rate": 0.00023166666666666667,
      "loss": 0.5075,
      "step": 230500
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.9050793647766113,
      "learning_rate": 0.00023,
      "loss": 0.5118,
      "step": 231000
    },
    {
      "epoch": 0.7716666666666666,
      "grad_norm": 0.9554919600486755,
      "learning_rate": 0.00022833333333333334,
      "loss": 0.5136,
      "step": 231500
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 1.0477019548416138,
      "learning_rate": 0.00022666666666666666,
      "loss": 0.5085,
      "step": 232000
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.9264244437217712,
      "learning_rate": 0.00022500000000000002,
      "loss": 0.5046,
      "step": 232500
    },
    {
      "epoch": 0.7766666666666666,
      "grad_norm": 1.3164000511169434,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.4982,
      "step": 233000
    },
    {
      "epoch": 0.7783333333333333,
      "grad_norm": 1.021167516708374,
      "learning_rate": 0.00022166666666666667,
      "loss": 0.5105,
      "step": 233500
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.9912539124488831,
      "learning_rate": 0.00022,
      "loss": 0.503,
      "step": 234000
    },
    {
      "epoch": 0.7816666666666666,
      "grad_norm": 1.0000331401824951,
      "learning_rate": 0.00021833333333333332,
      "loss": 0.5048,
      "step": 234500
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.8709298968315125,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.5032,
      "step": 235000
    },
    {
      "epoch": 0.785,
      "grad_norm": 1.1668354272842407,
      "learning_rate": 0.000215,
      "loss": 0.5006,
      "step": 235500
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 1.111188292503357,
      "learning_rate": 0.00021333333333333336,
      "loss": 0.5062,
      "step": 236000
    },
    {
      "epoch": 0.7883333333333333,
      "grad_norm": 0.9263865351676941,
      "learning_rate": 0.00021166666666666667,
      "loss": 0.5051,
      "step": 236500
    },
    {
      "epoch": 0.79,
      "grad_norm": 1.2692173719406128,
      "learning_rate": 0.00021,
      "loss": 0.5,
      "step": 237000
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 1.0581151247024536,
      "learning_rate": 0.00020833333333333335,
      "loss": 0.4998,
      "step": 237500
    },
    {
      "epoch": 0.7933333333333333,
      "grad_norm": 1.0300357341766357,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.497,
      "step": 238000
    },
    {
      "epoch": 0.795,
      "grad_norm": 0.8570288419723511,
      "learning_rate": 0.000205,
      "loss": 0.4982,
      "step": 238500
    },
    {
      "epoch": 0.7966666666666666,
      "grad_norm": 1.0248618125915527,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.4987,
      "step": 239000
    },
    {
      "epoch": 0.7983333333333333,
      "grad_norm": 1.0416219234466553,
      "learning_rate": 0.00020166666666666667,
      "loss": 0.5053,
      "step": 239500
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.1995069980621338,
      "learning_rate": 0.0002,
      "loss": 0.5015,
      "step": 240000
    },
    {
      "epoch": 0.8016666666666666,
      "grad_norm": 1.2775180339813232,
      "learning_rate": 0.00019833333333333335,
      "loss": 0.4959,
      "step": 240500
    },
    {
      "epoch": 0.8033333333333333,
      "grad_norm": 1.1990032196044922,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.5039,
      "step": 241000
    },
    {
      "epoch": 0.805,
      "grad_norm": 1.060180425643921,
      "learning_rate": 0.00019500000000000002,
      "loss": 0.4966,
      "step": 241500
    },
    {
      "epoch": 0.8066666666666666,
      "grad_norm": 0.9916186332702637,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.4948,
      "step": 242000
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.9418348670005798,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.4843,
      "step": 242500
    },
    {
      "epoch": 0.81,
      "grad_norm": 1.0065540075302124,
      "learning_rate": 0.00019,
      "loss": 0.4919,
      "step": 243000
    },
    {
      "epoch": 0.8116666666666666,
      "grad_norm": 0.9251338243484497,
      "learning_rate": 0.00018833333333333332,
      "loss": 0.4922,
      "step": 243500
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 1.1401361227035522,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.4903,
      "step": 244000
    },
    {
      "epoch": 0.815,
      "grad_norm": 1.0151492357254028,
      "learning_rate": 0.000185,
      "loss": 0.4947,
      "step": 244500
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.8339800238609314,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.4946,
      "step": 245000
    },
    {
      "epoch": 0.8183333333333334,
      "grad_norm": 1.064070463180542,
      "learning_rate": 0.00018166666666666667,
      "loss": 0.4893,
      "step": 245500
    },
    {
      "epoch": 0.82,
      "grad_norm": 1.1789861917495728,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.4913,
      "step": 246000
    },
    {
      "epoch": 0.8216666666666667,
      "grad_norm": 1.07906174659729,
      "learning_rate": 0.00017833333333333335,
      "loss": 0.4901,
      "step": 246500
    },
    {
      "epoch": 0.8233333333333334,
      "grad_norm": 0.9304428696632385,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.488,
      "step": 247000
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.9326521754264832,
      "learning_rate": 0.000175,
      "loss": 0.4932,
      "step": 247500
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 1.0219436883926392,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4913,
      "step": 248000
    },
    {
      "epoch": 0.8283333333333334,
      "grad_norm": 1.1094204187393188,
      "learning_rate": 0.00017166666666666667,
      "loss": 0.4869,
      "step": 248500
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.9841704368591309,
      "learning_rate": 0.00017,
      "loss": 0.4882,
      "step": 249000
    },
    {
      "epoch": 0.8316666666666667,
      "grad_norm": 0.9633117318153381,
      "learning_rate": 0.00016833333333333335,
      "loss": 0.4863,
      "step": 249500
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.8424472212791443,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.4877,
      "step": 250000
    },
    {
      "epoch": 0.835,
      "grad_norm": 1.0011622905731201,
      "learning_rate": 0.000165,
      "loss": 0.4839,
      "step": 250500
    },
    {
      "epoch": 0.8366666666666667,
      "grad_norm": 0.8884057998657227,
      "learning_rate": 0.00016333333333333334,
      "loss": 0.4833,
      "step": 251000
    },
    {
      "epoch": 0.8383333333333334,
      "grad_norm": 0.9581071734428406,
      "learning_rate": 0.00016166666666666665,
      "loss": 0.4796,
      "step": 251500
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.9896061420440674,
      "learning_rate": 0.00016,
      "loss": 0.4878,
      "step": 252000
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 1.0837253332138062,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.4841,
      "step": 252500
    },
    {
      "epoch": 0.8433333333333334,
      "grad_norm": 0.8851087689399719,
      "learning_rate": 0.0001566666666666667,
      "loss": 0.4789,
      "step": 253000
    },
    {
      "epoch": 0.845,
      "grad_norm": 0.9289234280586243,
      "learning_rate": 0.000155,
      "loss": 0.4842,
      "step": 253500
    },
    {
      "epoch": 0.8466666666666667,
      "grad_norm": 0.98601233959198,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.4803,
      "step": 254000
    },
    {
      "epoch": 0.8483333333333334,
      "grad_norm": 0.9247301816940308,
      "learning_rate": 0.00015166666666666668,
      "loss": 0.4815,
      "step": 254500
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.9654126167297363,
      "learning_rate": 0.00015,
      "loss": 0.4813,
      "step": 255000
    },
    {
      "epoch": 0.8516666666666667,
      "grad_norm": 1.0349657535552979,
      "learning_rate": 0.00014833333333333335,
      "loss": 0.4871,
      "step": 255500
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.8302717208862305,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.4754,
      "step": 256000
    },
    {
      "epoch": 0.855,
      "grad_norm": 0.9119906425476074,
      "learning_rate": 0.000145,
      "loss": 0.4736,
      "step": 256500
    },
    {
      "epoch": 0.8566666666666667,
      "grad_norm": 0.7673966884613037,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.4802,
      "step": 257000
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.9025556445121765,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.4815,
      "step": 257500
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.030291199684143,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.4809,
      "step": 258000
    },
    {
      "epoch": 0.8616666666666667,
      "grad_norm": 0.9498413801193237,
      "learning_rate": 0.00013833333333333333,
      "loss": 0.473,
      "step": 258500
    },
    {
      "epoch": 0.8633333333333333,
      "grad_norm": 1.005268931388855,
      "learning_rate": 0.00013666666666666666,
      "loss": 0.4701,
      "step": 259000
    },
    {
      "epoch": 0.865,
      "grad_norm": 1.0691388845443726,
      "learning_rate": 0.000135,
      "loss": 0.48,
      "step": 259500
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.9835836887359619,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.4711,
      "step": 260000
    },
    {
      "epoch": 0.8683333333333333,
      "grad_norm": 1.089308738708496,
      "learning_rate": 0.00013166666666666665,
      "loss": 0.4767,
      "step": 260500
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.9417548775672913,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.4751,
      "step": 261000
    },
    {
      "epoch": 0.8716666666666667,
      "grad_norm": 1.074537992477417,
      "learning_rate": 0.00012833333333333333,
      "loss": 0.4698,
      "step": 261500
    },
    {
      "epoch": 0.8733333333333333,
      "grad_norm": 0.9971756339073181,
      "learning_rate": 0.0001266666666666667,
      "loss": 0.4704,
      "step": 262000
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.9990177154541016,
      "learning_rate": 0.000125,
      "loss": 0.4695,
      "step": 262500
    },
    {
      "epoch": 0.8766666666666667,
      "grad_norm": 1.0505708456039429,
      "learning_rate": 0.00012333333333333334,
      "loss": 0.4734,
      "step": 263000
    },
    {
      "epoch": 0.8783333333333333,
      "grad_norm": 0.9249941110610962,
      "learning_rate": 0.00012166666666666668,
      "loss": 0.4701,
      "step": 263500
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.8937660455703735,
      "learning_rate": 0.00012,
      "loss": 0.4707,
      "step": 264000
    },
    {
      "epoch": 0.8816666666666667,
      "grad_norm": 1.0002046823501587,
      "learning_rate": 0.00011833333333333334,
      "loss": 0.4674,
      "step": 264500
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 1.0825377702713013,
      "learning_rate": 0.00011666666666666667,
      "loss": 0.4725,
      "step": 265000
    },
    {
      "epoch": 0.885,
      "grad_norm": 1.1298855543136597,
      "learning_rate": 0.000115,
      "loss": 0.4681,
      "step": 265500
    },
    {
      "epoch": 0.8866666666666667,
      "grad_norm": 1.0678375959396362,
      "learning_rate": 0.00011333333333333333,
      "loss": 0.4639,
      "step": 266000
    },
    {
      "epoch": 0.8883333333333333,
      "grad_norm": 1.0375138521194458,
      "learning_rate": 0.00011166666666666667,
      "loss": 0.4668,
      "step": 266500
    },
    {
      "epoch": 0.89,
      "grad_norm": 1.1156365871429443,
      "learning_rate": 0.00011,
      "loss": 0.4675,
      "step": 267000
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 1.1961889266967773,
      "learning_rate": 0.00010833333333333334,
      "loss": 0.4641,
      "step": 267500
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 1.0872836112976074,
      "learning_rate": 0.00010666666666666668,
      "loss": 0.464,
      "step": 268000
    },
    {
      "epoch": 0.895,
      "grad_norm": 1.0118725299835205,
      "learning_rate": 0.000105,
      "loss": 0.4661,
      "step": 268500
    },
    {
      "epoch": 0.8966666666666666,
      "grad_norm": 0.8655041456222534,
      "learning_rate": 0.00010333333333333333,
      "loss": 0.4658,
      "step": 269000
    },
    {
      "epoch": 0.8983333333333333,
      "grad_norm": 0.9485609531402588,
      "learning_rate": 0.00010166666666666667,
      "loss": 0.4672,
      "step": 269500
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.9012961387634277,
      "learning_rate": 0.0001,
      "loss": 0.4653,
      "step": 270000
    },
    {
      "epoch": 0.9016666666666666,
      "grad_norm": 0.8971113562583923,
      "learning_rate": 9.833333333333333e-05,
      "loss": 0.462,
      "step": 270500
    },
    {
      "epoch": 0.9033333333333333,
      "grad_norm": 0.9171062707901001,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.4653,
      "step": 271000
    },
    {
      "epoch": 0.905,
      "grad_norm": 0.8430575132369995,
      "learning_rate": 9.5e-05,
      "loss": 0.4638,
      "step": 271500
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 1.1368151903152466,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.4597,
      "step": 272000
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 1.175061821937561,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.4589,
      "step": 272500
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.9043254852294922,
      "learning_rate": 8.999999999999999e-05,
      "loss": 0.4622,
      "step": 273000
    },
    {
      "epoch": 0.9116666666666666,
      "grad_norm": 1.0086007118225098,
      "learning_rate": 8.833333333333333e-05,
      "loss": 0.464,
      "step": 273500
    },
    {
      "epoch": 0.9133333333333333,
      "grad_norm": 0.8304619193077087,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.4607,
      "step": 274000
    },
    {
      "epoch": 0.915,
      "grad_norm": 0.8937017321586609,
      "learning_rate": 8.5e-05,
      "loss": 0.4567,
      "step": 274500
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 1.0185614824295044,
      "learning_rate": 8.333333333333333e-05,
      "loss": 0.4586,
      "step": 275000
    },
    {
      "epoch": 0.9183333333333333,
      "grad_norm": 0.8948560357093811,
      "learning_rate": 8.166666666666667e-05,
      "loss": 0.4546,
      "step": 275500
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.9740508198738098,
      "learning_rate": 8e-05,
      "loss": 0.4595,
      "step": 276000
    },
    {
      "epoch": 0.9216666666666666,
      "grad_norm": 1.0145626068115234,
      "learning_rate": 7.833333333333334e-05,
      "loss": 0.4573,
      "step": 276500
    },
    {
      "epoch": 0.9233333333333333,
      "grad_norm": 1.0606966018676758,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.4556,
      "step": 277000
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.8327505588531494,
      "learning_rate": 7.5e-05,
      "loss": 0.4545,
      "step": 277500
    },
    {
      "epoch": 0.9266666666666666,
      "grad_norm": 0.7979722023010254,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.4581,
      "step": 278000
    },
    {
      "epoch": 0.9283333333333333,
      "grad_norm": 0.9858145117759705,
      "learning_rate": 7.166666666666667e-05,
      "loss": 0.4534,
      "step": 278500
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.9609532952308655,
      "learning_rate": 7.000000000000001e-05,
      "loss": 0.4545,
      "step": 279000
    },
    {
      "epoch": 0.9316666666666666,
      "grad_norm": 1.0925053358078003,
      "learning_rate": 6.833333333333333e-05,
      "loss": 0.4569,
      "step": 279500
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.1309449672698975,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.4482,
      "step": 280000
    },
    {
      "epoch": 0.935,
      "grad_norm": 1.0254744291305542,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.4548,
      "step": 280500
    },
    {
      "epoch": 0.9366666666666666,
      "grad_norm": 0.9415109157562256,
      "learning_rate": 6.333333333333335e-05,
      "loss": 0.454,
      "step": 281000
    },
    {
      "epoch": 0.9383333333333334,
      "grad_norm": 0.967265784740448,
      "learning_rate": 6.166666666666667e-05,
      "loss": 0.4524,
      "step": 281500
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.109588384628296,
      "learning_rate": 6e-05,
      "loss": 0.4525,
      "step": 282000
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.9977110028266907,
      "learning_rate": 5.833333333333333e-05,
      "loss": 0.4522,
      "step": 282500
    },
    {
      "epoch": 0.9433333333333334,
      "grad_norm": 0.9440179467201233,
      "learning_rate": 5.6666666666666664e-05,
      "loss": 0.4517,
      "step": 283000
    },
    {
      "epoch": 0.945,
      "grad_norm": 1.1383787393569946,
      "learning_rate": 5.5e-05,
      "loss": 0.4511,
      "step": 283500
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 1.1139401197433472,
      "learning_rate": 5.333333333333334e-05,
      "loss": 0.4552,
      "step": 284000
    },
    {
      "epoch": 0.9483333333333334,
      "grad_norm": 1.080871820449829,
      "learning_rate": 5.1666666666666664e-05,
      "loss": 0.4512,
      "step": 284500
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.9502654671669006,
      "learning_rate": 5e-05,
      "loss": 0.4464,
      "step": 285000
    },
    {
      "epoch": 0.9516666666666667,
      "grad_norm": 0.8737215399742126,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.4543,
      "step": 285500
    },
    {
      "epoch": 0.9533333333333334,
      "grad_norm": 0.9225478172302246,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.4515,
      "step": 286000
    },
    {
      "epoch": 0.955,
      "grad_norm": 1.1606719493865967,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 0.4506,
      "step": 286500
    },
    {
      "epoch": 0.9566666666666667,
      "grad_norm": 0.9275038838386536,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.4501,
      "step": 287000
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.9489580392837524,
      "learning_rate": 4.1666666666666665e-05,
      "loss": 0.4517,
      "step": 287500
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.9682732224464417,
      "learning_rate": 4e-05,
      "loss": 0.4505,
      "step": 288000
    },
    {
      "epoch": 0.9616666666666667,
      "grad_norm": 0.8415055871009827,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.4441,
      "step": 288500
    },
    {
      "epoch": 0.9633333333333334,
      "grad_norm": 0.9723289608955383,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.4482,
      "step": 289000
    },
    {
      "epoch": 0.965,
      "grad_norm": 0.8178036212921143,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 0.4469,
      "step": 289500
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.9963613152503967,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.447,
      "step": 290000
    },
    {
      "epoch": 0.9683333333333334,
      "grad_norm": 0.8870278596878052,
      "learning_rate": 3.166666666666667e-05,
      "loss": 0.4453,
      "step": 290500
    },
    {
      "epoch": 0.97,
      "grad_norm": 1.0672063827514648,
      "learning_rate": 3e-05,
      "loss": 0.4497,
      "step": 291000
    },
    {
      "epoch": 0.9716666666666667,
      "grad_norm": 0.9461423754692078,
      "learning_rate": 2.8333333333333332e-05,
      "loss": 0.4489,
      "step": 291500
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 1.24132239818573,
      "learning_rate": 2.666666666666667e-05,
      "loss": 0.4439,
      "step": 292000
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.9229159951210022,
      "learning_rate": 2.5e-05,
      "loss": 0.4479,
      "step": 292500
    },
    {
      "epoch": 0.9766666666666667,
      "grad_norm": 1.1812598705291748,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.4473,
      "step": 293000
    },
    {
      "epoch": 0.9783333333333334,
      "grad_norm": 1.042038083076477,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.4428,
      "step": 293500
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.8708123564720154,
      "learning_rate": 2e-05,
      "loss": 0.4439,
      "step": 294000
    },
    {
      "epoch": 0.9816666666666667,
      "grad_norm": 0.992571234703064,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.4476,
      "step": 294500
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 1.052068829536438,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.4453,
      "step": 295000
    },
    {
      "epoch": 0.985,
      "grad_norm": 0.9729802012443542,
      "learning_rate": 1.5e-05,
      "loss": 0.4449,
      "step": 295500
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.9117842316627502,
      "learning_rate": 1.3333333333333335e-05,
      "loss": 0.4434,
      "step": 296000
    },
    {
      "epoch": 0.9883333333333333,
      "grad_norm": 0.875163197517395,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.4503,
      "step": 296500
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.9047272801399231,
      "learning_rate": 1e-05,
      "loss": 0.4453,
      "step": 297000
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 1.0861645936965942,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.4457,
      "step": 297500
    },
    {
      "epoch": 0.9933333333333333,
      "grad_norm": 1.1279003620147705,
      "learning_rate": 6.6666666666666675e-06,
      "loss": 0.4454,
      "step": 298000
    },
    {
      "epoch": 0.995,
      "grad_norm": 1.2036645412445068,
      "learning_rate": 5e-06,
      "loss": 0.4426,
      "step": 298500
    },
    {
      "epoch": 0.9966666666666667,
      "grad_norm": 0.9519504904747009,
      "learning_rate": 3.3333333333333337e-06,
      "loss": 0.4429,
      "step": 299000
    },
    {
      "epoch": 0.9983333333333333,
      "grad_norm": 0.9805644750595093,
      "learning_rate": 1.6666666666666669e-06,
      "loss": 0.4423,
      "step": 299500
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0075443983078003,
      "learning_rate": 0.0,
      "loss": 0.4397,
      "step": 300000
    }
  ],
  "logging_steps": 500,
  "max_steps": 300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 100000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3018189529088e+18,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
