{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.00471127, -0.87703357, -1.77814144, ...,  0.21711742,\n",
      "        0.6824449 ,  0.79433402]), array([-0.88416185, -0.67742994, -0.57299668, ..., -0.13854467,\n",
      "       -1.07855125, -0.84565664]), array([-2.3087271 , -0.46575622, -2.00959807, ...,  0.92804894,\n",
      "        2.16451684, -0.4679564 ]), array([ 1.89416338, -0.78414736, -1.13666841, ..., -0.12561028,\n",
      "        1.27326486,  0.11685418]), array([-0.59436787,  0.58941964,  0.23066557, ..., -0.34023957,\n",
      "       -1.21866771,  0.25249599]), array([ 1.41991292, -1.945818  ,  0.32531899, ..., -1.43923915,\n",
      "       -0.3392343 ,  0.26581192]), array([-0.26276713, -0.85839122, -0.87210831, ..., -1.07359215,\n",
      "        0.08793539, -1.44327399]), array([-0.79269271,  0.36110837,  0.83297512, ..., -0.53730619,\n",
      "        0.74786492, -0.27518795]), array([ 1.22323374, -0.03332825,  1.14575547, ..., -0.17670895,\n",
      "       -1.00923528,  1.36739977]), array([-0.00491114,  1.503291  , -0.58151146, ...,  1.33589366,\n",
      "        0.86223473, -0.34649358]), array([ 1.00570997, -1.05383766,  1.51929619, ...,  0.10857138,\n",
      "       -0.78800323,  0.6729116 ]), array([ 0.42674343, -1.13573043,  0.8634544 , ..., -0.74795987,\n",
      "        1.35721341, -0.28842513]), array([ 1.49654566,  1.56440048,  0.23122528, ..., -1.01999976,\n",
      "        0.84585602,  1.17366665]), array([ 0.55584812, -0.5371069 ,  1.00714325, ..., -0.54215766,\n",
      "       -0.25436703, -0.59541383]), array([ 0.0241066 , -2.42968572, -0.34206531, ..., -0.26669041,\n",
      "        0.7126318 , -1.99936201]), array([-1.43299931, -1.87526622,  0.40242159, ..., -0.30138416,\n",
      "       -0.150736  , -0.66032089]), array([-0.39984902, -0.16121787, -0.40249541, ..., -0.8707803 ,\n",
      "        1.84886206, -1.73834748]), array([-0.86996253, -0.9423753 , -1.34368854, ...,  0.73542151,\n",
      "        1.21576326,  0.55632411]), array([ 0.4272587 , -0.4563591 ,  1.57275636, ...,  0.67146687,\n",
      "       -1.4340011 , -0.37377365]), array([-2.42404773, -1.9467084 ,  1.14542688, ..., -0.97384473,\n",
      "        0.775079  ,  0.04057328])]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "\n",
    "def convert_to_arrow(\n",
    "    path: Union[str, Path],\n",
    "    time_series: Union[List[np.ndarray], np.ndarray],\n",
    "    compression: str = \"lz4\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Store a given set of series into Arrow format at the specified path.\n",
    "\n",
    "    Input data can be either a list of 1D numpy arrays, or a single 2D\n",
    "    numpy array of shape (num_series, time_length).\n",
    "    \"\"\"\n",
    "    assert isinstance(time_series, list) or (\n",
    "        isinstance(time_series, np.ndarray) and\n",
    "        time_series.ndim == 2\n",
    "    )\n",
    "\n",
    "    # Set an arbitrary start time\n",
    "    start = np.datetime64(\"2000-01-01 00:00\", \"s\")\n",
    "\n",
    "    dataset = [\n",
    "        {\"start\": start, \"target\": ts} for ts in time_series\n",
    "    ]\n",
    "\n",
    "    ArrowWriter(compression=compression).write_to_file(\n",
    "        dataset,\n",
    "        path=path,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate 20 random time series of length 1024\n",
    "    time_series = [np.random.randn(1024) for i in range(20)]\n",
    "    print(time_series)\n",
    "\n",
    "    # Convert to GluonTS arrow format\n",
    "    convert_to_arrow(\"./arrow_data/noise-data.arrow\", time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      timestamp  open  low  high  close     volume\n",
      "0    2001-04-16   432  407   436    432          0\n",
      "1    2001-04-17   432  407   436    432          0\n",
      "2    2001-04-18   432  407   436    432          0\n",
      "3    2001-04-19   432  407   436    432          0\n",
      "4    2001-04-20   432  407   436    432          0\n",
      "...         ...   ...  ...   ...    ...        ...\n",
      "3963 2016-06-23   730  730   745    730   39292500\n",
      "3964 2016-06-24   730  705   750    730  190710000\n",
      "3965 2016-06-27   730  730   745    735   70339400\n",
      "3966 2016-06-28   740  720   745    720   58690300\n",
      "3967 2016-06-29   720  720   740    720  105793700\n",
      "\n",
      "[3968 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "def convert_to_arrow(path: Union[str, Path], csv_path: str, compression: str = \"lz4\"):\n",
    "    data = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "\n",
    "    # Hitung jumlah total baris\n",
    "    total_rows = len(data)\n",
    "\n",
    "    # Hitung 70% dari total baris\n",
    "    rows_to_take = int(0.7 * total_rows)\n",
    "\n",
    "    # Ambil 70% pertama dari data\n",
    "    data = data.head(rows_to_take)\n",
    "\n",
    "    # Menampilkan hasil\n",
    "    print(data)\n",
    "\n",
    "    # time_series = data['close'].values.reshape(-1, 1)  # Reshape for a single time series\n",
    "\n",
    "    # # Set an arbitrary start time from your data\n",
    "    # start = data['timestamp'].iloc[0]\n",
    "\n",
    "    dataset = [{\"start\": ts, \"target\": [close]} for ts, close in zip(data['timestamp'], data['close'])]\n",
    "\n",
    "    # dataset = [{\"start\": start, \"target\": ts.flatten()} for ts in time_series]\n",
    "\n",
    "    ArrowWriter(compression=compression).write_to_file(dataset, path=path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_to_arrow(\"./arrow_data/ANTM.arrow\", \"/home/yogi/chronos-research/dataset/LQ45-daily/ANTM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****USE THIS*****\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "def convert_to_arrow(path: Union[str, Path], csv_path: str, compression: str = \"lz4\"):\n",
    "    \"\"\"\n",
    "    Store a given set of series into Arrow format at the specified path.\n",
    "\n",
    "    Input data can be a CSV file with a 'close' column.\n",
    "    \"\"\"\n",
    "    # Baca file CSV\n",
    "    data = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "\n",
    "    # Hitung jumlah total baris\n",
    "    total_rows = len(data)\n",
    "\n",
    "    # Hitung 70% dari total baris\n",
    "    rows_to_take = int(0.7 * total_rows)\n",
    "\n",
    "    # Ambil 70% pertama dari data\n",
    "    data = data.head(rows_to_take)\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Ambil array dari kolom 'close'\n",
    "    time_series = data['close'].to_numpy()\n",
    "    \n",
    "    # Set an arbitrary start time\n",
    "    start = np.datetime64(data['timestamp'].iloc[0], \"s\")\n",
    "\n",
    "    # Buat dataset yang terdiri dari satu seri waktu dengan satu start time\n",
    "    dataset = [{\"start\": start, \"target\": time_series}]\n",
    "\n",
    "    # Tulis dataset ke file dengan format arrow menggunakan ArrowWriter\n",
    "    ArrowWriter(compression=compression).write_to_file(dataset, path=path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sesuaikan path ke lokasi file CSV dan output yang diinginkan\n",
    "    convert_to_arrow(\"/home/yogi/chronos-research/arrow_data/test.arrow\", \"/home/yogi/chronos-research/dataset/LQ45-daily/ANTM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****USE THIS*****\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "def convert_to_arrow(csv_path: str):\n",
    "    \"\"\"\n",
    "    Store a given set of series into Arrow format at the specified path.\n",
    "\n",
    "    Input data can be a CSV file with a 'close' column.\n",
    "    \"\"\"\n",
    "    # Baca file CSV\n",
    "    data = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "\n",
    "    # Hitung jumlah total baris\n",
    "    total_rows = len(data)\n",
    "\n",
    "    # Hitung 70% dari total baris\n",
    "    rows_to_take = int(0.7 * total_rows)\n",
    "\n",
    "    # Ambil 70% pertama dari data\n",
    "    data = data.head(rows_to_take)\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Ambil array dari kolom 'close'\n",
    "    time_series = data['close'].to_numpy()\n",
    "    \n",
    "    # Set an arbitrary start time\n",
    "    start = np.datetime64(data['timestamp'].iloc[0], \"s\")\n",
    "\n",
    "    # Buat dataset yang terdiri dari satu seri waktu dengan satu start time\n",
    "    dataset = {\"start\": start, \"target\": time_series}\n",
    "\n",
    "    return dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_name=[\"ACES\",\"AMRT\",\"ASII\",\"BBRI\",\"BRIS\",\"CPIN\",\"GGRM\",\"ICBP\",\n",
    "                  \"INKP\",\"ITMG\",\"MDKA\",\"PGAS\",\"SMGR\",\"TOWR\",\"ADRO\",\"ANTM\",\n",
    "                  \"BBCA\",\"BBTN\",\"BRPT\",\"ESSA\",\"GOTO\",\"INCO\",\"INTP\",\"KLBF\",\n",
    "                  \"MEDC\",\"PTBA\",\"SRTG\",\"UNTR\",\"AKRA\",\"ARTO\",\"BBNI\",\"BMRI\",\n",
    "                  \"BUKA\",\"EXCL\",\"HRUM\",\"INDF\",\"ISAT\",\"MAPI\",\"MTEL\",\"SIDO\",\n",
    "                  \"TLKM\",\"UNVR\"]\n",
    "\n",
    "    dataset =[]\n",
    "\n",
    "    for ds in dataset_name:\n",
    "        # Sesuaikan path ke lokasi file CSV dan output yang diinginkan\n",
    "        dataset.append(convert_to_arrow(f\"/home/yogi/chronos-research/dataset/LQ45-daily/{ds}.csv\"))\n",
    "\n",
    "    # Tulis dataset ke file dengan format arrow menggunakan ArrowWriter\n",
    "    ArrowWriter(compression=\"lz4\").write_to_file(dataset, path=\"/home/yogi/chronos-research/arrow_data/training.arrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        start                                             target\n",
      "0  2000-01-01  [0.004711269550239207, -0.8770335680325148, -1...\n",
      "1  2000-01-01  [-0.8841618518925635, -0.6774299429829096, -0....\n",
      "2  2000-01-01  [-2.3087271033101517, -0.4657562171381893, -2....\n",
      "3  2000-01-01  [1.8941633827896915, -0.7841473550389715, -1.1...\n",
      "4  2000-01-01  [-0.5943678675791243, 0.5894196369908294, 0.23...\n",
      "5  2000-01-01  [1.4199129205538275, -1.9458179988061894, 0.32...\n",
      "6  2000-01-01  [-0.2627671290535644, -0.8583912247861986, -0....\n",
      "7  2000-01-01  [-0.7926927119098559, 0.3611083673682498, 0.83...\n",
      "8  2000-01-01  [1.2232337375981264, -0.033328254807924146, 1....\n",
      "9  2000-01-01  [-0.0049111366678351994, 1.5032909958332126, -...\n",
      "10 2000-01-01  [1.0057099719862919, -1.05383766194054, 1.5192...\n",
      "11 2000-01-01  [0.4267434250158014, -1.1357304273660396, 0.86...\n",
      "12 2000-01-01  [1.4965456609573944, 1.5644004783960737, 0.231...\n",
      "13 2000-01-01  [0.5558481200728136, -0.5371069049176778, 1.00...\n",
      "14 2000-01-01  [0.024106600521738332, -2.4296857233750666, -0...\n",
      "15 2000-01-01  [-1.432999313917302, -1.8752662188679166, 0.40...\n",
      "16 2000-01-01  [-0.3998490230220926, -0.1612178737186876, -0....\n",
      "17 2000-01-01  [-0.8699625275736672, -0.942375295304742, -1.3...\n",
      "18 2000-01-01  [0.42725869618742707, -0.45635910189419476, 1....\n",
      "19 2000-01-01  [-2.424047728288408, -1.9467083967121193, 1.14...\n",
      "        start                                             target\n",
      "0  2007-11-06  [98, 101, 99, 95, 90, 86, 85, 87, 85, 84, 87, ...\n",
      "1  2009-01-15  [39, 40, 40, 39, 39, 39, 37, 37, 38, 40, 39, 4...\n",
      "2  2001-04-16  [98, 96, 98, 96, 88, 88, 90, 98, 98, 100, 108,...\n",
      "3  2003-11-10  [97, 100, 105, 105, 105, 100, 97, 100, 102, 10...\n",
      "4  2018-05-09  [545, 545, 620, 600, 600, 600, 585, 570, 555, ...\n",
      "5  2001-04-16  [31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 3...\n",
      "6  2001-04-16  [12100, 12100, 12100, 12100, 12100, 12100, 121...\n",
      "7  2010-10-07  [2975, 2850, 2725, 2775, 2800, 2775, 2725, 272...\n",
      "8  2001-04-16  [230, 235, 220, 200, 215, 230, 245, 240, 240, ...\n",
      "9  2007-12-18  [19600, 19000, 19000, 19000, 19000, 19000, 185...\n",
      "10 2015-06-19  [435, 422, 401, 391, 384, 384, 383, 383, 383, ...\n",
      "11 2003-12-15  [310, 300, 300, 305, 310, 305, 305, 305, 305, ...\n",
      "12 2001-04-16  [1750, 1750, 1750, 1750, 1750, 1750, 1750, 175...\n",
      "13 2010-03-08  [31, 39, 49, 61, 50, 46, 46, 44, 41, 41, 51, 5...\n",
      "14 2008-07-16  [1730, 1700, 1640, 1680, 1670, 1660, 1630, 163...\n",
      "15 2001-04-16  [432, 432, 432, 432, 432, 432, 432, 432, 432, ...\n",
      "16 2001-04-16  [177, 177, 177, 177, 177, 177, 177, 177, 177, ...\n",
      "17 2009-12-17  [807, 807, 797, 797, 807, 807, 807, 797, 807, ...\n",
      "18 2001-04-16  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, ...\n",
      "19 2012-02-01  [79, 90, 94, 93, 100, 103, 104, 99, 104, 120, ...\n",
      "20 2022-04-11  [382, 370, 374, 376, 376, 378, 358, 338, 340, ...\n",
      "21 2001-04-16  [1520, 1520, 1520, 1520, 1520, 1520, 1520, 152...\n",
      "22 2001-04-16  [1100, 1100, 1100, 1025, 975, 1000, 1075, 1075...\n",
      "23 2001-04-16  [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 4...\n",
      "24 2001-04-16  [157, 157, 157, 157, 152, 152, 152, 157, 157, ...\n",
      "25 2002-12-23  [120, 120, 120, 120, 120, 120, 120, 120, 115, ...\n",
      "26 2013-06-26  [910, 930, 915, 915, 920, 910, 915, 930, 930, ...\n",
      "27 2001-04-16  [312, 312, 298, 262, 243, 239, 239, 252, 252, ...\n",
      "28 2001-04-16  [53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 5...\n",
      "29 2016-01-12  [173, 161, 145, 134, 128, 128, 125, 124, 127, ...\n",
      "30 2001-04-16  [1232, 1232, 1232, 1232, 1232, 1232, 1232, 123...\n",
      "31 2003-07-14  [417, 417, 405, 417, 417, 417, 417, 417, 405, ...\n",
      "32 2021-08-06  [1060, 1110, 1035, 1035, 965, 955, 890, 890, 8...\n",
      "33 2005-09-29  [2268, 2391, 2441, 2490, 2835, 3403, 3698, 399...\n",
      "34 2010-10-06  [5450, 5450, 5350, 5250, 5300, 5400, 5450, 545...\n",
      "35 2001-04-16  [775, 800, 775, 775, 775, 750, 750, 750, 750, ...\n",
      "36 2001-04-16  [3500, 3500, 3500, 3500, 3500, 3500, 3500, 350...\n",
      "37 2004-11-10  [70, 62, 67, 67, 67, 67, 67, 67, 67, 67, 67, 7...\n",
      "38 2021-11-22  [765, 775, 775, 775, 770, 760, 780, 780, 785, ...\n",
      "39 2013-12-18  [347, 342, 342, 342, 337, 337, 337, 342, 347, ...\n",
      "40 2001-04-16  [825, 825, 825, 825, 825, 825, 825, 825, 825, ...\n",
      "41 2001-04-16  [675, 675, 675, 675, 675, 675, 675, 675, 675, ...\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "def read_arrow_file(file_path):\n",
    "    with pa.OSFile(file_path, 'rb') as f:\n",
    "        reader = pa.ipc.open_file(f)\n",
    "        table = reader.read_all()\n",
    "    print(table.to_pandas())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read_arrow_file(\"./arrow_data/noise-data.arrow\")\n",
    "    read_arrow_file(\"/home/yogi/chronos-research/arrow_data/noise-data.arrow\")\n",
    "    read_arrow_file(\"/home/yogi/chronos-research/arrow_data/training.arrow\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos-zero-shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
