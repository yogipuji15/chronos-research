{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6666666666666666,
  "eval_steps": 500,
  "global_step": 200000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.3145247995853424,
      "learning_rate": 0.0009983333333333333,
      "loss": 2.048,
      "step": 500
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.3670884370803833,
      "learning_rate": 0.0009966666666666668,
      "loss": 2.0321,
      "step": 1000
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.4369833171367645,
      "learning_rate": 0.000995,
      "loss": 2.023,
      "step": 1500
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.30696558952331543,
      "learning_rate": 0.0009933333333333333,
      "loss": 1.9874,
      "step": 2000
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.39862170815467834,
      "learning_rate": 0.0009916666666666667,
      "loss": 1.9806,
      "step": 2500
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.3430022597312927,
      "learning_rate": 0.00099,
      "loss": 1.9729,
      "step": 3000
    },
    {
      "epoch": 0.011666666666666667,
      "grad_norm": 0.4105353057384491,
      "learning_rate": 0.0009883333333333333,
      "loss": 1.9466,
      "step": 3500
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.3809437155723572,
      "learning_rate": 0.0009866666666666667,
      "loss": 1.9226,
      "step": 4000
    },
    {
      "epoch": 0.015,
      "grad_norm": 0.48969414830207825,
      "learning_rate": 0.000985,
      "loss": 1.9215,
      "step": 4500
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 0.4875754117965698,
      "learning_rate": 0.0009833333333333332,
      "loss": 1.9123,
      "step": 5000
    },
    {
      "epoch": 0.018333333333333333,
      "grad_norm": 0.4519388973712921,
      "learning_rate": 0.0009816666666666667,
      "loss": 1.8886,
      "step": 5500
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.5023013353347778,
      "learning_rate": 0.00098,
      "loss": 1.8862,
      "step": 6000
    },
    {
      "epoch": 0.021666666666666667,
      "grad_norm": 0.5382319688796997,
      "learning_rate": 0.0009783333333333334,
      "loss": 1.8641,
      "step": 6500
    },
    {
      "epoch": 0.023333333333333334,
      "grad_norm": 0.5118377804756165,
      "learning_rate": 0.0009766666666666667,
      "loss": 1.837,
      "step": 7000
    },
    {
      "epoch": 0.025,
      "grad_norm": 0.5474888682365417,
      "learning_rate": 0.000975,
      "loss": 1.8397,
      "step": 7500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.5352814197540283,
      "learning_rate": 0.0009733333333333334,
      "loss": 1.8261,
      "step": 8000
    },
    {
      "epoch": 0.028333333333333332,
      "grad_norm": 0.47805410623550415,
      "learning_rate": 0.0009716666666666667,
      "loss": 1.8024,
      "step": 8500
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.569683849811554,
      "learning_rate": 0.0009699999999999999,
      "loss": 1.793,
      "step": 9000
    },
    {
      "epoch": 0.03166666666666667,
      "grad_norm": 0.5558898448944092,
      "learning_rate": 0.0009683333333333334,
      "loss": 1.7676,
      "step": 9500
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 0.5806165933609009,
      "learning_rate": 0.0009666666666666667,
      "loss": 1.759,
      "step": 10000
    },
    {
      "epoch": 0.035,
      "grad_norm": 0.5116707682609558,
      "learning_rate": 0.000965,
      "loss": 1.7278,
      "step": 10500
    },
    {
      "epoch": 0.03666666666666667,
      "grad_norm": 0.6247261166572571,
      "learning_rate": 0.0009633333333333334,
      "loss": 1.7186,
      "step": 11000
    },
    {
      "epoch": 0.03833333333333333,
      "grad_norm": 0.5599643588066101,
      "learning_rate": 0.0009616666666666667,
      "loss": 1.7052,
      "step": 11500
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.5901594161987305,
      "learning_rate": 0.00096,
      "loss": 1.6977,
      "step": 12000
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 0.6362661123275757,
      "learning_rate": 0.0009583333333333334,
      "loss": 1.688,
      "step": 12500
    },
    {
      "epoch": 0.043333333333333335,
      "grad_norm": 0.594735860824585,
      "learning_rate": 0.0009566666666666666,
      "loss": 1.6736,
      "step": 13000
    },
    {
      "epoch": 0.045,
      "grad_norm": 0.700469434261322,
      "learning_rate": 0.000955,
      "loss": 1.6689,
      "step": 13500
    },
    {
      "epoch": 0.04666666666666667,
      "grad_norm": 0.815446674823761,
      "learning_rate": 0.0009533333333333334,
      "loss": 1.6462,
      "step": 14000
    },
    {
      "epoch": 0.04833333333333333,
      "grad_norm": 0.6297082901000977,
      "learning_rate": 0.0009516666666666666,
      "loss": 1.6443,
      "step": 14500
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.615233838558197,
      "learning_rate": 0.00095,
      "loss": 1.6179,
      "step": 15000
    },
    {
      "epoch": 0.051666666666666666,
      "grad_norm": 0.6963945031166077,
      "learning_rate": 0.0009483333333333334,
      "loss": 1.6269,
      "step": 15500
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.8352037668228149,
      "learning_rate": 0.0009466666666666667,
      "loss": 1.5874,
      "step": 16000
    },
    {
      "epoch": 0.055,
      "grad_norm": 0.6634514331817627,
      "learning_rate": 0.000945,
      "loss": 1.5984,
      "step": 16500
    },
    {
      "epoch": 0.056666666666666664,
      "grad_norm": 0.5913524627685547,
      "learning_rate": 0.0009433333333333334,
      "loss": 1.5768,
      "step": 17000
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 0.6899961233139038,
      "learning_rate": 0.0009416666666666667,
      "loss": 1.5629,
      "step": 17500
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.7048971652984619,
      "learning_rate": 0.00094,
      "loss": 1.5471,
      "step": 18000
    },
    {
      "epoch": 0.06166666666666667,
      "grad_norm": 0.811085045337677,
      "learning_rate": 0.0009383333333333333,
      "loss": 1.5521,
      "step": 18500
    },
    {
      "epoch": 0.06333333333333334,
      "grad_norm": 0.6933568716049194,
      "learning_rate": 0.0009366666666666667,
      "loss": 1.5448,
      "step": 19000
    },
    {
      "epoch": 0.065,
      "grad_norm": 1.012321949005127,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.5284,
      "step": 19500
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.7144045233726501,
      "learning_rate": 0.0009333333333333333,
      "loss": 1.5209,
      "step": 20000
    },
    {
      "epoch": 0.06833333333333333,
      "grad_norm": 0.7372322082519531,
      "learning_rate": 0.0009316666666666667,
      "loss": 1.5131,
      "step": 20500
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.8823671340942383,
      "learning_rate": 0.00093,
      "loss": 1.499,
      "step": 21000
    },
    {
      "epoch": 0.07166666666666667,
      "grad_norm": 0.8584184050559998,
      "learning_rate": 0.0009283333333333333,
      "loss": 1.5006,
      "step": 21500
    },
    {
      "epoch": 0.07333333333333333,
      "grad_norm": 0.9252480864524841,
      "learning_rate": 0.0009266666666666667,
      "loss": 1.4639,
      "step": 22000
    },
    {
      "epoch": 0.075,
      "grad_norm": 0.7121914625167847,
      "learning_rate": 0.000925,
      "loss": 1.4607,
      "step": 22500
    },
    {
      "epoch": 0.07666666666666666,
      "grad_norm": 0.7434954047203064,
      "learning_rate": 0.0009233333333333334,
      "loss": 1.4554,
      "step": 23000
    },
    {
      "epoch": 0.07833333333333334,
      "grad_norm": 0.7978551387786865,
      "learning_rate": 0.0009216666666666667,
      "loss": 1.4507,
      "step": 23500
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.8290761709213257,
      "learning_rate": 0.00092,
      "loss": 1.4367,
      "step": 24000
    },
    {
      "epoch": 0.08166666666666667,
      "grad_norm": 0.8549162149429321,
      "learning_rate": 0.0009183333333333334,
      "loss": 1.4296,
      "step": 24500
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 0.8267479538917542,
      "learning_rate": 0.0009166666666666666,
      "loss": 1.4207,
      "step": 25000
    },
    {
      "epoch": 0.085,
      "grad_norm": 0.9709092378616333,
      "learning_rate": 0.000915,
      "loss": 1.4057,
      "step": 25500
    },
    {
      "epoch": 0.08666666666666667,
      "grad_norm": 0.8264050483703613,
      "learning_rate": 0.0009133333333333334,
      "loss": 1.408,
      "step": 26000
    },
    {
      "epoch": 0.08833333333333333,
      "grad_norm": 0.9368544816970825,
      "learning_rate": 0.0009116666666666666,
      "loss": 1.4075,
      "step": 26500
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.8442668914794922,
      "learning_rate": 0.00091,
      "loss": 1.3881,
      "step": 27000
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.8220694065093994,
      "learning_rate": 0.0009083333333333334,
      "loss": 1.3824,
      "step": 27500
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.9085817337036133,
      "learning_rate": 0.0009066666666666666,
      "loss": 1.3731,
      "step": 28000
    },
    {
      "epoch": 0.095,
      "grad_norm": 0.8560416102409363,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.3677,
      "step": 28500
    },
    {
      "epoch": 0.09666666666666666,
      "grad_norm": 0.8761775493621826,
      "learning_rate": 0.0009033333333333334,
      "loss": 1.3579,
      "step": 29000
    },
    {
      "epoch": 0.09833333333333333,
      "grad_norm": 0.8525266647338867,
      "learning_rate": 0.0009016666666666666,
      "loss": 1.3602,
      "step": 29500
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.8696973919868469,
      "learning_rate": 0.0009000000000000001,
      "loss": 1.3491,
      "step": 30000
    },
    {
      "epoch": 0.10166666666666667,
      "grad_norm": 0.9217604398727417,
      "learning_rate": 0.0008983333333333333,
      "loss": 1.3356,
      "step": 30500
    },
    {
      "epoch": 0.10333333333333333,
      "grad_norm": 0.8074062466621399,
      "learning_rate": 0.0008966666666666666,
      "loss": 1.3342,
      "step": 31000
    },
    {
      "epoch": 0.105,
      "grad_norm": 1.0166605710983276,
      "learning_rate": 0.0008950000000000001,
      "loss": 1.3209,
      "step": 31500
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.8933285474777222,
      "learning_rate": 0.0008933333333333333,
      "loss": 1.3189,
      "step": 32000
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.869965136051178,
      "learning_rate": 0.0008916666666666667,
      "loss": 1.3042,
      "step": 32500
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.8371292352676392,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.3123,
      "step": 33000
    },
    {
      "epoch": 0.11166666666666666,
      "grad_norm": 0.8848492503166199,
      "learning_rate": 0.0008883333333333333,
      "loss": 1.2917,
      "step": 33500
    },
    {
      "epoch": 0.11333333333333333,
      "grad_norm": 0.9162200689315796,
      "learning_rate": 0.0008866666666666667,
      "loss": 1.2882,
      "step": 34000
    },
    {
      "epoch": 0.115,
      "grad_norm": 1.0902690887451172,
      "learning_rate": 0.000885,
      "loss": 1.2869,
      "step": 34500
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.8383001089096069,
      "learning_rate": 0.0008833333333333333,
      "loss": 1.2857,
      "step": 35000
    },
    {
      "epoch": 0.11833333333333333,
      "grad_norm": 0.9840482473373413,
      "learning_rate": 0.0008816666666666668,
      "loss": 1.2713,
      "step": 35500
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9618729948997498,
      "learning_rate": 0.00088,
      "loss": 1.2598,
      "step": 36000
    },
    {
      "epoch": 0.12166666666666667,
      "grad_norm": 0.8798874616622925,
      "learning_rate": 0.0008783333333333333,
      "loss": 1.2587,
      "step": 36500
    },
    {
      "epoch": 0.12333333333333334,
      "grad_norm": 0.8348428010940552,
      "learning_rate": 0.0008766666666666668,
      "loss": 1.2577,
      "step": 37000
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.9253926873207092,
      "learning_rate": 0.000875,
      "loss": 1.246,
      "step": 37500
    },
    {
      "epoch": 0.12666666666666668,
      "grad_norm": 0.9128542542457581,
      "learning_rate": 0.0008733333333333333,
      "loss": 1.2434,
      "step": 38000
    },
    {
      "epoch": 0.12833333333333333,
      "grad_norm": 0.9456049799919128,
      "learning_rate": 0.0008716666666666667,
      "loss": 1.2417,
      "step": 38500
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.8424668908119202,
      "learning_rate": 0.00087,
      "loss": 1.2265,
      "step": 39000
    },
    {
      "epoch": 0.13166666666666665,
      "grad_norm": 0.7750054597854614,
      "learning_rate": 0.0008683333333333333,
      "loss": 1.2207,
      "step": 39500
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.965517520904541,
      "learning_rate": 0.0008666666666666667,
      "loss": 1.2284,
      "step": 40000
    },
    {
      "epoch": 0.135,
      "grad_norm": 0.9375702142715454,
      "learning_rate": 0.000865,
      "loss": 1.2113,
      "step": 40500
    },
    {
      "epoch": 0.13666666666666666,
      "grad_norm": 0.9793090224266052,
      "learning_rate": 0.0008633333333333334,
      "loss": 1.2043,
      "step": 41000
    },
    {
      "epoch": 0.13833333333333334,
      "grad_norm": 0.9772701859474182,
      "learning_rate": 0.0008616666666666667,
      "loss": 1.1918,
      "step": 41500
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.9484562873840332,
      "learning_rate": 0.00086,
      "loss": 1.1858,
      "step": 42000
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.881858766078949,
      "learning_rate": 0.0008583333333333333,
      "loss": 1.1857,
      "step": 42500
    },
    {
      "epoch": 0.14333333333333334,
      "grad_norm": 0.8682808876037598,
      "learning_rate": 0.0008566666666666667,
      "loss": 1.1727,
      "step": 43000
    },
    {
      "epoch": 0.145,
      "grad_norm": 0.8330632448196411,
      "learning_rate": 0.000855,
      "loss": 1.1783,
      "step": 43500
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.8961045145988464,
      "learning_rate": 0.0008533333333333334,
      "loss": 1.1776,
      "step": 44000
    },
    {
      "epoch": 0.14833333333333334,
      "grad_norm": 0.9423724412918091,
      "learning_rate": 0.0008516666666666667,
      "loss": 1.1759,
      "step": 44500
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.985970675945282,
      "learning_rate": 0.00085,
      "loss": 1.1632,
      "step": 45000
    },
    {
      "epoch": 0.15166666666666667,
      "grad_norm": 0.992302417755127,
      "learning_rate": 0.0008483333333333334,
      "loss": 1.1546,
      "step": 45500
    },
    {
      "epoch": 0.15333333333333332,
      "grad_norm": 1.0257148742675781,
      "learning_rate": 0.0008466666666666667,
      "loss": 1.1551,
      "step": 46000
    },
    {
      "epoch": 0.155,
      "grad_norm": 0.9003452658653259,
      "learning_rate": 0.0008449999999999999,
      "loss": 1.1565,
      "step": 46500
    },
    {
      "epoch": 0.15666666666666668,
      "grad_norm": 1.0703679323196411,
      "learning_rate": 0.0008433333333333334,
      "loss": 1.1418,
      "step": 47000
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.9371930360794067,
      "learning_rate": 0.0008416666666666667,
      "loss": 1.1425,
      "step": 47500
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.9073662757873535,
      "learning_rate": 0.00084,
      "loss": 1.1363,
      "step": 48000
    },
    {
      "epoch": 0.16166666666666665,
      "grad_norm": 1.043548345565796,
      "learning_rate": 0.0008383333333333334,
      "loss": 1.1344,
      "step": 48500
    },
    {
      "epoch": 0.16333333333333333,
      "grad_norm": 0.9665871262550354,
      "learning_rate": 0.0008366666666666667,
      "loss": 1.1204,
      "step": 49000
    },
    {
      "epoch": 0.165,
      "grad_norm": 1.022303819656372,
      "learning_rate": 0.000835,
      "loss": 1.129,
      "step": 49500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.9595053195953369,
      "learning_rate": 0.0008333333333333334,
      "loss": 1.1231,
      "step": 50000
    },
    {
      "epoch": 0.16833333333333333,
      "grad_norm": 0.9304542541503906,
      "learning_rate": 0.0008316666666666666,
      "loss": 1.1179,
      "step": 50500
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.9225609302520752,
      "learning_rate": 0.00083,
      "loss": 1.1138,
      "step": 51000
    },
    {
      "epoch": 0.17166666666666666,
      "grad_norm": 1.0508579015731812,
      "learning_rate": 0.0008283333333333334,
      "loss": 1.0993,
      "step": 51500
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.9037558436393738,
      "learning_rate": 0.0008266666666666666,
      "loss": 1.0921,
      "step": 52000
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.9017411470413208,
      "learning_rate": 0.000825,
      "loss": 1.0854,
      "step": 52500
    },
    {
      "epoch": 0.17666666666666667,
      "grad_norm": 0.9588157534599304,
      "learning_rate": 0.0008233333333333334,
      "loss": 1.0905,
      "step": 53000
    },
    {
      "epoch": 0.17833333333333334,
      "grad_norm": 0.8999327421188354,
      "learning_rate": 0.0008216666666666667,
      "loss": 1.0781,
      "step": 53500
    },
    {
      "epoch": 0.18,
      "grad_norm": 1.2644951343536377,
      "learning_rate": 0.00082,
      "loss": 1.085,
      "step": 54000
    },
    {
      "epoch": 0.18166666666666667,
      "grad_norm": 0.9346084594726562,
      "learning_rate": 0.0008183333333333333,
      "loss": 1.0726,
      "step": 54500
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.0016111135482788,
      "learning_rate": 0.0008166666666666667,
      "loss": 1.0664,
      "step": 55000
    },
    {
      "epoch": 0.185,
      "grad_norm": 1.0392929315567017,
      "learning_rate": 0.000815,
      "loss": 1.0701,
      "step": 55500
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 1.0784858465194702,
      "learning_rate": 0.0008133333333333333,
      "loss": 1.0689,
      "step": 56000
    },
    {
      "epoch": 0.18833333333333332,
      "grad_norm": 0.9385334253311157,
      "learning_rate": 0.0008116666666666667,
      "loss": 1.0529,
      "step": 56500
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.9181732535362244,
      "learning_rate": 0.0008100000000000001,
      "loss": 1.0473,
      "step": 57000
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.9301743507385254,
      "learning_rate": 0.0008083333333333333,
      "loss": 1.0454,
      "step": 57500
    },
    {
      "epoch": 0.19333333333333333,
      "grad_norm": 0.9208477139472961,
      "learning_rate": 0.0008066666666666667,
      "loss": 1.0406,
      "step": 58000
    },
    {
      "epoch": 0.195,
      "grad_norm": 0.8477672934532166,
      "learning_rate": 0.000805,
      "loss": 1.0396,
      "step": 58500
    },
    {
      "epoch": 0.19666666666666666,
      "grad_norm": 0.9434536695480347,
      "learning_rate": 0.0008033333333333333,
      "loss": 1.0403,
      "step": 59000
    },
    {
      "epoch": 0.19833333333333333,
      "grad_norm": 0.8830878734588623,
      "learning_rate": 0.0008016666666666667,
      "loss": 1.039,
      "step": 59500
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.9186842441558838,
      "learning_rate": 0.0008,
      "loss": 1.0382,
      "step": 60000
    },
    {
      "epoch": 0.20166666666666666,
      "grad_norm": 0.94227135181427,
      "learning_rate": 0.0007983333333333334,
      "loss": 1.0214,
      "step": 60500
    },
    {
      "epoch": 0.20333333333333334,
      "grad_norm": 1.030145525932312,
      "learning_rate": 0.0007966666666666667,
      "loss": 1.0305,
      "step": 61000
    },
    {
      "epoch": 0.205,
      "grad_norm": 0.9480907320976257,
      "learning_rate": 0.000795,
      "loss": 1.0346,
      "step": 61500
    },
    {
      "epoch": 0.20666666666666667,
      "grad_norm": 0.9081334471702576,
      "learning_rate": 0.0007933333333333334,
      "loss": 1.017,
      "step": 62000
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.9496847987174988,
      "learning_rate": 0.0007916666666666666,
      "loss": 1.0049,
      "step": 62500
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.9969865083694458,
      "learning_rate": 0.00079,
      "loss": 1.0062,
      "step": 63000
    },
    {
      "epoch": 0.21166666666666667,
      "grad_norm": 0.9667900800704956,
      "learning_rate": 0.0007883333333333334,
      "loss": 1.0004,
      "step": 63500
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 1.0050307512283325,
      "learning_rate": 0.0007866666666666666,
      "loss": 0.9914,
      "step": 64000
    },
    {
      "epoch": 0.215,
      "grad_norm": 0.947862982749939,
      "learning_rate": 0.000785,
      "loss": 0.9905,
      "step": 64500
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.0415217876434326,
      "learning_rate": 0.0007833333333333334,
      "loss": 0.9927,
      "step": 65000
    },
    {
      "epoch": 0.21833333333333332,
      "grad_norm": 0.9923322796821594,
      "learning_rate": 0.0007816666666666666,
      "loss": 0.9799,
      "step": 65500
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.0935271978378296,
      "learning_rate": 0.0007800000000000001,
      "loss": 0.9672,
      "step": 66000
    },
    {
      "epoch": 0.22166666666666668,
      "grad_norm": 0.8746712803840637,
      "learning_rate": 0.0007783333333333334,
      "loss": 0.9798,
      "step": 66500
    },
    {
      "epoch": 0.22333333333333333,
      "grad_norm": 1.0607365369796753,
      "learning_rate": 0.0007766666666666666,
      "loss": 0.9847,
      "step": 67000
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.9717281460762024,
      "learning_rate": 0.0007750000000000001,
      "loss": 0.975,
      "step": 67500
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.9713417887687683,
      "learning_rate": 0.0007733333333333333,
      "loss": 0.9761,
      "step": 68000
    },
    {
      "epoch": 0.22833333333333333,
      "grad_norm": 0.9662894010543823,
      "learning_rate": 0.0007716666666666666,
      "loss": 0.9709,
      "step": 68500
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.9635417461395264,
      "learning_rate": 0.0007700000000000001,
      "loss": 0.9638,
      "step": 69000
    },
    {
      "epoch": 0.23166666666666666,
      "grad_norm": 1.0611015558242798,
      "learning_rate": 0.0007683333333333333,
      "loss": 0.9579,
      "step": 69500
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.052783727645874,
      "learning_rate": 0.0007666666666666667,
      "loss": 0.9479,
      "step": 70000
    },
    {
      "epoch": 0.235,
      "grad_norm": 0.9874024987220764,
      "learning_rate": 0.0007650000000000001,
      "loss": 0.9487,
      "step": 70500
    },
    {
      "epoch": 0.23666666666666666,
      "grad_norm": 0.9095467925071716,
      "learning_rate": 0.0007633333333333333,
      "loss": 0.953,
      "step": 71000
    },
    {
      "epoch": 0.23833333333333334,
      "grad_norm": 0.9150189161300659,
      "learning_rate": 0.0007616666666666667,
      "loss": 0.9527,
      "step": 71500
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9862821698188782,
      "learning_rate": 0.00076,
      "loss": 0.9505,
      "step": 72000
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.0377775430679321,
      "learning_rate": 0.0007583333333333333,
      "loss": 0.9362,
      "step": 72500
    },
    {
      "epoch": 0.24333333333333335,
      "grad_norm": 1.0778406858444214,
      "learning_rate": 0.0007566666666666668,
      "loss": 0.9399,
      "step": 73000
    },
    {
      "epoch": 0.245,
      "grad_norm": 0.8017762303352356,
      "learning_rate": 0.000755,
      "loss": 0.932,
      "step": 73500
    },
    {
      "epoch": 0.24666666666666667,
      "grad_norm": 0.9538426399230957,
      "learning_rate": 0.0007533333333333333,
      "loss": 0.9247,
      "step": 74000
    },
    {
      "epoch": 0.24833333333333332,
      "grad_norm": 0.9091150164604187,
      "learning_rate": 0.0007516666666666668,
      "loss": 0.9267,
      "step": 74500
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.9729794859886169,
      "learning_rate": 0.00075,
      "loss": 0.9183,
      "step": 75000
    },
    {
      "epoch": 0.25166666666666665,
      "grad_norm": 0.9511370062828064,
      "learning_rate": 0.0007483333333333333,
      "loss": 0.9236,
      "step": 75500
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 1.1392587423324585,
      "learning_rate": 0.0007466666666666667,
      "loss": 0.9225,
      "step": 76000
    },
    {
      "epoch": 0.255,
      "grad_norm": 0.9824836850166321,
      "learning_rate": 0.000745,
      "loss": 0.9201,
      "step": 76500
    },
    {
      "epoch": 0.25666666666666665,
      "grad_norm": 1.2202540636062622,
      "learning_rate": 0.0007433333333333333,
      "loss": 0.9125,
      "step": 77000
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.1829043626785278,
      "learning_rate": 0.0007416666666666667,
      "loss": 0.9038,
      "step": 77500
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.034352421760559,
      "learning_rate": 0.00074,
      "loss": 0.905,
      "step": 78000
    },
    {
      "epoch": 0.26166666666666666,
      "grad_norm": 0.8784035444259644,
      "learning_rate": 0.0007383333333333334,
      "loss": 0.9111,
      "step": 78500
    },
    {
      "epoch": 0.2633333333333333,
      "grad_norm": 0.987276017665863,
      "learning_rate": 0.0007366666666666667,
      "loss": 0.9025,
      "step": 79000
    },
    {
      "epoch": 0.265,
      "grad_norm": 1.010091781616211,
      "learning_rate": 0.000735,
      "loss": 0.9016,
      "step": 79500
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.9948071241378784,
      "learning_rate": 0.0007333333333333333,
      "loss": 0.8965,
      "step": 80000
    },
    {
      "epoch": 0.2683333333333333,
      "grad_norm": 1.1414493322372437,
      "learning_rate": 0.0007316666666666667,
      "loss": 0.8953,
      "step": 80500
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.8644397854804993,
      "learning_rate": 0.00073,
      "loss": 0.8893,
      "step": 81000
    },
    {
      "epoch": 0.27166666666666667,
      "grad_norm": 0.8651911020278931,
      "learning_rate": 0.0007283333333333334,
      "loss": 0.8925,
      "step": 81500
    },
    {
      "epoch": 0.2733333333333333,
      "grad_norm": 1.078849196434021,
      "learning_rate": 0.0007266666666666667,
      "loss": 0.8735,
      "step": 82000
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.0031846761703491,
      "learning_rate": 0.000725,
      "loss": 0.8796,
      "step": 82500
    },
    {
      "epoch": 0.27666666666666667,
      "grad_norm": 0.9444171190261841,
      "learning_rate": 0.0007233333333333334,
      "loss": 0.8709,
      "step": 83000
    },
    {
      "epoch": 0.2783333333333333,
      "grad_norm": 1.073241949081421,
      "learning_rate": 0.0007216666666666667,
      "loss": 0.8695,
      "step": 83500
    },
    {
      "epoch": 0.28,
      "grad_norm": 1.0047560930252075,
      "learning_rate": 0.0007199999999999999,
      "loss": 0.8654,
      "step": 84000
    },
    {
      "epoch": 0.2816666666666667,
      "grad_norm": 0.8852556943893433,
      "learning_rate": 0.0007183333333333334,
      "loss": 0.8626,
      "step": 84500
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 1.0103641748428345,
      "learning_rate": 0.0007166666666666667,
      "loss": 0.862,
      "step": 85000
    },
    {
      "epoch": 0.285,
      "grad_norm": 1.0340993404388428,
      "learning_rate": 0.000715,
      "loss": 0.8614,
      "step": 85500
    },
    {
      "epoch": 0.2866666666666667,
      "grad_norm": 1.0035418272018433,
      "learning_rate": 0.0007133333333333334,
      "loss": 0.8742,
      "step": 86000
    },
    {
      "epoch": 0.28833333333333333,
      "grad_norm": 0.9876898527145386,
      "learning_rate": 0.0007116666666666667,
      "loss": 0.8505,
      "step": 86500
    },
    {
      "epoch": 0.29,
      "grad_norm": 1.068000316619873,
      "learning_rate": 0.00071,
      "loss": 0.8581,
      "step": 87000
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.1775633096694946,
      "learning_rate": 0.0007083333333333334,
      "loss": 0.8562,
      "step": 87500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.9304940104484558,
      "learning_rate": 0.0007066666666666666,
      "loss": 0.8412,
      "step": 88000
    },
    {
      "epoch": 0.295,
      "grad_norm": 0.9869401454925537,
      "learning_rate": 0.000705,
      "loss": 0.8426,
      "step": 88500
    },
    {
      "epoch": 0.2966666666666667,
      "grad_norm": 0.9922100305557251,
      "learning_rate": 0.0007033333333333334,
      "loss": 0.8466,
      "step": 89000
    },
    {
      "epoch": 0.29833333333333334,
      "grad_norm": 0.9651406407356262,
      "learning_rate": 0.0007016666666666666,
      "loss": 0.8352,
      "step": 89500
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.845300018787384,
      "learning_rate": 0.0007,
      "loss": 0.8369,
      "step": 90000
    },
    {
      "epoch": 0.3016666666666667,
      "grad_norm": 0.9973007440567017,
      "learning_rate": 0.0006983333333333334,
      "loss": 0.8415,
      "step": 90500
    },
    {
      "epoch": 0.30333333333333334,
      "grad_norm": 1.0325700044631958,
      "learning_rate": 0.0006966666666666667,
      "loss": 0.8388,
      "step": 91000
    },
    {
      "epoch": 0.305,
      "grad_norm": 0.9977179765701294,
      "learning_rate": 0.000695,
      "loss": 0.8332,
      "step": 91500
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 1.0010859966278076,
      "learning_rate": 0.0006933333333333333,
      "loss": 0.8249,
      "step": 92000
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.9110919833183289,
      "learning_rate": 0.0006916666666666667,
      "loss": 0.8216,
      "step": 92500
    },
    {
      "epoch": 0.31,
      "grad_norm": 1.111680030822754,
      "learning_rate": 0.00069,
      "loss": 0.8333,
      "step": 93000
    },
    {
      "epoch": 0.31166666666666665,
      "grad_norm": 1.0470792055130005,
      "learning_rate": 0.0006883333333333333,
      "loss": 0.8245,
      "step": 93500
    },
    {
      "epoch": 0.31333333333333335,
      "grad_norm": 0.9915028810501099,
      "learning_rate": 0.0006866666666666667,
      "loss": 0.8172,
      "step": 94000
    },
    {
      "epoch": 0.315,
      "grad_norm": 0.9981078505516052,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.827,
      "step": 94500
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 1.3329799175262451,
      "learning_rate": 0.0006833333333333333,
      "loss": 0.8127,
      "step": 95000
    },
    {
      "epoch": 0.31833333333333336,
      "grad_norm": 1.0634467601776123,
      "learning_rate": 0.0006816666666666667,
      "loss": 0.8142,
      "step": 95500
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.110150933265686,
      "learning_rate": 0.00068,
      "loss": 0.8,
      "step": 96000
    },
    {
      "epoch": 0.32166666666666666,
      "grad_norm": 1.0115621089935303,
      "learning_rate": 0.0006783333333333333,
      "loss": 0.7978,
      "step": 96500
    },
    {
      "epoch": 0.3233333333333333,
      "grad_norm": 0.9721469283103943,
      "learning_rate": 0.0006766666666666667,
      "loss": 0.7994,
      "step": 97000
    },
    {
      "epoch": 0.325,
      "grad_norm": 1.0471912622451782,
      "learning_rate": 0.000675,
      "loss": 0.802,
      "step": 97500
    },
    {
      "epoch": 0.32666666666666666,
      "grad_norm": 1.0189098119735718,
      "learning_rate": 0.0006733333333333334,
      "loss": 0.8026,
      "step": 98000
    },
    {
      "epoch": 0.3283333333333333,
      "grad_norm": 1.0161978006362915,
      "learning_rate": 0.0006716666666666667,
      "loss": 0.7975,
      "step": 98500
    },
    {
      "epoch": 0.33,
      "grad_norm": 1.1255755424499512,
      "learning_rate": 0.00067,
      "loss": 0.797,
      "step": 99000
    },
    {
      "epoch": 0.33166666666666667,
      "grad_norm": 0.9729913473129272,
      "learning_rate": 0.0006683333333333334,
      "loss": 0.7896,
      "step": 99500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.9640668630599976,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.7904,
      "step": 100000
    },
    {
      "epoch": 0.335,
      "grad_norm": 0.9981409907341003,
      "learning_rate": 0.000665,
      "loss": 0.782,
      "step": 100500
    },
    {
      "epoch": 0.33666666666666667,
      "grad_norm": 0.9138562083244324,
      "learning_rate": 0.0006633333333333334,
      "loss": 0.7902,
      "step": 101000
    },
    {
      "epoch": 0.3383333333333333,
      "grad_norm": 0.9987477660179138,
      "learning_rate": 0.0006616666666666666,
      "loss": 0.7811,
      "step": 101500
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.0150388479232788,
      "learning_rate": 0.00066,
      "loss": 0.7737,
      "step": 102000
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 1.8367782831192017,
      "learning_rate": 0.0006583333333333334,
      "loss": 0.7805,
      "step": 102500
    },
    {
      "epoch": 0.3433333333333333,
      "grad_norm": 0.9919508099555969,
      "learning_rate": 0.0006566666666666666,
      "loss": 0.7724,
      "step": 103000
    },
    {
      "epoch": 0.345,
      "grad_norm": 0.953040599822998,
      "learning_rate": 0.0006550000000000001,
      "loss": 0.7673,
      "step": 103500
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 1.0806629657745361,
      "learning_rate": 0.0006533333333333333,
      "loss": 0.7617,
      "step": 104000
    },
    {
      "epoch": 0.34833333333333333,
      "grad_norm": 1.0416687726974487,
      "learning_rate": 0.0006516666666666666,
      "loss": 0.7636,
      "step": 104500
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.0378527641296387,
      "learning_rate": 0.0006500000000000001,
      "loss": 0.7745,
      "step": 105000
    },
    {
      "epoch": 0.3516666666666667,
      "grad_norm": 0.996288001537323,
      "learning_rate": 0.0006483333333333333,
      "loss": 0.758,
      "step": 105500
    },
    {
      "epoch": 0.35333333333333333,
      "grad_norm": 0.9542946815490723,
      "learning_rate": 0.0006466666666666666,
      "loss": 0.7627,
      "step": 106000
    },
    {
      "epoch": 0.355,
      "grad_norm": 0.9927533864974976,
      "learning_rate": 0.0006450000000000001,
      "loss": 0.7488,
      "step": 106500
    },
    {
      "epoch": 0.3566666666666667,
      "grad_norm": 0.9013589024543762,
      "learning_rate": 0.0006433333333333333,
      "loss": 0.7466,
      "step": 107000
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 1.016858458518982,
      "learning_rate": 0.0006416666666666667,
      "loss": 0.7509,
      "step": 107500
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.0463371276855469,
      "learning_rate": 0.00064,
      "loss": 0.7554,
      "step": 108000
    },
    {
      "epoch": 0.3616666666666667,
      "grad_norm": 1.0466737747192383,
      "learning_rate": 0.0006383333333333333,
      "loss": 0.7443,
      "step": 108500
    },
    {
      "epoch": 0.36333333333333334,
      "grad_norm": 0.8679410815238953,
      "learning_rate": 0.0006366666666666667,
      "loss": 0.7463,
      "step": 109000
    },
    {
      "epoch": 0.365,
      "grad_norm": 1.1220570802688599,
      "learning_rate": 0.000635,
      "loss": 0.7427,
      "step": 109500
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.9355452060699463,
      "learning_rate": 0.0006333333333333333,
      "loss": 0.7436,
      "step": 110000
    },
    {
      "epoch": 0.36833333333333335,
      "grad_norm": 0.9731543064117432,
      "learning_rate": 0.0006316666666666668,
      "loss": 0.7421,
      "step": 110500
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.9601234793663025,
      "learning_rate": 0.00063,
      "loss": 0.7408,
      "step": 111000
    },
    {
      "epoch": 0.37166666666666665,
      "grad_norm": 0.8958061933517456,
      "learning_rate": 0.0006283333333333333,
      "loss": 0.7382,
      "step": 111500
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 1.1227399110794067,
      "learning_rate": 0.0006266666666666668,
      "loss": 0.7376,
      "step": 112000
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.060674786567688,
      "learning_rate": 0.000625,
      "loss": 0.7309,
      "step": 112500
    },
    {
      "epoch": 0.37666666666666665,
      "grad_norm": 1.146462082862854,
      "learning_rate": 0.0006233333333333333,
      "loss": 0.7235,
      "step": 113000
    },
    {
      "epoch": 0.37833333333333335,
      "grad_norm": 1.3657130002975464,
      "learning_rate": 0.0006216666666666667,
      "loss": 0.7306,
      "step": 113500
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.1109049320220947,
      "learning_rate": 0.00062,
      "loss": 0.7274,
      "step": 114000
    },
    {
      "epoch": 0.38166666666666665,
      "grad_norm": 0.9118052124977112,
      "learning_rate": 0.0006183333333333333,
      "loss": 0.7298,
      "step": 114500
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.1131932735443115,
      "learning_rate": 0.0006166666666666667,
      "loss": 0.7286,
      "step": 115000
    },
    {
      "epoch": 0.385,
      "grad_norm": 0.9930132627487183,
      "learning_rate": 0.000615,
      "loss": 0.7126,
      "step": 115500
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.9967590570449829,
      "learning_rate": 0.0006133333333333334,
      "loss": 0.7216,
      "step": 116000
    },
    {
      "epoch": 0.3883333333333333,
      "grad_norm": 1.0086743831634521,
      "learning_rate": 0.0006116666666666667,
      "loss": 0.7118,
      "step": 116500
    },
    {
      "epoch": 0.39,
      "grad_norm": 1.0429542064666748,
      "learning_rate": 0.00061,
      "loss": 0.7133,
      "step": 117000
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.9792120456695557,
      "learning_rate": 0.0006083333333333333,
      "loss": 0.7164,
      "step": 117500
    },
    {
      "epoch": 0.3933333333333333,
      "grad_norm": 1.0084900856018066,
      "learning_rate": 0.0006066666666666667,
      "loss": 0.7112,
      "step": 118000
    },
    {
      "epoch": 0.395,
      "grad_norm": 0.9981223344802856,
      "learning_rate": 0.000605,
      "loss": 0.7046,
      "step": 118500
    },
    {
      "epoch": 0.39666666666666667,
      "grad_norm": 1.020755648612976,
      "learning_rate": 0.0006033333333333334,
      "loss": 0.7062,
      "step": 119000
    },
    {
      "epoch": 0.3983333333333333,
      "grad_norm": 1.0067640542984009,
      "learning_rate": 0.0006016666666666667,
      "loss": 0.7063,
      "step": 119500
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.0172033309936523,
      "learning_rate": 0.0006,
      "loss": 0.6961,
      "step": 120000
    },
    {
      "epoch": 0.40166666666666667,
      "grad_norm": 1.0368870496749878,
      "learning_rate": 0.0005983333333333334,
      "loss": 0.703,
      "step": 120500
    },
    {
      "epoch": 0.4033333333333333,
      "grad_norm": 1.1987180709838867,
      "learning_rate": 0.0005966666666666667,
      "loss": 0.7046,
      "step": 121000
    },
    {
      "epoch": 0.405,
      "grad_norm": 0.9223681092262268,
      "learning_rate": 0.0005949999999999999,
      "loss": 0.6922,
      "step": 121500
    },
    {
      "epoch": 0.4066666666666667,
      "grad_norm": 1.0479843616485596,
      "learning_rate": 0.0005933333333333334,
      "loss": 0.6945,
      "step": 122000
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.8805484175682068,
      "learning_rate": 0.0005916666666666667,
      "loss": 0.687,
      "step": 122500
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.9855422377586365,
      "learning_rate": 0.00059,
      "loss": 0.6903,
      "step": 123000
    },
    {
      "epoch": 0.4116666666666667,
      "grad_norm": 1.109349250793457,
      "learning_rate": 0.0005883333333333334,
      "loss": 0.6841,
      "step": 123500
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 1.3078685998916626,
      "learning_rate": 0.0005866666666666667,
      "loss": 0.6902,
      "step": 124000
    },
    {
      "epoch": 0.415,
      "grad_norm": 1.1277384757995605,
      "learning_rate": 0.000585,
      "loss": 0.6947,
      "step": 124500
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.0482032299041748,
      "learning_rate": 0.0005833333333333334,
      "loss": 0.681,
      "step": 125000
    },
    {
      "epoch": 0.41833333333333333,
      "grad_norm": 0.9936891794204712,
      "learning_rate": 0.0005816666666666666,
      "loss": 0.6899,
      "step": 125500
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.0735328197479248,
      "learning_rate": 0.00058,
      "loss": 0.6777,
      "step": 126000
    },
    {
      "epoch": 0.4216666666666667,
      "grad_norm": 1.0850545167922974,
      "learning_rate": 0.0005783333333333334,
      "loss": 0.6788,
      "step": 126500
    },
    {
      "epoch": 0.42333333333333334,
      "grad_norm": 0.960783064365387,
      "learning_rate": 0.0005766666666666666,
      "loss": 0.6794,
      "step": 127000
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.140621304512024,
      "learning_rate": 0.000575,
      "loss": 0.6738,
      "step": 127500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 1.044935703277588,
      "learning_rate": 0.0005733333333333334,
      "loss": 0.6688,
      "step": 128000
    },
    {
      "epoch": 0.42833333333333334,
      "grad_norm": 0.966465413570404,
      "learning_rate": 0.0005716666666666667,
      "loss": 0.6735,
      "step": 128500
    },
    {
      "epoch": 0.43,
      "grad_norm": 1.0576527118682861,
      "learning_rate": 0.00057,
      "loss": 0.6688,
      "step": 129000
    },
    {
      "epoch": 0.43166666666666664,
      "grad_norm": 1.0938142538070679,
      "learning_rate": 0.0005683333333333333,
      "loss": 0.6747,
      "step": 129500
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.00864839553833,
      "learning_rate": 0.0005666666666666667,
      "loss": 0.6654,
      "step": 130000
    },
    {
      "epoch": 0.435,
      "grad_norm": 0.8497617840766907,
      "learning_rate": 0.000565,
      "loss": 0.665,
      "step": 130500
    },
    {
      "epoch": 0.43666666666666665,
      "grad_norm": 1.021868348121643,
      "learning_rate": 0.0005633333333333333,
      "loss": 0.6644,
      "step": 131000
    },
    {
      "epoch": 0.43833333333333335,
      "grad_norm": 0.92791748046875,
      "learning_rate": 0.0005616666666666667,
      "loss": 0.6689,
      "step": 131500
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.9683578014373779,
      "learning_rate": 0.0005600000000000001,
      "loss": 0.6585,
      "step": 132000
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.1765509843826294,
      "learning_rate": 0.0005583333333333333,
      "loss": 0.6576,
      "step": 132500
    },
    {
      "epoch": 0.44333333333333336,
      "grad_norm": 1.0107463598251343,
      "learning_rate": 0.0005566666666666667,
      "loss": 0.6613,
      "step": 133000
    },
    {
      "epoch": 0.445,
      "grad_norm": 0.8962523341178894,
      "learning_rate": 0.000555,
      "loss": 0.6581,
      "step": 133500
    },
    {
      "epoch": 0.44666666666666666,
      "grad_norm": 1.06494140625,
      "learning_rate": 0.0005533333333333333,
      "loss": 0.6487,
      "step": 134000
    },
    {
      "epoch": 0.4483333333333333,
      "grad_norm": 1.014523983001709,
      "learning_rate": 0.0005516666666666667,
      "loss": 0.6515,
      "step": 134500
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.985160231590271,
      "learning_rate": 0.00055,
      "loss": 0.6439,
      "step": 135000
    },
    {
      "epoch": 0.45166666666666666,
      "grad_norm": 0.9380677938461304,
      "learning_rate": 0.0005483333333333334,
      "loss": 0.6428,
      "step": 135500
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.9270495176315308,
      "learning_rate": 0.0005466666666666667,
      "loss": 0.645,
      "step": 136000
    },
    {
      "epoch": 0.455,
      "grad_norm": 0.8794169425964355,
      "learning_rate": 0.000545,
      "loss": 0.6414,
      "step": 136500
    },
    {
      "epoch": 0.45666666666666667,
      "grad_norm": 1.0392996072769165,
      "learning_rate": 0.0005433333333333334,
      "loss": 0.6473,
      "step": 137000
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.9516978859901428,
      "learning_rate": 0.0005416666666666666,
      "loss": 0.638,
      "step": 137500
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.8978258371353149,
      "learning_rate": 0.00054,
      "loss": 0.6345,
      "step": 138000
    },
    {
      "epoch": 0.46166666666666667,
      "grad_norm": 0.994134247303009,
      "learning_rate": 0.0005383333333333334,
      "loss": 0.641,
      "step": 138500
    },
    {
      "epoch": 0.4633333333333333,
      "grad_norm": 1.0528019666671753,
      "learning_rate": 0.0005366666666666666,
      "loss": 0.639,
      "step": 139000
    },
    {
      "epoch": 0.465,
      "grad_norm": 1.14760422706604,
      "learning_rate": 0.000535,
      "loss": 0.6358,
      "step": 139500
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 1.0270466804504395,
      "learning_rate": 0.0005333333333333334,
      "loss": 0.6388,
      "step": 140000
    },
    {
      "epoch": 0.4683333333333333,
      "grad_norm": 0.9670595526695251,
      "learning_rate": 0.0005316666666666666,
      "loss": 0.6315,
      "step": 140500
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.9451064467430115,
      "learning_rate": 0.0005300000000000001,
      "loss": 0.6336,
      "step": 141000
    },
    {
      "epoch": 0.4716666666666667,
      "grad_norm": 0.9980092644691467,
      "learning_rate": 0.0005283333333333333,
      "loss": 0.6311,
      "step": 141500
    },
    {
      "epoch": 0.47333333333333333,
      "grad_norm": 1.1028369665145874,
      "learning_rate": 0.0005266666666666666,
      "loss": 0.631,
      "step": 142000
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.9588748812675476,
      "learning_rate": 0.0005250000000000001,
      "loss": 0.6218,
      "step": 142500
    },
    {
      "epoch": 0.4766666666666667,
      "grad_norm": 1.1443842649459839,
      "learning_rate": 0.0005233333333333333,
      "loss": 0.6203,
      "step": 143000
    },
    {
      "epoch": 0.47833333333333333,
      "grad_norm": 1.0116826295852661,
      "learning_rate": 0.0005216666666666666,
      "loss": 0.6187,
      "step": 143500
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.0086276531219482,
      "learning_rate": 0.0005200000000000001,
      "loss": 0.6226,
      "step": 144000
    },
    {
      "epoch": 0.4816666666666667,
      "grad_norm": 1.251560926437378,
      "learning_rate": 0.0005183333333333333,
      "loss": 0.6135,
      "step": 144500
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 1.0875002145767212,
      "learning_rate": 0.0005166666666666667,
      "loss": 0.6207,
      "step": 145000
    },
    {
      "epoch": 0.485,
      "grad_norm": 0.966788649559021,
      "learning_rate": 0.000515,
      "loss": 0.6091,
      "step": 145500
    },
    {
      "epoch": 0.4866666666666667,
      "grad_norm": 1.0312577486038208,
      "learning_rate": 0.0005133333333333333,
      "loss": 0.6094,
      "step": 146000
    },
    {
      "epoch": 0.48833333333333334,
      "grad_norm": 0.9296697974205017,
      "learning_rate": 0.0005116666666666667,
      "loss": 0.6137,
      "step": 146500
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9779425859451294,
      "learning_rate": 0.00051,
      "loss": 0.6127,
      "step": 147000
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.9624021053314209,
      "learning_rate": 0.0005083333333333333,
      "loss": 0.6085,
      "step": 147500
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.9897448420524597,
      "learning_rate": 0.0005066666666666668,
      "loss": 0.6084,
      "step": 148000
    },
    {
      "epoch": 0.495,
      "grad_norm": 1.043439507484436,
      "learning_rate": 0.000505,
      "loss": 0.6102,
      "step": 148500
    },
    {
      "epoch": 0.49666666666666665,
      "grad_norm": 1.0113071203231812,
      "learning_rate": 0.0005033333333333333,
      "loss": 0.6042,
      "step": 149000
    },
    {
      "epoch": 0.49833333333333335,
      "grad_norm": 1.0195058584213257,
      "learning_rate": 0.0005016666666666668,
      "loss": 0.607,
      "step": 149500
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9510048627853394,
      "learning_rate": 0.0005,
      "loss": 0.6003,
      "step": 150000
    },
    {
      "epoch": 0.5016666666666667,
      "grad_norm": 1.0273196697235107,
      "learning_rate": 0.0004983333333333334,
      "loss": 0.6014,
      "step": 150500
    },
    {
      "epoch": 0.5033333333333333,
      "grad_norm": 1.1042592525482178,
      "learning_rate": 0.0004966666666666666,
      "loss": 0.5969,
      "step": 151000
    },
    {
      "epoch": 0.505,
      "grad_norm": 1.5565612316131592,
      "learning_rate": 0.000495,
      "loss": 0.6045,
      "step": 151500
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.9304568767547607,
      "learning_rate": 0.0004933333333333334,
      "loss": 0.5935,
      "step": 152000
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.0781795978546143,
      "learning_rate": 0.0004916666666666666,
      "loss": 0.595,
      "step": 152500
    },
    {
      "epoch": 0.51,
      "grad_norm": 1.1962711811065674,
      "learning_rate": 0.00049,
      "loss": 0.6009,
      "step": 153000
    },
    {
      "epoch": 0.5116666666666667,
      "grad_norm": 1.0571341514587402,
      "learning_rate": 0.0004883333333333333,
      "loss": 0.5975,
      "step": 153500
    },
    {
      "epoch": 0.5133333333333333,
      "grad_norm": 0.8929877281188965,
      "learning_rate": 0.0004866666666666667,
      "loss": 0.5915,
      "step": 154000
    },
    {
      "epoch": 0.515,
      "grad_norm": 0.9895422458648682,
      "learning_rate": 0.00048499999999999997,
      "loss": 0.5957,
      "step": 154500
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.9151380658149719,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.595,
      "step": 155000
    },
    {
      "epoch": 0.5183333333333333,
      "grad_norm": 1.1361452341079712,
      "learning_rate": 0.0004816666666666667,
      "loss": 0.5936,
      "step": 155500
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0153956413269043,
      "learning_rate": 0.00048,
      "loss": 0.583,
      "step": 156000
    },
    {
      "epoch": 0.5216666666666666,
      "grad_norm": 0.9055264592170715,
      "learning_rate": 0.0004783333333333333,
      "loss": 0.5846,
      "step": 156500
    },
    {
      "epoch": 0.5233333333333333,
      "grad_norm": 1.0076733827590942,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.5805,
      "step": 157000
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.8530507683753967,
      "learning_rate": 0.000475,
      "loss": 0.5894,
      "step": 157500
    },
    {
      "epoch": 0.5266666666666666,
      "grad_norm": 0.9769659042358398,
      "learning_rate": 0.00047333333333333336,
      "loss": 0.5848,
      "step": 158000
    },
    {
      "epoch": 0.5283333333333333,
      "grad_norm": 1.0507062673568726,
      "learning_rate": 0.0004716666666666667,
      "loss": 0.5887,
      "step": 158500
    },
    {
      "epoch": 0.53,
      "grad_norm": 1.125154733657837,
      "learning_rate": 0.00047,
      "loss": 0.5824,
      "step": 159000
    },
    {
      "epoch": 0.5316666666666666,
      "grad_norm": 1.090774655342102,
      "learning_rate": 0.00046833333333333335,
      "loss": 0.5846,
      "step": 159500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.9035694003105164,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.5778,
      "step": 160000
    },
    {
      "epoch": 0.535,
      "grad_norm": 0.9501013159751892,
      "learning_rate": 0.000465,
      "loss": 0.5654,
      "step": 160500
    },
    {
      "epoch": 0.5366666666666666,
      "grad_norm": 0.9517428278923035,
      "learning_rate": 0.00046333333333333334,
      "loss": 0.5808,
      "step": 161000
    },
    {
      "epoch": 0.5383333333333333,
      "grad_norm": 1.1450235843658447,
      "learning_rate": 0.0004616666666666667,
      "loss": 0.5715,
      "step": 161500
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.9900879859924316,
      "learning_rate": 0.00046,
      "loss": 0.5678,
      "step": 162000
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 1.0173894166946411,
      "learning_rate": 0.0004583333333333333,
      "loss": 0.5718,
      "step": 162500
    },
    {
      "epoch": 0.5433333333333333,
      "grad_norm": 1.0658587217330933,
      "learning_rate": 0.0004566666666666667,
      "loss": 0.5788,
      "step": 163000
    },
    {
      "epoch": 0.545,
      "grad_norm": 1.1038382053375244,
      "learning_rate": 0.000455,
      "loss": 0.57,
      "step": 163500
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 1.2014002799987793,
      "learning_rate": 0.0004533333333333333,
      "loss": 0.5689,
      "step": 164000
    },
    {
      "epoch": 0.5483333333333333,
      "grad_norm": 0.9431982040405273,
      "learning_rate": 0.0004516666666666667,
      "loss": 0.568,
      "step": 164500
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.9062801003456116,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.5646,
      "step": 165000
    },
    {
      "epoch": 0.5516666666666666,
      "grad_norm": 1.0570930242538452,
      "learning_rate": 0.0004483333333333333,
      "loss": 0.57,
      "step": 165500
    },
    {
      "epoch": 0.5533333333333333,
      "grad_norm": 0.9973031878471375,
      "learning_rate": 0.00044666666666666666,
      "loss": 0.5641,
      "step": 166000
    },
    {
      "epoch": 0.555,
      "grad_norm": 1.0126806497573853,
      "learning_rate": 0.00044500000000000003,
      "loss": 0.5606,
      "step": 166500
    },
    {
      "epoch": 0.5566666666666666,
      "grad_norm": 0.8665674328804016,
      "learning_rate": 0.00044333333333333334,
      "loss": 0.5567,
      "step": 167000
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.9957442283630371,
      "learning_rate": 0.00044166666666666665,
      "loss": 0.5616,
      "step": 167500
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.9579408764839172,
      "learning_rate": 0.00044,
      "loss": 0.559,
      "step": 168000
    },
    {
      "epoch": 0.5616666666666666,
      "grad_norm": 1.005600094795227,
      "learning_rate": 0.0004383333333333334,
      "loss": 0.5621,
      "step": 168500
    },
    {
      "epoch": 0.5633333333333334,
      "grad_norm": 0.8040379285812378,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.5515,
      "step": 169000
    },
    {
      "epoch": 0.565,
      "grad_norm": 0.8502169251441956,
      "learning_rate": 0.000435,
      "loss": 0.5538,
      "step": 169500
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.9474074244499207,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.5518,
      "step": 170000
    },
    {
      "epoch": 0.5683333333333334,
      "grad_norm": 0.8585811853408813,
      "learning_rate": 0.0004316666666666667,
      "loss": 0.5453,
      "step": 170500
    },
    {
      "epoch": 0.57,
      "grad_norm": 1.0213141441345215,
      "learning_rate": 0.00043,
      "loss": 0.5567,
      "step": 171000
    },
    {
      "epoch": 0.5716666666666667,
      "grad_norm": 1.0072314739227295,
      "learning_rate": 0.00042833333333333335,
      "loss": 0.5473,
      "step": 171500
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.9865872263908386,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.5448,
      "step": 172000
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.9009928107261658,
      "learning_rate": 0.000425,
      "loss": 0.5479,
      "step": 172500
    },
    {
      "epoch": 0.5766666666666667,
      "grad_norm": 1.0511373281478882,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.5506,
      "step": 173000
    },
    {
      "epoch": 0.5783333333333334,
      "grad_norm": 1.0439672470092773,
      "learning_rate": 0.0004216666666666667,
      "loss": 0.5403,
      "step": 173500
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.039286494255066,
      "learning_rate": 0.00042,
      "loss": 0.5435,
      "step": 174000
    },
    {
      "epoch": 0.5816666666666667,
      "grad_norm": 1.1310127973556519,
      "learning_rate": 0.00041833333333333333,
      "loss": 0.5423,
      "step": 174500
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.9597508311271667,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.5436,
      "step": 175000
    },
    {
      "epoch": 0.585,
      "grad_norm": 1.0287166833877563,
      "learning_rate": 0.000415,
      "loss": 0.5416,
      "step": 175500
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 1.1202902793884277,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.5389,
      "step": 176000
    },
    {
      "epoch": 0.5883333333333334,
      "grad_norm": 1.0138999223709106,
      "learning_rate": 0.0004116666666666667,
      "loss": 0.5428,
      "step": 176500
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.9712451696395874,
      "learning_rate": 0.00041,
      "loss": 0.5301,
      "step": 177000
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.9137016534805298,
      "learning_rate": 0.00040833333333333336,
      "loss": 0.533,
      "step": 177500
    },
    {
      "epoch": 0.5933333333333334,
      "grad_norm": 0.9301744699478149,
      "learning_rate": 0.00040666666666666667,
      "loss": 0.5338,
      "step": 178000
    },
    {
      "epoch": 0.595,
      "grad_norm": 0.9125961065292358,
      "learning_rate": 0.00040500000000000003,
      "loss": 0.5341,
      "step": 178500
    },
    {
      "epoch": 0.5966666666666667,
      "grad_norm": 0.8809047937393188,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.5332,
      "step": 179000
    },
    {
      "epoch": 0.5983333333333334,
      "grad_norm": 1.1081032752990723,
      "learning_rate": 0.00040166666666666665,
      "loss": 0.5292,
      "step": 179500
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0383501052856445,
      "learning_rate": 0.0004,
      "loss": 0.5326,
      "step": 180000
    },
    {
      "epoch": 0.6016666666666667,
      "grad_norm": 0.848702609539032,
      "learning_rate": 0.00039833333333333333,
      "loss": 0.5347,
      "step": 180500
    },
    {
      "epoch": 0.6033333333333334,
      "grad_norm": 0.9390897154808044,
      "learning_rate": 0.0003966666666666667,
      "loss": 0.5195,
      "step": 181000
    },
    {
      "epoch": 0.605,
      "grad_norm": 1.0108509063720703,
      "learning_rate": 0.000395,
      "loss": 0.5247,
      "step": 181500
    },
    {
      "epoch": 0.6066666666666667,
      "grad_norm": 0.8881744146347046,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.5243,
      "step": 182000
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.9500270485877991,
      "learning_rate": 0.0003916666666666667,
      "loss": 0.524,
      "step": 182500
    },
    {
      "epoch": 0.61,
      "grad_norm": 1.0167280435562134,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.5305,
      "step": 183000
    },
    {
      "epoch": 0.6116666666666667,
      "grad_norm": 0.9893699288368225,
      "learning_rate": 0.0003883333333333333,
      "loss": 0.5227,
      "step": 183500
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 1.132095456123352,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.5198,
      "step": 184000
    },
    {
      "epoch": 0.615,
      "grad_norm": 1.1504327058792114,
      "learning_rate": 0.00038500000000000003,
      "loss": 0.5169,
      "step": 184500
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.7923588156700134,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.5244,
      "step": 185000
    },
    {
      "epoch": 0.6183333333333333,
      "grad_norm": 1.2050124406814575,
      "learning_rate": 0.00038166666666666666,
      "loss": 0.5193,
      "step": 185500
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.148461937904358,
      "learning_rate": 0.00038,
      "loss": 0.5172,
      "step": 186000
    },
    {
      "epoch": 0.6216666666666667,
      "grad_norm": 0.9871645569801331,
      "learning_rate": 0.0003783333333333334,
      "loss": 0.5139,
      "step": 186500
    },
    {
      "epoch": 0.6233333333333333,
      "grad_norm": 1.0779856443405151,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.5183,
      "step": 187000
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.9146093726158142,
      "learning_rate": 0.000375,
      "loss": 0.5168,
      "step": 187500
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 1.1089320182800293,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.5154,
      "step": 188000
    },
    {
      "epoch": 0.6283333333333333,
      "grad_norm": 0.9747845530509949,
      "learning_rate": 0.00037166666666666663,
      "loss": 0.5149,
      "step": 188500
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.9779942631721497,
      "learning_rate": 0.00037,
      "loss": 0.5178,
      "step": 189000
    },
    {
      "epoch": 0.6316666666666667,
      "grad_norm": 1.0319007635116577,
      "learning_rate": 0.00036833333333333336,
      "loss": 0.5125,
      "step": 189500
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 1.0183097124099731,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.5137,
      "step": 190000
    },
    {
      "epoch": 0.635,
      "grad_norm": 0.9737736582756042,
      "learning_rate": 0.000365,
      "loss": 0.5067,
      "step": 190500
    },
    {
      "epoch": 0.6366666666666667,
      "grad_norm": 1.0220075845718384,
      "learning_rate": 0.00036333333333333335,
      "loss": 0.5067,
      "step": 191000
    },
    {
      "epoch": 0.6383333333333333,
      "grad_norm": 0.8687080144882202,
      "learning_rate": 0.0003616666666666667,
      "loss": 0.5062,
      "step": 191500
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.1843695640563965,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.5057,
      "step": 192000
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.9002715945243835,
      "learning_rate": 0.00035833333333333333,
      "loss": 0.5029,
      "step": 192500
    },
    {
      "epoch": 0.6433333333333333,
      "grad_norm": 1.0098624229431152,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.509,
      "step": 193000
    },
    {
      "epoch": 0.645,
      "grad_norm": 1.051666498184204,
      "learning_rate": 0.000355,
      "loss": 0.5001,
      "step": 193500
    },
    {
      "epoch": 0.6466666666666666,
      "grad_norm": 0.845779538154602,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.5104,
      "step": 194000
    },
    {
      "epoch": 0.6483333333333333,
      "grad_norm": 0.9609007239341736,
      "learning_rate": 0.0003516666666666667,
      "loss": 0.5035,
      "step": 194500
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.9060437083244324,
      "learning_rate": 0.00035,
      "loss": 0.5021,
      "step": 195000
    },
    {
      "epoch": 0.6516666666666666,
      "grad_norm": 1.0181834697723389,
      "learning_rate": 0.00034833333333333336,
      "loss": 0.5002,
      "step": 195500
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.9722155332565308,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.4982,
      "step": 196000
    },
    {
      "epoch": 0.655,
      "grad_norm": 1.1245949268341064,
      "learning_rate": 0.000345,
      "loss": 0.4977,
      "step": 196500
    },
    {
      "epoch": 0.6566666666666666,
      "grad_norm": 0.9159996509552002,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.4967,
      "step": 197000
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.8533723950386047,
      "learning_rate": 0.00034166666666666666,
      "loss": 0.4962,
      "step": 197500
    },
    {
      "epoch": 0.66,
      "grad_norm": 1.0347739458084106,
      "learning_rate": 0.00034,
      "loss": 0.4936,
      "step": 198000
    },
    {
      "epoch": 0.6616666666666666,
      "grad_norm": 0.9981224536895752,
      "learning_rate": 0.00033833333333333334,
      "loss": 0.4995,
      "step": 198500
    },
    {
      "epoch": 0.6633333333333333,
      "grad_norm": 1.047278642654419,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.4926,
      "step": 199000
    },
    {
      "epoch": 0.665,
      "grad_norm": 0.9887679815292358,
      "learning_rate": 0.000335,
      "loss": 0.4979,
      "step": 199500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.9264817833900452,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.4962,
      "step": 200000
    }
  ],
  "logging_steps": 500,
  "max_steps": 300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 100000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.678793019392e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
