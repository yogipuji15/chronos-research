{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_arrow(csv_path: str):\n",
    "    \"\"\"\n",
    "    Store a given set of series into Arrow format at the specified path.\n",
    "\n",
    "    Input data can be a CSV file with a 'close' column.\n",
    "    \"\"\"\n",
    "    # Baca file CSV\n",
    "    data = pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
    "\n",
    "    # Hitung jumlah total baris\n",
    "    total_rows = len(data)\n",
    "\n",
    "    # Hitung 70% dari total baris\n",
    "    rows_to_take = int(0.7 * total_rows)\n",
    "\n",
    "    # Ambil 70% pertama dari data\n",
    "    data = data.head(rows_to_take)\n",
    "\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    data = data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Ambil array dari kolom 'close'\n",
    "    time_series = data['close'].to_numpy()\n",
    "    \n",
    "    # Set an arbitrary start time\n",
    "    start = np.datetime64(data['timestamp'].iloc[0], \"s\")\n",
    "\n",
    "    # Buat dataset yang terdiri dari satu seri waktu dengan satu start time\n",
    "    dataset = {\"start\": start, \"target\": time_series}\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 42 LQ45\n",
    "dataset_name=[\"ACES\",\"AMRT\",\"ASII\",\"BBRI\",\"BRIS\",\"CPIN\",\"GGRM\",\"ICBP\",\n",
    "              \"INKP\",\"ITMG\",\"MDKA\",\"PGAS\",\"SMGR\",\"TOWR\",\"ADRO\",\"ANTM\",\n",
    "              \"BBCA\",\"BBTN\",\"BRPT\",\"ESSA\",\"GOTO\",\"INCO\",\"INTP\",\"KLBF\",\n",
    "              \"MEDC\",\"PTBA\",\"SRTG\",\"UNTR\",\"AKRA\",\"ARTO\",\"BBNI\",\"BMRI\",\n",
    "              \"BUKA\",\"EXCL\",\"HRUM\",\"INDF\",\"ISAT\",\"MAPI\",\"MTEL\",\"SIDO\",\n",
    "              \"TLKM\",\"UNVR\"]\n",
    "\n",
    "dataset_train_model =[]\n",
    "\n",
    "count=0\n",
    "for ds in dataset_name:\n",
    "  # Sesuaikan path ke lokasi file CSV dan output yang diinginkan\n",
    "\n",
    "\n",
    "  data = pd.read_csv(f\"/content/drive/MyDrive/Dataset Skripsi/daily-lq45/{ds}.csv\", parse_dates=['timestamp'])\n",
    "\n",
    "  # Hitung jumlah total baris\n",
    "  total_rows = len(data)\n",
    "\n",
    "  print(ds,\"= \",total_rows)\n",
    "  count+=1\n",
    "\n",
    "  dataset_train_model.append(convert_to_arrow(f\"/content/drive/MyDrive/Dataset Skripsi/daily-lq45/{ds}.csv\"))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.evaluation import Evaluator, make_evaluation_predictions\n",
    "from gluonts.torch import TemporalFusionTransformerEstimator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "\n",
    "def load_and_process_data(\n",
    "    filename,\n",
    "    date_column_name,\n",
    "    index_timezone=\"America/New_York\",\n",
    "    fillna_method=\"ffill\",\n",
    "):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Ensure the Date column is a datetime object\n",
    "    df[date_column_name] = pd.to_datetime(df[date_column_name], utc=True)\n",
    "\n",
    "    # Convert the timezone of the Date column to the specified timezone\n",
    "    df[date_column_name] = df[date_column_name].dt.tz_convert(index_timezone)\n",
    "\n",
    "    # Set Date column as the index\n",
    "    df.set_index(date_column_name, inplace=True)\n",
    "\n",
    "    # If Adj Close column exists, keep only this column and drop others\n",
    "    if \"Adj Close\" in df.columns:\n",
    "        df = df[[\"Adj Close\"]]\n",
    "        # Rename 'Adj Close' to be more specific based on the filename\n",
    "        new_column_name = filename.split(\"/\")[-1].split(\"_\")[0] + \"_Adj_Close\"\n",
    "        df.rename(columns={\"Adj Close\": new_column_name}, inplace=True)\n",
    "\n",
    "    # If fillna method is specified, fill the missing values\n",
    "    if fillna_method:\n",
    "        df.fillna(method=fillna_method, inplace=True)\n",
    "\n",
    "    # Check if there are still NaN values and backfill if needed\n",
    "    if df.isnull().sum().any():\n",
    "        df.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_and_process_data(\n",
    "    \"/content/drive/MyDrive/Dataset Skripsi/ANTM.csv\", \"timestamp\"\n",
    ")\n",
    "\n",
    "# Create GluonTS datasets and split it\n",
    "def split_data(df, prediction_length, windows=1):\n",
    "    # Ensure the df index is datetime type\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Create GluonTS format dataset\n",
    "    start_timestamp = pd.Period(df.index[0], \"D\")\n",
    "    dataset = [\n",
    "        {\n",
    "            \"start\": start_timestamp,\n",
    "            \"target\": df[\"close\"].values.astype(np.float32),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Split the data (!note the negative symbol in front of prediction length!)\n",
    "    train_data, test_gen = split(dataset, offset=-prediction_length)\n",
    "\n",
    "    # Generate test instances\n",
    "    test_data = test_gen.generate_instances(prediction_length, windows=windows)\n",
    "\n",
    "    return dataset, train_data, test_data\n",
    "\n",
    "\n",
    "prediction_length = 64\n",
    "\n",
    "# Create the estimator\n",
    "\n",
    "estimator = TemporalFusionTransformerEstimator(\n",
    "    freq = \"D\",\n",
    "    context_length=512,\n",
    "    prediction_length = prediction_length,\n",
    "    num_heads= 4,  # Jumlah heads di self-attention layer, 4 adalah titik awal yang baik\n",
    "    hidden_dim= 40,  # Ukuran hidden layer untuk menjaga kompleksitas tetap rendah\n",
    "    lr= 0.001,  # Sama dengan learning rate pada Chronos\n",
    "    patience= 10,  # Patience untuk scheduler; angka yang moderat\n",
    "    trainer_kwargs={\"max_epochs\": 50}\n",
    ")\n",
    "\n",
    "# Train on training dataset\n",
    "dataset, training_data, test_data = split_data(df, prediction_length)\n",
    "\n",
    "# train model dataset\n",
    "train_model_data = ListDataset(\n",
    "    dataset_train_model,\n",
    "    freq = \"D\"\n",
    ")\n",
    "\n",
    "model = estimator.train(train_model_data, num_workers=0)\n",
    "\n",
    "\n",
    "# Make forecast\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=dataset, predictor=model\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "evaluator = Evaluator()\n",
    "evaluator(tss, forecasts)\n",
    "agg_metrics, item_metrics = evaluator(\n",
    "    iter(tss), iter(forecasts), num_series=len(dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "item_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from gluonts.dataset.split import split\n",
    "\n",
    "\n",
    "from gluonts.ev.metrics import MASE, MeanWeightedSumQuantileLoss\n",
    "from gluonts.model.evaluation import evaluate_forecasts\n",
    "\n",
    "metrics_df = evaluate_forecasts(\n",
    "    forecasts,\n",
    "    test_data=test_data,\n",
    "    metrics=[\n",
    "        MASE(),\n",
    "        MeanWeightedSumQuantileLoss(np.arange(0.1, 1.0, 0.1)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
